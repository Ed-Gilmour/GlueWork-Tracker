{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b60b7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 100 rows from /Users/stevefeng/Downloads/GlueWork-Tracker-branch-mentoring/glue_work_bot/training_data/maintainance_training_dataset.csv\n",
      "Using TEXT_COL='body', LABEL_COL='label' ‚Üí normalized to 'label_norm'\n",
      "Label distribution (raw): {'n': 50, 'y': 50}\n",
      "Label distribution (norm): {-1: 50, 0: 50}\n",
      "Rules: short_text=True, maint_regex=False\n",
      "Model: gpt-4o-mini\n",
      "\n",
      "================= Round 1/5 =================\n",
      "Round 1: 50 random test cases\n",
      "Gold counts: {-1: 30, 0: 20}\n",
      "\n",
      "### Per-instance outputs for: maintenance_random_round1 | N=50\n",
      "[0] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[1] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[2] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[3] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[4] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[5] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[6] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[7] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[8] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[9] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[10] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[11] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[12] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[13] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[14] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[15] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[16] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[17] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[18] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[19] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[20] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[21] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[22] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[23] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[24] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[25] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[26] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[27] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[28] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[29] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[30] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[31] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[32] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[33] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[34] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[35] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[36] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[37] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[38] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[39] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[40] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[41] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[42] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[43] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[44] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[45] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[46] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[47] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[48] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[49] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "üìÑ Saved per-instance details: maintenance_random_round1_details.csv\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.789     1.000     0.882        30\n",
      "           0      1.000     0.600     0.750        20\n",
      "\n",
      "    accuracy                          0.840        50\n",
      "   macro avg      0.895     0.800     0.816        50\n",
      "weighted avg      0.874     0.840     0.829        50\n",
      "\n",
      "Confusion matrix (rows=true [-1,0], cols=pred):\n",
      " [[30  0]\n",
      " [ 8 12]]\n",
      "Pred counts: {-1: 38, 0: 12}\n",
      "\n",
      "================= Round 2/5 =================\n",
      "Round 2: 50 random test cases\n",
      "Gold counts: {0: 30, -1: 20}\n",
      "\n",
      "### Per-instance outputs for: maintenance_random_round2 | N=50\n",
      "[0] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[1] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[2] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[3] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[4] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[5] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[6] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[7] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[8] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[9] GOLD=-1 | PRED=0 | ‚ùå\n",
      "[10] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[11] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[12] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[13] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[14] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[15] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[16] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[17] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[18] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[19] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[20] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[21] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[22] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[23] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[24] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[25] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[26] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[27] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[28] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[29] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[30] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[31] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[32] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[33] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[34] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[35] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[36] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[37] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[38] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[39] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[40] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[41] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[42] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[43] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[44] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[45] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[46] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[47] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[48] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[49] GOLD=0 | PRED=-1 | ‚ùå\n",
      "üìÑ Saved per-instance details: maintenance_random_round2_details.csv\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.613     0.950     0.745        20\n",
      "           0      0.947     0.600     0.735        30\n",
      "\n",
      "    accuracy                          0.740        50\n",
      "   macro avg      0.780     0.775     0.740        50\n",
      "weighted avg      0.814     0.740     0.739        50\n",
      "\n",
      "Confusion matrix (rows=true [-1,0], cols=pred):\n",
      " [[19  1]\n",
      " [12 18]]\n",
      "Pred counts: {-1: 31, 0: 19}\n",
      "\n",
      "================= Round 3/5 =================\n",
      "Round 3: 50 random test cases\n",
      "Gold counts: {-1: 25, 0: 25}\n",
      "\n",
      "### Per-instance outputs for: maintenance_random_round3 | N=50\n",
      "[0] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[1] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[2] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[3] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[4] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[5] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[6] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[7] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[8] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[9] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[10] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[11] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[12] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[13] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[14] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[15] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[16] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[17] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[18] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[19] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[20] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[21] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[22] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[23] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[24] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[25] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[26] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[27] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[28] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[29] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[30] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[31] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[32] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[33] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[34] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[35] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[36] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[37] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[38] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[39] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[40] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[41] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[42] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[43] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[44] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[45] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[46] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[47] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[48] GOLD=-1 | PRED=0 | ‚ùå\n",
      "[49] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "üìÑ Saved per-instance details: maintenance_random_round3_details.csv\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.800     0.960     0.873        25\n",
      "           0      0.950     0.760     0.844        25\n",
      "\n",
      "    accuracy                          0.860        50\n",
      "   macro avg      0.875     0.860     0.859        50\n",
      "weighted avg      0.875     0.860     0.859        50\n",
      "\n",
      "Confusion matrix (rows=true [-1,0], cols=pred):\n",
      " [[24  1]\n",
      " [ 6 19]]\n",
      "Pred counts: {-1: 30, 0: 20}\n",
      "\n",
      "================= Round 4/5 =================\n",
      "Round 4: 50 random test cases\n",
      "Gold counts: {0: 26, -1: 24}\n",
      "\n",
      "### Per-instance outputs for: maintenance_random_round4 | N=50\n",
      "[0] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[1] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[2] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[3] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[4] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[5] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[6] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[7] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[8] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[9] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[10] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[11] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[12] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[13] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[14] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[15] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[16] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[17] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[18] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[19] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[20] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[21] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[22] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[23] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[24] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[25] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[26] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[27] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[28] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[29] GOLD=-1 | PRED=0 | ‚ùå\n",
      "[30] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[31] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[32] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[33] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[34] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[35] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[36] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[37] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[38] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[39] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[40] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[41] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[42] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[43] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[44] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[45] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[46] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[47] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[48] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[49] GOLD=0 | PRED=0 | ‚úÖ\n",
      "üìÑ Saved per-instance details: maintenance_random_round4_details.csv\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.719     0.958     0.821        24\n",
      "           0      0.944     0.654     0.773        26\n",
      "\n",
      "    accuracy                          0.800        50\n",
      "   macro avg      0.832     0.806     0.797        50\n",
      "weighted avg      0.836     0.800     0.796        50\n",
      "\n",
      "Confusion matrix (rows=true [-1,0], cols=pred):\n",
      " [[23  1]\n",
      " [ 9 17]]\n",
      "Pred counts: {-1: 32, 0: 18}\n",
      "\n",
      "================= Round 5/5 =================\n",
      "Round 5: 50 random test cases\n",
      "Gold counts: {0: 26, -1: 24}\n",
      "\n",
      "### Per-instance outputs for: maintenance_random_round5 | N=50\n",
      "[0] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[1] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[2] GOLD=-1 | PRED=0 | ‚ùå\n",
      "[3] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[4] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[5] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[6] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[7] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[8] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[9] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[10] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[11] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[12] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[13] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[14] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[15] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[16] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[17] GOLD=-1 | PRED=0 | ‚ùå\n",
      "[18] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[19] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[20] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[21] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[22] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[23] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[24] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[25] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[26] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[27] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[28] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[29] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[30] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[31] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[32] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[33] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[34] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[35] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[36] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[37] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[38] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[39] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[40] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[41] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[42] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[43] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[44] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[45] GOLD=-1 | PRED=-1 | ‚úÖ\n",
      "[46] GOLD=0 | PRED=0 | ‚úÖ\n",
      "[47] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[48] GOLD=0 | PRED=-1 | ‚ùå\n",
      "[49] GOLD=0 | PRED=-1 | ‚ùå\n",
      "üìÑ Saved per-instance details: maintenance_random_round5_details.csv\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.759     0.917     0.830        24\n",
      "           0      0.905     0.731     0.809        26\n",
      "\n",
      "    accuracy                          0.820        50\n",
      "   macro avg      0.832     0.824     0.819        50\n",
      "weighted avg      0.835     0.820     0.819        50\n",
      "\n",
      "Confusion matrix (rows=true [-1,0], cols=pred):\n",
      " [[22  2]\n",
      " [ 7 19]]\n",
      "Pred counts: {-1: 29, 0: 21}\n",
      "\n",
      "üìÑ Saved random sampling metrics to maintenance_random_5round_metrics.csv\n",
      "\n",
      "================= 5-round RANDOM SAMPLING SUMMARY =================\n",
      " round  accuracy  f1_macro  precision_macro  recall_macro\n",
      "     1      0.84  0.816176         0.894737      0.800000\n",
      "     2      0.74  0.739896         0.780136      0.775000\n",
      "     3      0.86  0.858586         0.875000      0.860000\n",
      "     4      0.80  0.797078         0.831597      0.806090\n",
      "     5      0.82  0.819350         0.831691      0.823718\n",
      "\n",
      "MEAN ¬± STD over 5 rounds:\n",
      "- Accuracy         : 0.8120 ¬± 0.0460\n",
      "- F1 (macro)       : 0.8062 ¬± 0.0433\n",
      "- Precision (macro): 0.8426 ¬± 0.0445\n",
      "- Recall (macro)   : 0.8130 ¬± 0.0316\n",
      "\n",
      "üìÑ Saved mean ¬± std summary to maintenance_random_5round_summary_mean.csv\n",
      "\n",
      "================= MISCLASSIFIED (All Rounds) =================\n",
      "Total misclassified: 47 of 250\n",
      "\n",
      "Counts by (gold_label -> pred_label):\n",
      " gold_label  pred_label  count\n",
      "         -1           0      5\n",
      "          0          -1     42\n",
      "\n",
      "--- Examples (up to 3 per (gold -> pred)) ---\n",
      "\n",
      "[gold=-1 -> pred=0]  n=5\n",
      "‚Ä¢ round=2 gold=-1 pred=0\n",
      "  cleaned: Essentially all other packages have unit tests that run in less than a minute; flutter_migrate is doing something more like integration tests, and the tests tak‚Ä¶\n",
      "‚Ä¢ round=3 gold=-1 pred=0\n",
      "  cleaned: https://skia.googlesource.com/skia.git/+log/da724d312e65..ed42a94ee066 2025-08-25 kjlubick@google.com Fix singleton handing of CPU recorder/context 2025-08-25 s‚Ä¶\n",
      "‚Ä¢ round=4 gold=-1 pred=0\n",
      "  cleaned: https://skia.googlesource.com/skia.git/+log/da724d312e65..ed42a94ee066 2025-08-25 kjlubick@google.com Fix singleton handing of CPU recorder/context 2025-08-25 s‚Ä¶\n",
      "\n",
      "[gold=0 -> pred=-1]  n=42\n",
      "‚Ä¢ round=1 gold=0 pred=-1\n",
      "  cleaned: This PR introduces a `.separated` constructor to the `ReorderableListView` widget. The `.separated` constructor allows developers to define custom separators be‚Ä¶\n",
      "‚Ä¢ round=1 gold=0 pred=-1\n",
      "  cleaned: Refiling of #169273 (reverted in https://github.com/flutter/flutter/pull/170034), which is a refiling of #164094, which itself is a rebase of #159675. This PR a‚Ä¶\n",
      "‚Ä¢ round=1 gold=0 pred=-1\n",
      "  cleaned: When failing with `Build failed due to use of deleted Android v1 embedding.`, there are no reasons given. Finding the reason can be time consuming. Just adding ‚Ä¶\n",
      "\n",
      "üìÑ Saved misclassified instances: maintenance_misclassified_5rounds.csv\n",
      "üìÑ Saved JSONL: maintenance_misclassified_5rounds.jsonl\n",
      "\n",
      "================= CORRECTLY CLASSIFIED (All Rounds) =================\n",
      "Total correct: 203 of 250\n",
      "\n",
      "Counts by gold_label:\n",
      " gold_label  count\n",
      "         -1    118\n",
      "          0     85\n",
      "\n",
      "üìÑ Saved correctly classified instances: maintenance_correct_5rounds.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# GLOBAL CONFIG (Monte Carlo sampling)\n",
    "# ------------------------------\n",
    "DEFAULT_CSV = \"/Users/stevefeng/Downloads/GlueWork-Tracker-branch-mentoring/glue_work_bot/training_data/maintainance_training_dataset.csv\"\n",
    "CSV_PATH = os.environ.get(\"MAINT_DATASET\", DEFAULT_CSV)\n",
    "\n",
    "# Columns preferences; will auto-detect among these\n",
    "PREFERRED_TEXT_COLS: List[str] = [\"comments\", \"comment\", \"text\", \"body\", \"message\", \"content\"]\n",
    "PREFERRED_LABEL_COLS: List[str] = [\"label_norm\", \"label\", \"y\", \"target\"]\n",
    "\n",
    "# Monte Carlo settings\n",
    "N_ROUNDS = int(os.environ.get(\"N_ROUNDS\", 5))\n",
    "SAMPLE_SIZE = int(os.environ.get(\"SAMPLE_SIZE\", 50))\n",
    "RANDOM_STATE = int(os.environ.get(\"RANDOM_STATE\", 42))\n",
    "\n",
    "# Optional rules\n",
    "USE_RULE_SHORT_TEXT = True      # classify ultra-short comments as -1\n",
    "USE_RULE_REGEX     = False      # light positive regex for classic maintenance cues\n",
    "\n",
    "# OpenAI model\n",
    "OPENAI_MODEL = os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Helpers\n",
    "# ------------------------------\n",
    "def normalize_text_series(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "def autodetect_columns(df: pd.DataFrame) -> Tuple[str, str]:\n",
    "    text_col = next((c for c in PREFERRED_TEXT_COLS if c in df.columns), None) or df.columns[0]\n",
    "    label_col = next((c for c in PREFERRED_LABEL_COLS if c in df.columns), None)\n",
    "    if label_col is None:\n",
    "        raise ValueError(\"No label column found. Expected one of: \" + \", \".join(PREFERRED_LABEL_COLS))\n",
    "    return text_col, label_col\n",
    "\n",
    "def strip_quoted_lines(text: str) -> str:\n",
    "    lines = []\n",
    "    for ln in str(text).splitlines():\n",
    "        if ln.lstrip().startswith(\">\"):\n",
    "            continue\n",
    "        lines.append(ln)\n",
    "    out = \"\\n\".join(lines).strip()\n",
    "    return re.sub(r\"\\s+\\n\", \"\\n\", out)\n",
    "\n",
    "# --- GitHub template & checklist stripper (for PR bodies) ---\n",
    "GITHUB_TEMPLATE_SECTIONS = [\n",
    "    r\"##\\s*Pre-?launch Checklist.*?(?=^##|\\Z)\",\n",
    "    r\"<!--.*?-->\",\n",
    "    r\"^\\s*Thanks for filing a pull request!.*?$\",\n",
    "    r\"^\\s*If you need help, consider asking.*?$\",\n",
    "    r\"^\\s*\\[/?(Contributor Guide|Tree Hygiene|Flutter Style Guide|CLA|tests|breaking change policy|Discord|Data Driven Fixes).*$\",\n",
    "]\n",
    "def strip_templates(text: str) -> str:\n",
    "    t = str(text)\n",
    "    # Remove fenced code and media/links first\n",
    "    t = re.sub(r\"`{3}[\\s\\S]*?`{3}\", \" \", t)                         # ``` code fences\n",
    "    t = re.sub(r\"!\\[[^\\]]*\\]\\([^)]+\\)\", \" \", t)                     # images\n",
    "    t = re.sub(r\"\\[[^\\]]*\\]\\([^)]+\\)\", \" \", t)                      # links\n",
    "    t = re.sub(r\"^-\\s*\\[[ xX]\\]\\s*.*$\", \" \", t, flags=re.MULTILINE) # checklist items\n",
    "    # Remove common GH template sections\n",
    "    for pat in GITHUB_TEMPLATE_SECTIONS:\n",
    "        t = re.sub(pat, \" \", t, flags=re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "    # Collapse whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Rules (optional)\n",
    "# ------------------------------\n",
    "def rule_based_short_text(comment: str) -> Optional[int]:\n",
    "    text = str(comment).strip()\n",
    "    words = re.findall(r\"\\b\\w+\\b\", text)\n",
    "    if len(words) < 3:\n",
    "        return -1\n",
    "    return None\n",
    "\n",
    "MAINT_PATTERNS = re.compile(\n",
    "    r\"(fix(es|ed)?|resolves?|closes? #\\d+|regression|deflake|flaky|timeout|\"\n",
    "    r\"backport|revert|migrat(e|ion)|deprecat(ed|ion)|refactor|remove dead code|\"\n",
    "    r\"cve-\\d{4}-\\d+|security|bump|upgrade|pin(ned|ning)?|ci|build|tests?)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "def rule_based_regex(comment: str) -> Optional[int]:\n",
    "    txt = str(comment)\n",
    "    if MAINT_PATTERNS.search(txt):\n",
    "        # require some second technical token to reduce false positives\n",
    "        if re.search(r\"\\b(\\d+\\.\\d+(\\.\\d+)?)\\b|#\\d+|\\bCI\\b|\\btest(s)?\\b|\\bbuild\\b\", txt, re.IGNORECASE):\n",
    "            return 0\n",
    "    return None\n",
    "\n",
    "# ------------------------------\n",
    "# 3) LLM client (OpenAI)\n",
    "# ------------------------------\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"openai package missing. Install with `pip install openai`\") from e\n",
    "\n",
    "def get_client() -> \"OpenAI\":\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"‚ùå OPENAI_API_KEY not set. Export it before running.\")\n",
    "    return OpenAI(api_key=api_key)\n",
    "\n",
    "def to_label(resp: str) -> int:\n",
    "    s = (resp or \"\").strip()\n",
    "    s = s.replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n",
    "    m = re.search(r\"-?\\d+\", s)\n",
    "    if m:\n",
    "        try:\n",
    "            v = int(m.group(0))\n",
    "            if v in (0, -1):\n",
    "                return v\n",
    "        except ValueError:\n",
    "            pass\n",
    "    # fallbacks\n",
    "    sl = s.lower()\n",
    "    if \"not maintenance\" in sl:\n",
    "        return -1\n",
    "    if \"maintenance\" in sl:\n",
    "        return 0\n",
    "    if re.search(r\"\\b0\\b\", s):\n",
    "        return 0\n",
    "    return -1\n",
    "\n",
    "SYSTEM_MSG = \"\"\"You are a GitHub review-classifier.\n",
    "\n",
    "Label each comment/PR text as:\n",
    "- Maintenance (0) ‚Äî it repairs or sustains the project: bug/regression fixes, parity with platforms, refactors/cleanup/moves, backports/reverts, dependency/security updates, CI/test deflakes, migrations/deprecations, documentation corrections that prevent errors, or changes that unblock broken/mismatched behavior. Count it even if terse.\n",
    "- Not Maintenance (-1) ‚Äî new features/APIs with no upkeep/repair motivation; pure social/process chatter; bot messages.\n",
    "\n",
    "DECISION CHECKLIST (apply in order):\n",
    "A. Does the text claim or imply REPAIR/STABILITY/UNBLOCKING? (e.g., ‚ÄúFix/Fixes/Fixed/Resolves/Closes #‚Ä¶‚Äù, ‚Äúregression/crash/timeout/deflake/flaky‚Äù, ‚Äúunblocks‚Äù, ‚Äúaligns/matches/parity with native‚Äù, ‚Äúprevent misuse‚Äù, ‚Äúcorrect behavior‚Äù, ‚Äúsecurity/CVE‚Äù). ‚Üí Maintenance (0).\n",
    "B. Is it a MIGRATION/REFILE/REFOLLOW-UP/RELOCATION/REFACTOR/CLEANUP even without long rationale? (e.g., ‚Äúfollow up of #‚Ä¶‚Äù, ‚Äúrefiling of ‚Ä¶‚Äù, ‚Äúmigrate from X to Y‚Äù, ‚Äúmove A to B to reduce conflicts‚Äù, ‚Äúremove dead code/duplication‚Äù). ‚Üí Maintenance (0).\n",
    "C. Does it reference an issue/PR and say it **handles/addresses/unblocks/fixes** that problem, even if it starts with ‚ÄúAdds/Introduces/Improves‚Äù? ‚Üí Maintenance (0).\n",
    "D. Dependency/version updates, security patches, or CI/test stabilization (timeouts, flakes) ‚Üí Maintenance (0).\n",
    "E. Purely **new capability** (new API/constructor/parameter/widget) with no repair/parity/stability/security/cleanup/migration intent ‚Üí Not Maintenance (-1).\n",
    "F. Process/scheduling/acknowledgements/bots ‚Üí Not Maintenance (-1).\n",
    "\n",
    "Ambiguity resolution:\n",
    "‚Ä¢ If both ‚ÄúAdds/Introduces ‚Ä¶‚Äù and clear **repair/unblock/parity/migration/cleanup** intent are present, choose Maintenance (0).\n",
    "‚Ä¢ ‚ÄúRelated to #‚Ä¶‚Äù alone is not enough; combine with ‚Äúhandles/addresses/unblocks/fixes/migrates/cleans up‚Äù.\n",
    "‚Ä¢ Documentation counts as Maintenance (0) only when it corrects wrong behavior or prevents errors; otherwise -1.\n",
    "\n",
    "Output exactly one number: 0 or -1.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "POS_CUES = [\n",
    "    # Feature-looking openers that are actually repair/unblock/parity\n",
    "    (\"Adds glob syntax to proxy server to resolve mismatch with rules and unblock issue #173435.\", 0),\n",
    "    (\"Adds headers to proxy rules to align behavior across platforms and fix incorrect proxying in #173434.\", 0),\n",
    "    (\"This change improves overscroll to match native Android behavior; fixes clipped fling behavior (Fixes #169659).\", 0),\n",
    "\n",
    "    # Follow-up / refiling / migration / move-as-cleanup\n",
    "    (\"Follow up of #174421: migrate some files to WidgetState to reduce conflicts; remaining files in later PRs.\", 0),\n",
    "    (\"Refiling of #169273: bundle experimental data assets to restore expected tool behavior and unblock usage.\", 0),\n",
    "    (\"Move PageTransitionsBuilder from material/ to widget/ to keep types in the correct layer.\", 0),\n",
    "\n",
    "    # Classic fixes & CI/test stability\n",
    "    (\"Fix DropdownMenu filtering by storing selected value instead of index; add a regression test.\", 0),\n",
    "    (\"Deflake GPU tests by removing real-time sleeps; use a virtual clock to prevent timeouts.\", 0),\n",
    "]\n",
    "\n",
    "NEAR_MISS_NEGS = [\n",
    "    # Pure feature without upkeep intent\n",
    "    (\"Introduces ReorderableListView.separated constructor (new API).\", -1),\n",
    "    (\"Adds weekType parameter to CupertinoDatePicker to control selectable days (feature).\", -1),\n",
    "    (\"Widget previewer filters previews by active editor location; includes UI changes (feature).\", -1),\n",
    "\n",
    "    # Process/social only\n",
    "    (\"Thanks! I'll merge after CI.\", -1),\n",
    "    (\"Please rebase on main.\", -1),\n",
    "\n",
    "    # Issue mention without maintenance intent\n",
    "    (\"Related to #173838\", -1),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def fewshot_block(examples):\n",
    "    return \"\\n\\n\".join([f\"Comment: {t}\\nLabel: {y}\" for t, y in examples])\n",
    "\n",
    "FEWSHOT_CURATED = fewshot_block(POS_CUES + NEAR_MISS_NEGS)\n",
    "\n",
    "def fs_strict(comment: str) -> str:\n",
    "    return f\"\"\"{BASE}\n",
    "\n",
    "Few-shot examples:\n",
    "{FEWSHOT_CURATED}\n",
    "\n",
    "Comment to classify:\n",
    "{comment}\n",
    "\"\"\"\n",
    "\n",
    "def classify_once(client, prompt: str) -> Tuple[int, str]:\n",
    "    r = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_completion_tokens=10,\n",
    "    )\n",
    "    raw = (r.choices[0].message.content or \"\").strip()\n",
    "    if not raw:\n",
    "        return -1, \"\"\n",
    "    return to_label(raw), raw\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Evaluation\n",
    "# ------------------------------\n",
    "def eval_on_split(name: str, X_split: pd.Series, y_split: pd.Series, use_rules: bool=True) -> Dict[str, Any]:\n",
    "    client = get_client()\n",
    "    preds, raws, prompts_preview, detailed_rows = [], [], [], []\n",
    "\n",
    "    print(f\"\\n### Per-instance outputs for: {name} | N={len(X_split)}\")\n",
    "    for i, (txt, gold) in enumerate(zip(X_split, y_split)):\n",
    "        # Clean text (strip quotes + GH template noise)\n",
    "        clean = strip_quoted_lines(txt)\n",
    "        clean = strip_templates(clean)\n",
    "\n",
    "        prompt = fs_strict(clean)\n",
    "        prompt_preview = prompt[:240].replace(\"\\n\", \" \") + (\"...\" if len(prompt) > 240 else \"\")\n",
    "\n",
    "        if use_rules:\n",
    "            rb = rule_based_short_text(clean)\n",
    "            if rb is not None:\n",
    "                pred, raw = rb, \"[RULE: short-text <3 words]\"\n",
    "            else:\n",
    "                if USE_RULE_REGEX:\n",
    "                    rb2 = rule_based_regex(clean)\n",
    "                    if rb2 is not None:\n",
    "                        pred, raw = rb2, \"[RULE: maint-regex]\"\n",
    "                    else:\n",
    "                        pred, raw = classify_once(client, prompt)\n",
    "                else:\n",
    "                    pred, raw = classify_once(client, prompt)\n",
    "        else:\n",
    "            pred, raw = classify_once(client, prompt)\n",
    "\n",
    "        preds.append(pred)\n",
    "        raws.append(raw)\n",
    "        detailed_rows.append({\n",
    "            \"index\": i,\n",
    "            \"comment\": txt,\n",
    "            \"cleaned\": clean,\n",
    "            \"gold_label\": int(gold),\n",
    "            \"pred_label\": int(pred),\n",
    "            \"correct\": int(pred) == int(gold),\n",
    "            \"raw_model_output\": raw,\n",
    "            \"prompt_preview\": prompt_preview,\n",
    "        })\n",
    "\n",
    "        correct = (int(pred) == int(gold))\n",
    "        print(f\"[{i}] GOLD={int(gold)} | PRED={int(pred)} | {'‚úÖ' if correct else '‚ùå'}\")\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    y_true = y_split.values\n",
    "\n",
    "    detailed_df = pd.DataFrame(detailed_rows)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"accuracy\": float(accuracy_score(y_true, preds)),\n",
    "        \"f1_macro\": float(f1_score(y_true, preds, average=\"macro\")),\n",
    "        \"f1_weighted\": float(f1_score(y_true, preds, average=\"weighted\")),\n",
    "        \"precision_macro\": float(precision_recall_fscore_support(y_true, preds, average=\"macro\", zero_division=0)[0]),\n",
    "        \"recall_macro\": float(precision_recall_fscore_support(y_true, preds, average=\"macro\", zero_division=0)[1]),\n",
    "        \"report\": classification_report(y_true, preds, digits=3),\n",
    "        \"cm\": confusion_matrix(y_true, preds),\n",
    "        \"preds\": preds,\n",
    "        \"detailed_df\": detailed_df,\n",
    "    }\n",
    "\n",
    "# --- robust y/n ‚Üí 0/-1 normalization\n",
    "def normalize_label_value(v: str) -> int:\n",
    "    LABEL_MAP = {\n",
    "        \"y\": 0, \"yes\": 0, \"true\": 0, \"t\": 0, \"1\": 0, \"maint\": 0, \"maintenance\": 0, \"0\": 0,\n",
    "        \"n\": -1, \"no\": -1, \"false\": -1, \"f\": -1, \"-1\": -1, \"not maintenance\": -1,\n",
    "    }\n",
    "    s = str(v).strip().lower()\n",
    "    if s in LABEL_MAP:\n",
    "        return LABEL_MAP[s]\n",
    "    try:\n",
    "        x = int(float(s))\n",
    "        if x in (0, -1):\n",
    "            return x\n",
    "    except Exception:\n",
    "        pass\n",
    "    raise ValueError(f\"Unrecognized label value: {v!r}. Expected y/n or 0/-1.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Main (Monte Carlo 5 rounds √ó 50 samples)\n",
    "# ------------------------------\n",
    "def main():\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        raise FileNotFoundError(f\"CSV not found: {CSV_PATH}. Override with MAINT_DATASET=/path/to.csv\")\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    text_col, label_col = autodetect_columns(df)\n",
    "\n",
    "    # Normalize text and labels\n",
    "    X_all = normalize_text_series(df[text_col]).reset_index(drop=True)\n",
    "    df[\"label_norm\"] = df[label_col].apply(normalize_label_value).astype(int)\n",
    "    y_all = df[\"label_norm\"].reset_index(drop=True).astype(int)\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(df)} rows from {CSV_PATH}\")\n",
    "    print(f\"Using TEXT_COL='{text_col}', LABEL_COL='{label_col}' ‚Üí normalized to 'label_norm'\")\n",
    "    print(\"Label distribution (raw):\", df[label_col].value_counts(dropna=False).to_dict())\n",
    "    print(\"Label distribution (norm):\", df[\"label_norm\"].value_counts().to_dict())\n",
    "    print(f\"Rules: short_text={USE_RULE_SHORT_TEXT}, maint_regex={USE_RULE_REGEX}\")\n",
    "    print(f\"Model: {OPENAI_MODEL}\")\n",
    "\n",
    "    # Monte Carlo rounds\n",
    "    fold_metrics = []\n",
    "    all_round_rows = []\n",
    "\n",
    "    for round_idx in range(1, N_ROUNDS + 1):\n",
    "        # random 50 (or fewer if dataset smaller)\n",
    "        size = min(SAMPLE_SIZE, len(X_all))\n",
    "        sample_idx = np.random.choice(len(X_all), size=size, replace=False)\n",
    "        X_sample = X_all.iloc[sample_idx].reset_index(drop=True)\n",
    "        y_sample = y_all.iloc[sample_idx].reset_index(drop=True)\n",
    "\n",
    "        print(f\"\\n================= Round {round_idx}/{N_ROUNDS} =================\")\n",
    "        print(f\"Round {round_idx}: {len(X_sample)} random test cases\")\n",
    "        print(\"Gold counts:\", y_sample.value_counts().to_dict())\n",
    "\n",
    "        res = eval_on_split(\n",
    "            name=f\"maintenance_random_round{round_idx}\",\n",
    "            X_split=X_sample,\n",
    "            y_split=y_sample,\n",
    "            use_rules=False,   # pure LLM evaluation; set True to turn on rules\n",
    "        )\n",
    "\n",
    "        round_df = res[\"detailed_df\"].copy()\n",
    "        round_df[\"round\"] = round_idx\n",
    "        out_csv = f\"maintenance_random_round{round_idx}_details.csv\"\n",
    "        round_df.to_csv(out_csv, index=False)\n",
    "        print(f\"üìÑ Saved per-instance details: {out_csv}\")\n",
    "\n",
    "        print(\"\\n--- Classification Report ---\")\n",
    "        print(res[\"report\"])\n",
    "        print(\"Confusion matrix (rows=true [-1,0], cols=pred):\\n\", res[\"cm\"])\n",
    "        print(\"Pred counts:\", pd.Series(res[\"preds\"]).value_counts().to_dict())\n",
    "\n",
    "        fold_metrics.append({\n",
    "            \"round\": round_idx,\n",
    "            \"accuracy\": res[\"accuracy\"],\n",
    "            \"f1_macro\": res[\"f1_macro\"],\n",
    "            \"precision_macro\": res[\"precision_macro\"],\n",
    "            \"recall_macro\": res[\"recall_macro\"],\n",
    "        })\n",
    "        all_round_rows.append(round_df)\n",
    "\n",
    "    # Aggregate metrics across rounds\n",
    "    cv_df = pd.DataFrame(fold_metrics)\n",
    "    cv_df.to_csv(\"maintenance_random_5round_metrics.csv\", index=False)\n",
    "    print(\"\\nüìÑ Saved random sampling metrics to maintenance_random_5round_metrics.csv\")\n",
    "\n",
    "    # === Compute mean ¬± std and export ===\n",
    "    def mean_std(series: pd.Series) -> Tuple[float, float]:\n",
    "        mu = float(series.mean())\n",
    "        sd = float(series.std(ddof=1)) if len(series) > 1 else 0.0\n",
    "        return mu, sd\n",
    "\n",
    "    acc_mu, acc_sd = mean_std(cv_df[\"accuracy\"])\n",
    "    f1_mu,  f1_sd  = mean_std(cv_df[\"f1_macro\"])\n",
    "    pr_mu,  pr_sd  = mean_std(cv_df[\"precision_macro\"])\n",
    "    rc_mu,  rc_sd  = mean_std(cv_df[\"recall_macro\"])\n",
    "\n",
    "    print(\"\\n================= 5-round RANDOM SAMPLING SUMMARY =================\")\n",
    "    print(cv_df.to_string(index=False))\n",
    "    print(\n",
    "        f\"\\nMEAN ¬± STD over {N_ROUNDS} rounds:\\n\"\n",
    "        f\"- Accuracy         : {acc_mu:.4f} ¬± {acc_sd:.4f}\\n\"\n",
    "        f\"- F1 (macro)       : {f1_mu:.4f} ¬± {f1_sd:.4f}\\n\"\n",
    "        f\"- Precision (macro): {pr_mu:.4f} ¬± {pr_sd:.4f}\\n\"\n",
    "        f\"- Recall (macro)   : {rc_mu:.4f} ¬± {rc_sd:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Save mean ¬± std summary CSV\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"metric\": [\"Accuracy\", \"F1_macro\", \"Precision_macro\", \"Recall_macro\"],\n",
    "        \"mean\":   [acc_mu, f1_mu, pr_mu, rc_mu],\n",
    "        \"std\":    [acc_sd, f1_sd, pr_sd, rc_sd],\n",
    "    })\n",
    "    summary_path = \"maintenance_random_5round_summary_mean.csv\"\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"\\nüìÑ Saved mean ¬± std summary to {summary_path}\")\n",
    "\n",
    "    # Aggregate & export misclassified/correct across all rounds\n",
    "    if len(all_round_rows):\n",
    "        all_df = pd.concat(all_round_rows, ignore_index=True)\n",
    "\n",
    "        # MISCLASSIFIED\n",
    "        mis = all_df[~all_df[\"correct\"]].copy()\n",
    "        print(\"\\n================= MISCLASSIFIED (All Rounds) =================\")\n",
    "        if len(mis) == 0:\n",
    "            print(\"üéâ No misclassified instances.\")\n",
    "        else:\n",
    "            pair_counts = (\n",
    "                mis.groupby([\"gold_label\", \"pred_label\"])\n",
    "                   .size()\n",
    "                   .reset_index(name=\"count\")\n",
    "                   .sort_values([\"gold_label\", \"pred_label\", \"count\"], ascending=[True, True, False])\n",
    "            )\n",
    "            print(f\"Total misclassified: {len(mis)} of {len(all_df)}\")\n",
    "            print(\"\\nCounts by (gold_label -> pred_label):\")\n",
    "            print(pair_counts.to_string(index=False))\n",
    "\n",
    "            def trunc(s, n=160): s=str(s); return (s[:n]+\"‚Ä¶\") if len(s)>n else s\n",
    "            print(\"\\n--- Examples (up to 3 per (gold -> pred)) ---\")\n",
    "            for (g,p), grp in mis.groupby([\"gold_label\",\"pred_label\"]):\n",
    "                print(f\"\\n[gold={g} -> pred={p}]  n={len(grp)}\")\n",
    "                for _, row in grp.head(3).iterrows():\n",
    "                    print(f\"‚Ä¢ round={int(row['round'])} gold={int(row['gold_label'])} pred={int(row['pred_label'])}\")\n",
    "                    print(f\"  cleaned: {trunc(row['cleaned'])}\")\n",
    "\n",
    "            mis_out = \"maintenance_misclassified_5rounds.csv\"\n",
    "            cols = [\"round\", \"index\", \"gold_label\", \"pred_label\", \"correct\", \"comment\", \"cleaned\", \"raw_model_output\", \"prompt_preview\"]\n",
    "            mis[cols].to_csv(mis_out, index=False)\n",
    "            print(f\"\\nüìÑ Saved misclassified instances: {mis_out}\")\n",
    "\n",
    "            # Optional JSONL\n",
    "            try:\n",
    "                import pathlib\n",
    "                jpath = pathlib.Path(\"maintenance_misclassified_5rounds.jsonl\")\n",
    "                with jpath.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                    for _, r in mis.iterrows():\n",
    "                        f.write(json.dumps({\n",
    "                            \"round\": int(r[\"round\"]),\n",
    "                            \"index\": int(r[\"index\"]),\n",
    "                            \"gold_label\": int(r[\"gold_label\"]),\n",
    "                            \"pred_label\": int(r[\"pred_label\"]),\n",
    "                            \"comment\": r[\"comment\"],\n",
    "                            \"cleaned\": r[\"cleaned\"],\n",
    "                            \"raw_model_output\": r[\"raw_model_output\"],\n",
    "                            \"prompt_preview\": r[\"prompt_preview\"],\n",
    "                        }, ensure_ascii=False) + \"\\n\")\n",
    "                print(f\"üìÑ Saved JSONL: {jpath}\")\n",
    "            except Exception as e:\n",
    "                print(f\"JSONL save skipped: {e}\")\n",
    "\n",
    "        # CORRECT\n",
    "        correct_df = all_df[all_df[\"correct\"]].copy()\n",
    "        print(\"\\n================= CORRECTLY CLASSIFIED (All Rounds) =================\")\n",
    "        if len(correct_df) == 0:\n",
    "            print(\"‚ö†Ô∏è No correctly classified instances found.\")\n",
    "        else:\n",
    "            print(f\"Total correct: {len(correct_df)} of {len(all_df)}\")\n",
    "            correct_counts = (\n",
    "                correct_df.groupby([\"gold_label\"])\n",
    "                          .size()\n",
    "                          .reset_index(name=\"count\")\n",
    "                          .sort_values(\"gold_label\")\n",
    "            )\n",
    "            print(\"\\nCounts by gold_label:\")\n",
    "            print(correct_counts.to_string(index=False))\n",
    "\n",
    "            correct_out = \"maintenance_correct_5rounds.csv\"\n",
    "            cols2 = [\"round\", \"index\", \"gold_label\", \"pred_label\", \"comment\", \"cleaned\", \"raw_model_output\", \"prompt_preview\"]\n",
    "            correct_df[cols2].to_csv(correct_out, index=False)\n",
    "            print(f\"\\nüìÑ Saved correctly classified instances: {correct_out}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
