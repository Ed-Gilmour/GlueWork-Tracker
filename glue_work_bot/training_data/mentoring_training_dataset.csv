PR_number,comments,user,date_diff_with_PR_user,mentoring,PR-Author
tinkerpop1247,i have no clue why docker is failing on the gremlin-python portion of the build. works fine for me locally - it's just doing `docker/build.sh` with no flags. not sure if anyone else can get it to fail locally...,spmallette,-1832 days,N,Haapsaari-Juha
tinkerpop1247,"As a quick note @Haapsaari-Juha given that we are trying to quickly turn around a 3.4.6 release to fix a bug in 3.4.5:



https://lists.apache.org/thread.html/r7de7d160173d0774720b347122409bb6e46268613205b08edaa687ee%40%3Cdev.tinkerpop.apache.org%3E



it may take some extra time to see this change reviewed/merged.",spmallette,-1832 days,N,Haapsaari-Juha
tinkerpop1247,"Change looks pretty safe. Event check for `null` in message is kind of reasonable, because could return null https://github.com/JamesNK/Newtonsoft.Json/blob/a31156e90a14038872f54eb60ff0e9676ca4a0d8/Src/Newtonsoft.Json/JsonConvert.cs#L823 . Also at same time not sure if it hides bad JSON setting or other json related behind. Should we fail totally in this case?",dzmitry-lahoda,4 days,Y,Haapsaari-Juha
tinkerpop1247,"> Also at same time not sure if it hides bad JSON setting or other json related behind. Should we fail totally in this case?



We would still get an error in that case [from here](https://github.com/apache/tinkerpop/blob/master/gremlin-dotnet/src/Gremlin.Net/Driver/Connection.cs#L136) where the `receivedMsg` is also accessed without a null check. If `receivedMsg` is `null` then `TryParseResponseMessage` will throw a `NullReferenceException` which will be caught by the try/catch in `Parse` and we notify the response handler about the exception.

I think that we should however ensure that we're not getting an exception in a `catch` clause.",FlorianHockmann,-1172 days,N,Haapsaari-Juha
tinkerpop1247,"> Parse and we notify the response handler about the exception.



Will not because message is null. There there is silent error eating. 



Could we just check that deserialized object is not null and throw exception.



```

var receivedMsg = _messageSerializer.DeserializeMessage<ResponseMessage<JToken>>(received);

if (receivedMsg == null)

{

throw new Exception(""receivedMsg"");

}

```



To check what happens in case of null array passed, in case of empty array passed, and do property based testing for any other array passed. So it will be more explainable failure than just eating null.",dzmitry-lahoda,4 days,Y,Haapsaari-Juha
tinkerpop1247,"> Will not because message is null. There there is silent error eating.



You're right. I should have looked taken a closer look.



> Could we just check that deserialized object is not null and throw exception.



Yes, I think that might be the best option as we expect the message to never be null. So, if for some reason is actually null, then we can throw an exception which will result in a call to `CloseConnectionBecauseOfFailureAsync` which closes the connection completely and notifies all existing response handlers about the exception.



@Haapsaari-Juha: I don't know if you want to incorporate these changes into your PR as you were the first to contribute a fix for this issue or if @dzmitry-lahoda should update his PR accordingly.



> property based testing for any other array passed



Could you explain a bit further what you mean by this? Do you want to add property based testing for the internal classes of the driver?",FlorianHockmann,-1172 days,N,Haapsaari-Juha
tinkerpop1247,"@FlorianHockmann I did change in my PR. Please review.



> property based testing



Given we do not have fix which only improves situation (like my PR), we would have to prove.



For this:

0. Check latest Json library and see it returns `object?` - so null is possible. DONE

1. Test with byte array being null, empty. What error we see?

2. Test byte array contains string ""null"", """", ""[]"", ""{}"", not null terminated string, etc.

3. Run generator of byte arrays with https://github.com/haf/expecto#property-based-tests (in C#)



Check that we never ever get null reference in this case. Or in some case we do. Than we should look into Json or into layer creating byte array. And have better semantics for error. 



But this seems no needed for now.",dzmitry-lahoda,4 days,Y,Haapsaari-Juha
tinkerpop1247,closing in favor of #1250,spmallette,-1832 days,N,Haapsaari-Juha
hadoop2462,":confetti_ball: **+1 overall**













| Vote | Subsystem | Runtime | Logfile | Comment |

|:----:|----------:|--------:|:--------:|:-------:|

| +0 :ok: | reexec | 26m 3s | | Docker mode activated. |

|||| _ Prechecks _ |

| +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. |

| +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. |

| +1 :green_heart: | | 0m 0s | [test4tests](test4tests) | The patch appears to include 1 new or modified test files. |

|||| _ trunk Compile Tests _ |

| +1 :green_heart: | mvninstall | 34m 10s | | trunk passed |

| +1 :green_heart: | shadedclient | 49m 29s | | branch has no errors when building and testing our client artifacts. |

|||| _ Patch Compile Tests _ |

| +1 :green_heart: | mvninstall | 0m 37s | | the patch passed |

| +1 :green_heart: | whitespace | 0m 0s | | The patch has no whitespace issues. |

| +1 :green_heart: | xml | 0m 2s | | The patch has no ill-formed XML file. |

| +1 :green_heart: | shadedclient | 16m 56s | | patch has no errors when building and testing our client artifacts. |

|||| _ Other Tests _ |

| +1 :green_heart: | unit | 0m 32s | | hadoop-aws in the patch passed. |

| +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. |

| | | 95m 56s | | |





| Subsystem | Report/Notes |

|----------:|:-------------|

| Docker | ClientAPI=1.40 ServerAPI=1.40 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2462/1/artifact/out/Dockerfile |

| GITHUB PR | https://github.com/apache/hadoop/pull/2462 |

| Optional Tests | dupname asflicense unit xml |

| uname | Linux a7d152c39402 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |

| Build tool | maven |

| Personality | dev-support/bin/hadoop.sh |

| git revision | trunk / fc961b63d14 |

| Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2462/1/testReport/ |

| Max. process+thread count | 535 (vs. ulimit of 5500) |

| modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |

| Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2462/1/console |

| versions | git=2.17.1 maven=3.6.0 |

| Powered by | Apache Yetus 0.13.0-SNAPSHOT https://yetus.apache.org |





This message was automatically generated.",hadoop-yetus,-66 days,N,adoroszlai
hadoop2462,"LGTM, 


+1. merging. (Attempting to do it from IntelliJ, FWIW)

That NoVersionAttributeException means that the code to detect changes in a file while open didn't get an etag in the GET response. Ozone needs to fix that. In the meantime, you can set a different change detection policy for your buckets, so disable that logic",steveloughran,-1226 days,Y,adoroszlai
groovy105,"Hi Esteban,

thanks for the pull request. :)

You talking about getXZYPath() has me wondering if your fix will work with Java 6 (because Groovy still supports that)?

Thanks,
Pascal",PascalSchumacher,-24 days,Y,eginez
groovy105,"One comment I would make regarding this, if we start adding `isAssignableFrom` checks for every possible type then the performance is going to start to suffer. It may be that there needs to be an extensible codec mechanism added instead.",graemerocher,98 days,Y,eginez
groovy105,Thanks! :+1:,PascalSchumacher,-24 days,N,eginez
flink15865,"Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community
to review your pull request. We will use this comment to track the progress of the review.


## Automated Checks
Last check on commit 195d03891a9c5cef972527218e8f1be6693ff695 (Sat May 08 07:57:45 UTC 2021)

✅no warnings

<sub>Mention the bot in a comment to re-run the automated checks.</sub>
## Review Progress

* ❓ 1. The [description] looks good.
* ❓ 2. There is [consensus] that the contribution should go into to Flink.
* ❓ 3. Needs [attention] from.
* ❓ 4. The change fits into the overall [architecture].
* ❓ 5. Overall code [quality] is good.

Please see the [Pull Request Review Guide](https://flink.apache.org/contributing/reviewing-prs.html) for a full explanation of the review process.<details>
The Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>
The @flinkbot bot supports the following commands:

- `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)
- `@flinkbot approve all` to approve all aspects
- `@flinkbot approve-until architecture` to approve everything until `architecture`
- `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention
- `@flinkbot disapprove architecture` to remove an approval you gave earlier
</details>",flinkbot,-551 days,N,Shawn-Hx
flink15865,"<!--
Meta data
{
""version"" : 1,
""metaDataEntries"" : [ {
""hash"" : ""195d03891a9c5cef972527218e8f1be6693ff695"",
""status"" : ""DELETED"",
""url"" : ""https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=17738"",
""triggerID"" : ""195d03891a9c5cef972527218e8f1be6693ff695"",
""triggerType"" : ""PUSH""
}, {
""hash"" : ""7b43e89b82548db8d911c7ef04a56846d153dd15"",
""status"" : ""FAILURE"",
""url"" : ""https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=17843"",
""triggerID"" : ""7b43e89b82548db8d911c7ef04a56846d153dd15"",
""triggerType"" : ""PUSH""
} ]
}-->
## CI report:

* 7b43e89b82548db8d911c7ef04a56846d153dd15 Azure: [FAILURE](https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=17843) 

<details>
<summary>Bot commands</summary>
The @flinkbot bot supports the following commands:

- `@flinkbot run travis` re-run the last Travis build
- `@flinkbot run azure` re-run the last Azure build
</details>",flinkbot,-551 days,N,Shawn-Hx
flink15865,"Thanks @Shawn-Hx, I'll take a look.



A quick question, how did you locate these broken images? Did you just manually go through all the pages, or is there an automatic approach? I'm asking because, if there's an automatic approach, we could introduce a ci-check against broken images.",xintongsong,-456 days,Y,Shawn-Hx
flink15865,"Thanks for opening this, I’ll take a closer look on Monday. Unfortunately there isn’t a great way to automate this process. The migration was done via a custom tool I wrote and there were clearly edge cases I missed.",sjwiesman,-1248 days,N,Shawn-Hx
flink15865,"Thanks for the information, @Shawn-Hx and @sjwiesman.

Sound like both of you were checking against legacy Jekyll formats. I guess it's fine that we can't automate this process, since the migration is done and ideally we should not introduce any new broken links/images.",xintongsong,-456 days,Y,Shawn-Hx
flink15865,"LGTM, Merging into master and release-1.13",sjwiesman,-1248 days,N,Shawn-Hx
cloudstack1376,"The changes are only limited to UI, so there is no need to run smoke/integration tests.
Cursory look at the changes looks fine.
@swill I think these should be merged unless some language expert want to comment.",koushik-das,-3 days,Y,milamberspace
cloudstack1376,Thanks koushik. I tend to agree with you on this one. I will add it to my merge list. Thanks...,swill,-31 days,N,milamberspace
cloudstack1376,Jenkins failed with a timeout (not related to your code). Can you squash your commits and do a force push again? Sorry for the runaround..,swill,-31 days,N,milamberspace
cloudstack1376,@DaanHoogland do you know why jenkins is failing here?,swill,-31 days,N,milamberspace
cloudstack1376,"@swill, @milamberspace sorry, the build is gone, I didn't react quick enough",DaanHoogland,-238 days,N,milamberspace
cloudstack1376,That is fine. @milamberspace would you mind just doing a force push again to kick off jenkins. I have seen other jenkins runs passing so I think jenkins is just not happy when it has a lot of load.,swill,-31 days,Y,milamberspace
cloudstack1376,"LGTM

tag:easypr",rhtyd,-255 days,N,milamberspace
cloudstack1376,"This PR currently has merge conflicts, but #1515 is next in line, so you may want to wait till it is merged before you fix these conflicts.",swill,-31 days,N,milamberspace
cloudstack1376,"#1515 has now been merged (sorry for the delay). Once you have a chance to fix the merge conflicts, I can get this merged. Thx...",swill,-31 days,N,milamberspace
cloudstack1376,"@swill @milamberspace weird, we've lost our Travis integration with PRs here. https://travis-ci.org/apache/cloudstack/ says ""The repository at apache/cloudstack was not found""",rhtyd,-255 days,N,milamberspace
cloudstack1376,"@milamberspace can you do a force push again. I have pushed fixes to Jenkins and Travis this morning, so with a new push we should be able to get this green. Thx...",swill,-31 days,N,milamberspace
cloudstack1376,"LGTM

tag:mergeready",rhtyd,-255 days,N,milamberspace
cloudstack1376,"### ACS CI BVT Run

**Sumarry:**
Build Number 56
Hypervisor xenserver
NetworkType Advanced
Passed=72
Failed=1
Skipped=3

_Link to logs Folder (search by build_no):_ https://www.dropbox.com/sh/yj3wnzbceo9uef2/AAB6u-Iap-xztdm6jHX9SjPja?dl=0

**Failed tests:**
- test_vpc_vpn.py
- test_01_redundant_vpc_site2site_vpn Failed

**Skipped tests:**
test_vm_nic_adapter_vmxnet3
test_static_role_account_acls
test_deploy_vgpu_enabled_vm

**Passed test suits:**
test_deploy_vm_with_userdata.py
test_affinity_groups_projects.py
test_portable_publicip.py
test_over_provisioning.py
test_global_settings.py
test_scale_vm.py
test_service_offerings.py
test_routers_iptables_default_policy.py
test_routers.py
test_reset_vm_on_reboot.py
test_snapshots.py
test_deploy_vms_with_varied_deploymentplanners.py
test_deploy_vm_iso.py
test_list_ids_parameter.py
test_public_ip_range.py
test_multipleips_per_nic.py
test_regions.py
test_affinity_groups.py
test_network_acl.py
test_pvlan.py
test_volumes.py
test_nic.py
test_deploy_vm_root_resize.py
test_resource_detail.py
test_secondary_storage.py
test_vm_life_cycle.py
test_disk_offerings.py",bvbharatk,103 days,N,milamberspace
isis138,"I can see 2 enhancements to this class 
1) let 'random' be a final field 
2) use Random.nextInt(int bound) instead, to improve code readability 

I'm using this inspiration to update the class in the 'v2' development branch. Thx! 

```java 
private static final int NUMBER_CHARACTERS = 10; 
private static final String CHARACTERS = ""ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890""; 

private final Random random = new Random(); 

@Override 
public String generateRandomCode() { 
final StringBuilder buf = new StringBuilder(NUMBER_CHARACTERS); 
for (int i = 0; i < NUMBER_CHARACTERS; i++) { 
final int pos = random.nextInt(CHARACTERS.length()); 
buf.append(CHARACTERS.charAt(pos)); 
} 
return buf.toString(); 
} 
```",andi-huber,-1165 days,Y,bd2019us
isis138,"Optimization applied to '2112_Spring' branch, which is designated to be merged into the final 2.0.0 release.",andi-huber,-1165 days,N,bd2019us
poi73,Can one of the admins verify this patch?,asfgit,435 days,N,irafishbein
poi73,@ADIR01 / @SEMION1956 FYI.,tomerm,439 days,N,irafishbein
poi73,"I took a quick look, one question in general: Is the long-value useful outside of POI at all? Or is it rather something that is only needed internally in the on-disk format? Maybe we can just remove the setter/getter which set/get ""long"" and only provide the enum to the outside to keep the interface simple?",centic9,-623 days,Y,irafishbein
poi73,"I have pushed a slightly modified version of the patch which only adds setter/getter for the enum-value. I see no use in providing the long-value to the outside and we probably should switch to the Enum for HSSF as well in the future and deprecate the ""short"" versions there. 

Please close this PR if the change is sufficient for you or comment if you think this should be done differently.",centic9,-623 days,Y,irafishbein
poi73,"@centic9 , answering the ""when"" question would be great. However I realize that it might be problematic and in any case it would be only tentative. Yet, if you can say in which **version** of POI it will be present for sure it would help a lot. For example, any chances it can make it into 7.10 ? Many thanks.",tomerm,439 days,Y,irafishbein
poi73,"When is probably hard to say as we currently do not follow a defined release-timeline, release version will be 4.0, which we started to work on 2 months ago, a release typically happens every few months, so early 2018 would be a rough guess.",centic9,-623 days,N,irafishbein
spark10434,Can one of the admins verify this patch?,AmplabJenkins,-664 days,N,XD-DENG
spark10434,"This isn't quite how we report issues or suggest changes. Please read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark first.

This is just an example of getting metrics, so the test/train split isn't as important. Still it is showing evaluating metrics and another example in isotonic_regression_example.py does do a test/train split. You might follow its example. Look for other similar examples that should be standardized in this way.",srowen,-663 days,Y,XD-DENG
spark10434,@XD-DENG can you address my comments or close this PR?,srowen,-663 days,N,XD-DENG
spark10434,"OK, either way. I was suggesting that it's fine to make this example consistent with the others, even if test/train isn't the point of this example. if you do proceed, read the contributing wiki above to see how to make a JIRA, and then investigate whether there are other similar examples to change like the one I mentioned.",srowen,-663 days,Y,XD-DENG
tinkerpop964,"Thanks for submitting this....Travis seems unhappy on the gremlin-python version of the build....could you please have a look at that? Also, this looks like it needs a rebase now that we have the tp33 branch re-opened for development. That should get CHANGELOG setup for new entries. Please don't include the JIRA issue there - just a bullet point with your representation of what changed will suffice.",spmallette,-646 days,Y,aboudreault
tinkerpop964,"I think there needs to be a documentation update too - i just noticed this: 

http://tinkerpop.apache.org/docs/3.3.4/reference/#_limitations",spmallette,-646 days,N,aboudreault
tinkerpop964,I re-ran the travis build and the error is gone. probably a fluke of travis or something. I guess this is good now. do you happen to know if there is a test for the alternative scenario somewhere (i.e. where a set becomes a list)?,spmallette,-646 days,N,aboudreault
tinkerpop964,"I think this one is ok at this point - note the creation of https://issues.apache.org/jira/browse/TINKERPOP-2082 for other GLVs 

VOTE +1",spmallette,-646 days,N,aboudreault
wicket129,"I've added some comments - we have to upgrade to latest Wicket. I can help here if you want.

I don't see the change in BaseWicketTester in this PR. Please add it so we can review it too.

Thanks!
",martin-g,-1402 days,Y,phlbrz
wicket129,"The example/demo/test will be part of Wicket from now on so we have to adapt it (package names, versions, etc.).
I guess the example will be only in 7.x but the just improvement in BaseWicketTester will be in 6.x as well.
",martin-g,-1402 days,Y,phlbrz
wicket129,"Just minor nitpicks.
Looks good to me!
Thanks!
",martin-g,-1402 days,N,phlbrz
wicket129,"It seems your branch is too far from current master. I have problems to merge it locally.
Could you please rebase it and probably squash your commits into one?
Thanks!
",martin-g,-1402 days,Y,phlbrz
wicket129,"Many thanks, Felipe!
",martin-g,-1402 days,N,phlbrz
beam1903,👍 I think this is better than reverting the three commits.,aljoscha,35 days,N,kennknowles
beam1903,"Since Aljoscha LGTMed I will merge.



The jenkins failure is expected because of the archetype mess pre-stable API.",dhalperi,2 days,N,kennknowles
jena928,Looks good. Would you mind squashing this down to one commit please (it retains your name rather than us squash it which doesn't).,afs,-2351 days,Y,OyvindLGjesdal
jena928,"TDB (1 and 2) tests can run into issues. Memory mapped files on MS Windows do not get deleted from the file system until the JVM process exits This is a long standing Java bug.



The tests for Windows try to reuse database but it still leaves a significant over head.



Aslo, any crashes have before left dead directories (JUnit clean up not happening fully?).



If it causes problems, then just have Linux and MacOS - we have MS Windows testing on Jenkins . Adding MacOS to the GH action CI is still a step forward.",afs,-2351 days,Y,OyvindLGjesdal
jena928,"Thanks - merged.



We can see what happens when run in the Apache Software Foundation account.",afs,-2351 days,N,OyvindLGjesdal
hbase1711,":confetti_ball: **+1 overall**













| Vote | Subsystem | Runtime | Comment |

|:----:|----------:|--------:|:--------|

| +0 :ok: | reexec | 1m 30s | Docker mode activated. |

||| _ Prechecks _ |

| +1 :green_heart: | dupname | 0m 0s | No case conflicting files found. |

| +1 :green_heart: | hbaseanti | 0m 0s | Patch does not have any anti-patterns. |

| +1 :green_heart: | @author | 0m 0s | The patch does not contain any @author tags. |

||| _ master Compile Tests _ |

| +1 :green_heart: | mvninstall | 4m 9s | master passed |

| +1 :green_heart: | checkstyle | 1m 17s | master passed |

| +1 :green_heart: | spotbugs | 2m 9s | master passed |

||| _ Patch Compile Tests _ |

| +1 :green_heart: | mvninstall | 3m 46s | the patch passed |

| +1 :green_heart: | checkstyle | 1m 12s | the patch passed |

| +1 :green_heart: | whitespace | 0m 0s | The patch has no whitespace issues. |

| +1 :green_heart: | hadoopcheck | 12m 23s | Patch does not cause any errors with Hadoop 3.1.2 3.2.1. |

| +1 :green_heart: | spotbugs | 2m 16s | the patch passed |

||| _ Other Tests _ |

| +1 :green_heart: | asflicense | 0m 13s | The patch does not generate ASF License warnings. |

| | | 36m 20s | |





| Subsystem | Report/Notes |

|----------:|:-------------|

| Docker | Client=19.03.8 Server=19.03.8 base: https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-1711/1/artifact/yetus-general-check/output/Dockerfile |

| GITHUB PR | https://github.com/apache/hbase/pull/1711 |

| Optional Tests | dupname asflicense spotbugs hadoopcheck hbaseanti checkstyle |

| uname | Linux 42bd3610f948 4.15.0-74-generic #84-Ubuntu SMP Thu Dec 19 08:06:28 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |

| Build tool | maven |

| Personality | dev-support/hbase-personality.sh |

| git revision | master / 5e32e08782 |

| Max. process+thread count | 84 (vs. ulimit of 12500) |

| modules | C: hbase-server U: hbase-server |

| Console output | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-1711/1/console |

| versions | git=2.17.1 maven=(cecedd343002696d0abb50b32b541b8a6ba2883f) spotbugs=3.1.12 |

| Powered by | Apache Yetus 0.11.1 https://yetus.apache.org |





This message was automatically generated.",Apache-HBase,-25 days,N,binlijin
beam13245,"Thanks. Two other notes:

1. Do you think you can add a unit test in `source_test.go` that would check your functionality?

2. Please do not squash review changes. It makes it hard for a reviewer to follow up with what has changed. Instead, create a commit like ""fix: a short summary"". Those commits are squashed after review is complete.",kamilwu,-518 days,Y,tszerszen
jclouds100,"Did you make these changes in some automated fashion? I am not sure how I would review this in any sane way.



I would also caution that these kinds of formatting changes are harmless in one sense but don't deliver any user benefit. They also have a cost in maintainer/reviewer time that is small and shrinking.",gaul,-1413 days,Y,JnRouvignac
cxf775,@reta not sure I get the custom scope impl and need (@ApplicationScoped or @RequestScoped should do he same and properly impl get(bean)) but otherwise looks good,rmannibucau,311 days,N,reta
cxf775,"@reta hmm, both case are interesting but my point was that normal scoped beans are required to be proxied whereas others not. That said it also depends how CDI beans are proxies so maybe saner to test with a custom fake proxy not copying annotation since I suspect several CDI impl will copy them to not break frameworks, wdyt?",rmannibucau,311 days,Y,reta
fop8,All these PRs need a bug in fop jira with replication steps of issue,simonsteiner1984,0 days,N,Kui-Liu
Ant83,"I like the idea of checking the html files.
But why are these changes required? Only because a tool sais that is not enough for me. Do you have pointers to some specs?",janmaterne,-313 days,Y,twogee
Ant83,"Thanks for the links, sounds reasonable.
Two things I found:
1. maybe you should add a short paragraph in the &lt;description> section like for the other tools
2. why you named the target ""vnu""? Something like ""html-check"" sounds more descriptive. Compare to ""dependency-check"" instead of ""owasp""",janmaterne,-313 days,Y,twogee
Ant83,"Just summarize/quote the link contents:
- **doctype**: that declaration was used in HTML5 to distinguish between a standards-compliant parsing mode and a so-called quirks parsing mode.
- **&lt;meta http-equiv=""Content-Language"">**: Due to long-standing confusions and inconsistent implementations of this element, the HTML5 specification made this non-conforming in HTML, so you should no longer use it.
- **&lt;th scope=""row|col"">**: tell screenreaders exactly what cells the header is a header for",janmaterne,-313 days,Y,twogee
Ant83,"Tool ""Copyright (c) 2013-2018 Mozilla Foundation"" (https://github.com/validator/validator/blob/master/src/nu/validator/client/SimpleCommandLineValidator.java), so license seems to be ok.

Runs fine. Errors are printed to the console.

It seems that the tool does not support writing a report to file.
Could you try to create a report file with Ant (&lt;redirector>)? So this target would behave like the other report-targets.",janmaterne,-313 days,Y,twogee
Ant83,"This PR can't be responsible for the failing tests as the sources or test files are not touched.

Failing tests are:
src.tests.antunit.types.resources.test_xml.testhttpurl2
src.tests.antunit.taskdefs.get-test_xml.testTemporaryRedirect
src.tests.antunit.taskdefs.get-test_xml.testNestedResources
src.tests.antunit.taskdefs.get-test_xml.testInfiniteRedirect
src.tests.antunit.taskdefs.gunzip-test_xml.testWithNonFileResourceToDir
src.tests.antunit.taskdefs.gunzip-test_xml.testWithNonFileResourceToFile
",janmaterne,-313 days,Y,twogee
beam12996,"> R: @aaltay
> PTAL.
> 
> Let's talk offline to see if we need to create release branch and tags for a release.

Change LGTM. I am assuming you are planning to release this soon.

@kennknowles - Do we use branches or tags for releasing vendored dependencies? The release of this sub-directory will be similar to that. (I believe we do not do anything special other than a vote on dev list.)",aaltay,-1138 days,Y,KevinGG
beam12996,I would just use a tag. The only purpose of a release branch is cherrypicking and continuing to do point releases.,kennknowles,-1245 days,Y,KevinGG
beam12996,"I merged this but I could not push the tag. I got 403 errors when I run `git push https://github.com/apache/beam jupyterlab-sidepanel-v1.0.0`.

I thought I should be able to do this because I did this for releases previously. @kennknowles - do you know what I might be doing wrong?",aaltay,-1138 days,Y,KevinGG
beam12996,"Done: https://github.com/apache/beam/releases/tag/jupyterlab-sidepanel-v1.0.0

It worked using a personal access token instead of an ssh key (https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/creating-a-personal-access-token#using-a-token-on-the-command-line). Not sure why.",aaltay,-1138 days,Y,KevinGG
beam533,"I would still duplicate a WordCount copy into the Spark runner like I did in https://github.com/apache/incubator-beam/pull/539 because it's widely used in the runner's unit tests.
Maybe this could be removed after the runner is mature enough to rely only on the RunnableOnService tests.
And like I also said in https://github.com/apache/incubator-beam/pull/539, transitive dependency is your enemy here, I can't come up with something better than adding Spark provided/runtime dependencies.

This could be resolved by removing the provided scope on spark dependencies from the Spark runner, but I don't think that's a good idea. Looping in @jbonofre WDYT ? this could make the Spark runner Jar become very heavy.. and what about different Spark distributions on clusters ?
",amitsela,10 days,Y,peihe
beam533,"@peihe anything new here ? because https://github.com/apache/incubator-beam/pull/539 is passing tests now - but like you said, it doesn't eliminate code duplication.

I don't see this working if the runner doesn't have a `compile` scope dependency on the engine, and at least for Spark, I'm not sure it's the best way to go.

Pinging @jbonofre: from your experience with customers, is Spark usually provided ? 
",amitsela,10 days,Y,peihe
beam533,"While my point of view on things is of a Spark (+YARN) cluster, I'm starting to get the feeling that there are a lot of interest in ""out-of-the-box"" packaging..

Let me raise that in the mailing list to get people's thought on this, and I might change the build to either compile or use profiles or something.
",amitsela,10 days,Y,peihe
cxf746,"Thanks for the PR, @ropalka, I think we got to the point when it is probably make sense to completely separate Jakartaee and Javaee name handler chains (I think you would agree). I suggest to:
1. Introduce `org.apache.cxf.jaxws.handler.jakartaee` and `org.apache.cxf.jaxws.handler.javaee` packages
2. Introduce `JakartaeeHandlerChainBuilder` and `JavaeeHandlerChainBuilder`
3. Basically, I think it safe to assume that either JakartaEE or JavaEE namespace is going to be used, so in this case `AnnotationHandlerChainBuilder` could inspect the namespace, instantiate `JakartaeeHandlerChainBuilder` or`JavaeeHandlerChainBuilder`, and delegate to one of those. In this case, the conditionals should be gone and implementation becomes much cleaner. 

What do you think?",reta,-1186 days,Y,ropalka
cxf746,"Thanks for the reply @ropalka. We won't be changing the public API, which is outlined by `AnnotationHandlerChainBuilder`. The `JakartaeeHandlerChainBuilder` and `JavaeeHandlerChainBuilder` would encapsulate the implementation specifics to each namespace, and are not supposed to be used outside `AnnotationHandlerChainBuilder`. All the existing classes, under `org.apache.cxf.jaxws.handler.types` fe, could stay were they are. What do you think?",reta,-1186 days,Y,ropalka
cxf746,"> 
> 
> I enhanced this PR to be as much backward compatible as possible @reta.
> I introduced new _org.apache.cxf.jaxws.handler.jakartaee_ package as you proposed.
> I left _org.apache.cxf.jaxws.handler.types_ package untouched because of BC.
> I introduced JAXB deserialization adaptors to be able to eliminate HandlerChainBuilder
> 'jakartaee' methods I indroduced in previous pull request proposal. WDYT about it now?

@ropalka thank you for the effort, it definitely looks better but the main problem is still unsolved: the processing of the JavaEE and JakartaEE namespaces is tangled. May I ask you please to take at look at https://github.com/apache/cxf/pull/756, it is based of `master` (without your changes) but illustrates the idea of how the processing could be separated. AFAIK it does not change or alter any public APIs, only the implementation details have been moved around. Would appreciate to hear any concerns, thank you.",reta,-1186 days,Y,ropalka
cxf746,"@ropalka thank you, LGTM, I have a few minor comments which I could address myself if you prefer, otherwise - I think we are good to go",reta,-1186 days,N,ropalka
cxf746,"Thanks again for your work, @ropalka",reta,-1186 days,N,ropalka
cxf746,"I have tried 3.5.0-SNAPSHOT and miss a lot of jakarta.* packages. When do you plan to move to jakarta namespaces?
The change will not be backward compatible. What is the use of making it backward compatible?
I would create 4.0.0-SNAPSHOT and start jakarta namespace from there and keep 3.x.x javax namespace.

I am getting java.lang.NoClassDefFoundError: javax.annotation.Resource when using 3.5.0-SNAPSHOT.",lulseged,769 days,Y,ropalka
cxf746,@lulseged please follow https://github.com/apache/cxf/pull/737,reta,-1186 days,N,ropalka
flink2546,"All in all some minor change requests, otherwise this seems good.
",StephanEwen,-388 days,N,kl0u
flink2546,"Actually, let me take a step back and understand a few things deeper, first.
Who actually generates the watermarks (in ingestion time)? The operator that creates the file splits, or the operator that reads the splits?

If the configuration is set to IngestionTime, will the operator that creates the file splits emit a final LongMax watermark? Is that one passing through by the split-reading operator? Is there a test that test that specific scenario? (I believe it was the initially reported bug).
",StephanEwen,-388 days,Y,kl0u
flink2546,"I added some more comments. I could not find in that test anywhere the notion of checking that elements are not late, but properly interleaved with the watermarks.

Is there a test that checks that the reader does not let LongMax watermarks pass through? Or that the split generating task does not emit a long-max watermark on exit?

Also, is there a test that tests the interplay between the split-generator and the reader?

That would be important to have, I think.
",StephanEwen,-388 days,Y,kl0u
flink2546,"Just a quick comment (I didn't review all code): Why does this touch the AlignedWindowOperator tests? I would like to keep this commit as small as possible because we're dealing with sensitive stuff where I'd like to clearly separate things.

In `OneInputStreamOperatorTestHarness` and `KeyedOneInputStreamOperatorTestHarness`, restricting the time provider parameter to a `TestTimeServiceProvider` does not change anything, right? So I think we can leave it as is. Also in `OneInputStreamOperatorTestHarness` the additional `TimeCharacteristic` parameter is only useful for one specific test so I think it would be better to instead expose the `StreamConfig` and set the parameter there for the one test to keep the number of constructors manageable. 
",aljoscha,-361 days,Y,kl0u
groovy325,"+1 to the change itself, but I feel it does not solve GROOVY-7646, because that should have not been happening even without you removing classes from the cache actively. I can for example change the eval script to contain a class besides the script class and then it should blow up again. Furthermore, Eval just uses GroovyShell. So you should get the same problem with GroovyShell too. So in my opinion your fix is not addressing the underlying issue
",blackdrag,-139 days,Y,jwagenleitner
groovy325,"you are very right about the bean cache of course. It is a problem known for years... that and the stupid synchronization mechanism inside the Introspector. If you now can tell me the performance impact of this change, then I will soon be able to give my ok. 
",blackdrag,-139 days,Y,jwagenleitner
groovy325,"Is there anything in the way of merging this? We're doing something quite similar using reflection and it improves the memory leak situation dramatically.
",jochenberger,165 days,Y,jwagenleitner
groovy325,"what needs to be done to merge this is, remove the demo test case and resolve the small merge conflict
",blackdrag,-139 days,N,jwagenleitner
groovy325,"Could this please be merged then?
",jochenberger,165 days,N,jwagenleitner
groovy325,"I compile and `run()` script classes manually. When compiling, I keep track of the classes that are generated for a script and manually unload them using `InvokerHelper.removeClass()` afterward. Then I have to use reflection to remove them from `globalClassSet`/`globalClassValue`. So what I really need from this PR is the added `ClassInfo.remove()` method and its being called from `InvokerHelper.removeClass()`.
",jochenberger,165 days,Y,jwagenleitner
groovy325,"What are the plans for this now that #219 is merged? There should be much less of an issue, but it would still be nice to be able to eagerly evict classes from the cache when they are not used anymore instead of waiting for the GC to kick in.
",jochenberger,165 days,N,jwagenleitner
groovy325,"ClassInfo#remove is fine, adding it to InvokerHelper#removeClass is probably fine too. Doing this to Eval is probably working out, since you rarely will create a class within the eval and then using meta programming on it. For the other three this is different, but adding a flushCaches we could have. Of course if we use Closeable, we could use close instead. Adding this to GroovyShell and GroovyScriptEngine would be actually a good idea I think. GroovyClassLoader is, because it is extending URLClassloader, already closeable... so it could go there even without adding a new interface.
",blackdrag,-139 days,Y,jwagenleitner
groovy325,"Maybe we should split this into smaller chunks. I think we all agree about `ClassInfo#remove` and adding it to `InvokerHelper#removeClass`, so that could be one PR.
Using `flushCaches/close` in `GroovyShell`, `GroovyScriptEngine`, and `GroovyClassLoader` seems reasonable to me too. The latter would just do a `classCache.values().forEach(InvokerHelper::removeClass)`, right?
`GroovyShell` and `GroovyScriptEngine` would probably just have to `close()` their `GroovyClassLoader` from their `close()` methods then, but since they are both extensible public API, adding the `Closeable` interface would probably have to wait for 3.x.
",jochenberger,165 days,Y,jwagenleitner
groovy325,"I created PRs #444 and #445, I hope we can get these in for 2.4.7.
",jochenberger,165 days,N,jwagenleitner
jackrabbit19,"hi alfusainey

i just had a closer look at your lastest patch.
while i definitely see that your current approach makes things easy from a developer
point of view, it somehow defeats the purpose of having json-based remoting if
the complete json string is finally just converted to xml. that's the biggest concern
that i currently have with your approach.

apart from that, just a kind reminder with respect to the diffs: i would be really glad
if you could make sure that your patches don't contain modifications unrelated to
your work.. that also applies for the import statements.

kind regards
angela

From: Alfusainey <notifications@github.com<mailto:notifications@github.com>>
Reply-To: apache/jackrabbit <reply@reply.github.com<mailto:reply@reply.github.com>>
Date: Sunday 25 May 2014 02:36
To: apache/jackrabbit <jackrabbit@noreply.github.com<mailto:jackrabbit@noreply.github.com>>
Cc: anchela <angela@apache.org<mailto:angela@apache.org>>
Subject: Re: [jackrabbit] XML content import for remoting server (#19)

Reply to this email directly or view it on GitHubhttps://github.com/apache/jackrabbit/pull/19#issuecomment-44108291.
",anchela,54 days,Y,Alfusainey
jackrabbit19,"@Alfusainey while i see the benefit of having less redundant code, i think the additional serialize to xml and then in importXml deserialize again is a severe performance hit. can't you just use the same deserialized objects that represent the information and feed them into the same code?

another thing: to get the acl import working, i would not go about too heavy refactorings of jackrabbit. it will be much more difficult to convince people to accept a severe refactoring with all its potential of bugs or performance penalties over just handling acl nodes with a special handler, with almost no impact on the operations that where already supported.

this decision is up to @anchela and other jackrabbit maintainers but i guess they will be more at ease with less invasive changes. once you got your primary goal through, you can still look at jackrabbit and also oak and see if you want to propose further refactoring. but lets first focus on the primary goal of remoting acl information.
",dbu,54 days,Y,Alfusainey
jackrabbit19,"Hi Alfusainey,

> one change to the json-string: i now encode jcr properties as json objects instead of simple name:value(s) pairs

while I see that using objects for properties is more robust, it defies to purpose of using a simple, human readable format like json. for a machine-to-machine format we can use sysview XML. 

I would really try to use name/value pairs as properties. and encode the type either in the value or in the name (the later has the advantage, that you can define types on jcr mv-properties easier). eg

```
{
""jcr:lastModified@Date"": ""2014-06-10T14:30Z"",
""numbers@Long"": []
}
```

(btw: I'd prefer this discussion on the jackrabbit dev mailing list)
",tripodsan,69 days,Y,Alfusainey
jackrabbit19,"Hi Alfusainey,

rethinking this, we cannot change the format of the JSON for davex, as other clients also use it.
",tripodsan,69 days,Y,Alfusainey
jclouds50,"Failed with:

```
[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.12:check (default) on project jclouds: Too many files with unapproved license: 1 See RAT report in: /home/travis/build/apache/jclouds/target/rat.txt -> [Help 1]
```

Perhaps rat should ignore `.travis.yml`?",gaul,-928 days,N,nacx
jena72,"@osma - I'll find some time to review this but 

To your points:
1. I dont think that change is a problem and \@Deprecated isn't needed because (a) Jena3 allows us to be a bit more flexibility to do things right and (b) the major use is via SPARQL and that is not affected. I don't recall anyone asking about detailed direct use of the module.
1. IIRC solr works differently and puts in a ""score"" field in the results.
1. Yes - we already have enough old practices! I'll look at the PR for these. 

I'd like to proceed getting functionality in and worry about clearing up very soon after. It exposes the changes early for those interested.
",afs,-210 days,Y,osma
jena72,"There are a few `// *** score` markers left - presumably these can all go now.
",afs,-210 days,N,osma
jena72,"About changing return result from `DatasetGraphText.search`:

I'm unsure whether it is better to add `searchWithScore` (or better name?), which returns the `List<TextHit>` form, and retain `search` returning `List<Node>` (just a `Iter.map` added each time):

Example:

```
/** Search the text index on the default text field */
public Iterator<TextHit> searchWithScore(String queryString) {
return search(queryString, null) ;
}

/** Search the text index on the default text field */
public Iterator<Node> search(String queryString) {
return Iter.map(searchWithScore(queryString, textHit->textHit.getNode()) ;
}
```

If we do make a change, then Jena3 is the time to do it.

What do you think?
",afs,-210 days,Y,osma
jena72,"Re: Map1Iterator -- yes is it similar (and older). `Iter` works on any iterators, not just `ExtendedIterator`.

Re: naming of search - your choice. It is alwayss difficult to choose between compatibility and creating legacy.
",afs,-210 days,Y,osma
jena72,"`Iter` is also a good choice because it provides access to transform-behavior as well as many other forms of iterator-processing. `Map1Iterator` is more of an ""implementation"" class. As the code evolves, I would expect `Iter` to last longer and be better supported.
",ajs6f,20 days,Y,osma
jena72,"> Maybe @amiara514 would have a comment as I understood he is using jena-text via Java code? 

@osma : Not exactly, I execute Sparql queries via java code which involve jena-text. I don't manipulate it at this level.
",amiara514,-8 days,N,osma
jena72,"Merge done! Thank you.
",afs,-210 days,N,osma
jmeter390,"Thanks for your contribution, but I don't think this is the correct solution.
If we only update the jars, the code will not compile anymore. There are a lot of missing and deprecated methods and classes. I tried to update our usage of the api to the current one, but that is not possible without a lot of work. (The authentication mechanism has changed and the eval mechanism, that we depend on has been deprecated/removed).
As the mongo sampler has been deprecated with JMeter 3.0. So a better patch would be to remove the client jars altogether.",FSchumacher,-1131 days,Y,IgorSasovets
jmeter390,"When I compile your PR with ```ant download_jars clean install```, I get the following:
```
compile-mongodb:
[javac] Compiling 9 source files to /xxx/jmeter/build/protocol/mongodb
[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/config/MongoSourceElement.java:117: error: cannot find symbol
[javac] .autoConnectRetry(getAutoConnectRetry())
[javac] ^
[javac] symbol: method autoConnectRetry(boolean)
[javac] location: class Builder
[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/config/MongoSourceElement.java:130: error: no suitable constructor found for WriteConcern(int,int,boolean,boolean,boolean)
[javac] builder.writeConcern(new WriteConcern(
[javac] ^
[javac] constructor WriteConcern.WriteConcern() is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] constructor WriteConcern.WriteConcern(int) is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] constructor WriteConcern.WriteConcern(String) is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] constructor WriteConcern.WriteConcern(int,int) is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] constructor WriteConcern.WriteConcern(boolean) is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] constructor WriteConcern.WriteConcern(int,int,boolean) is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] constructor WriteConcern.WriteConcern(int,int,boolean,boolean) is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] constructor WriteConcern.WriteConcern(String,int,boolean,boolean) is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] constructor WriteConcern.WriteConcern(Object,Integer,Boolean,Boolean) is not applicable
[javac] (actual and formal argument lists differ in length)
[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/mongo/MongoDB.java:53: error: cannot find symbol
[javac] boolean authenticated = db.isAuthenticated();
[javac] ^
[javac] symbol: method isAuthenticated()
[javac] location: variable db of type DB
[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/mongo/MongoDB.java:57: error: cannot find symbol
[javac] authenticated = db.authenticate(username, password.toCharArray());
[javac] ^
[javac] symbol: method authenticate(String,char[])
[javac] location: variable db of type DB
[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/sampler/MongoScriptRunner.java:55: error: cannot find symbol
[javac] db.requestStart();
[javac] ^
[javac] symbol: method requestStart()
[javac] location: variable db of type DB
[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/sampler/MongoScriptRunner.java:57: error: cannot find symbol
[javac] db.requestEnsureConnection();
[javac] ^
[javac] symbol: method requestEnsureConnection()
[javac] location: variable db of type DB
[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/sampler/MongoScriptRunner.java:66: error: cannot find symbol
[javac] db.requestDone();
[javac] ^
[javac] symbol: method requestDone()
[javac] location: variable db of type DB
[javac] Note: Some input files use or override a deprecated API.
[javac] Note: Recompile with -Xlint:deprecation for details.
[javac] 7 errors
```
That means, that the current code doesn't work with the new jars.

If you want to update the code the new jars, go ahead, but as of JMeter 3.0 the sampler has been deprecated and it should have been removed with version 3.1 (which obviously didn't happen).

It would be interesting to see your JSR223 sampler. I suspect it handles all of the connection setup and client usage itself. Maybe it would be a good idea to discuss your use case and solution on the mailing list.",FSchumacher,-1131 days,Y,IgorSasovets
jmeter390,"You can reach me (and other JMeter users/developers) on the [JMeter mailing list](https://jmeter.apache.org/mail2.html#JMeterDev)

I hacked together an updated version of the Mongo Sampler at https://github.com/FSchumacher/jmeter/tree/pr-390-mongo, but I am not sure, that it is a good idea, as it will break all old test plans.

And note, that I don't use MongoDB, so I am not sure, if it is correct at all.",FSchumacher,-1131 days,Y,IgorSasovets
jmeter390,"Hi Felix,
My opinion is that if you did the upgrade it’s ok to break old plan provided we clearly state we did this.

It is fair that we upgrade now a very old version of MongoDB API and drop deprecated code.

I think we had some complaints about this very old driver version, missing new auth methods both on mailing list and surely on stackoverflow.

So it will be a good thing to upgrade. 
Regards",pmouawad,-1302 days,Y,IgorSasovets
jmeter390,"Maybe revive old idea of extracting MongoDB components into external plugin? Custom plugin detaches version from core JMeter, which means some users can use older version of plugin, while rest of JMeter can be upgraded to newer.",undera,-1305 days,Y,IgorSasovets
jmeter390,"I always build with Oracle JDK 8 (u171 at the moment). ant version is 1.10.3. 
I suspect that you have an old mongo db jar lying around in your build path.",FSchumacher,-1131 days,N,IgorSasovets
jmeter544,"# [Codecov](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=h1) Report
> Merging [#544](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=desc) into [master](https://codecov.io/gh/apache/jmeter/commit/231436138836aafd9f697896cdc9b442faa4e8d2?src=pr&el=desc) will **increase** coverage by `0.04%`.
> The diff coverage is `79.12%`.

[![Impacted file tree graph](https://codecov.io/gh/apache/jmeter/pull/544/graphs/tree.svg?width=650&token=6Q7CI1wFSh&height=150&src=pr)](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=tree)

```diff
@@ Coverage Diff @@
## master #544 +/- ##
============================================
+ Coverage 55.33% 55.37% +0.04% 
- Complexity 9960 9976 +16 
============================================
Files 1033 1035 +2 
Lines 63345 63399 +54 
Branches 7157 7159 +2 
============================================
+ Hits 35052 35109 +57 
+ Misses 25818 25815 -3 
Partials 2475 2475
```


| [Impacted Files](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=tree) | Coverage Δ | Complexity Δ | |
|---|---|---|---|
| [...lizers/backend/influxdb/InfluxdbMetricsSender.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL2luZmx1eGRiL0luZmx1eGRiTWV0cmljc1NlbmRlci5qYXZh) | `100% <ø> (ø)` | `0 <0> (ø)` | :arrow_down: |
| [...alizers/backend/AbstractBackendListenerClient.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL0Fic3RyYWN0QmFja2VuZExpc3RlbmVyQ2xpZW50LmphdmE=) | `23.07% <ø> (+1.64%)` | `3 <0> (ø)` | :arrow_down: |
| [...ackend/influxdb/InfluxdbBackendListenerClient.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL2luZmx1eGRiL0luZmx1eGRiQmFja2VuZExpc3RlbmVyQ2xpZW50LmphdmE=) | `39.91% <0%> (+0.35%)` | `12 <0> (ø)` | :arrow_down: |
| [...visualizers/backend/influxdb/UdpMetricsSender.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL2luZmx1eGRiL1VkcE1ldHJpY3NTZW5kZXIuamF2YQ==) | `0% <0%> (ø)` | `0 <0> (ø)` | :arrow_down: |
| [...ter/visualizers/backend/BackendListenerClient.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL0JhY2tlbmRMaXN0ZW5lckNsaWVudC5qYXZh) | `0% <0%> (ø)` | `0 <0> (?)` | |
| [...end/influxdb/InfluxDBRawBackendListenerClient.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL2luZmx1eGRiL0luZmx1eERCUmF3QmFja2VuZExpc3RlbmVyQ2xpZW50LmphdmE=) | `100% <100%> (ø)` | `14 <14> (?)` | |
| [...isualizers/backend/influxdb/HttpMetricsSender.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL2luZmx1eGRiL0h0dHBNZXRyaWNzU2VuZGVyLmphdmE=) | `76.84% <70%> (-0.34%)` | `11 <1> (+1)` | |
| [...er/visualizers/backend/BackendListenerContext.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL0JhY2tlbmRMaXN0ZW5lckNvbnRleHQuamF2YQ==) | `23.68% <0%> (ø)` | `7% <0%> (ø)` | :arrow_down: |
| ... and [1 more](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree-more) | |

------

[Continue to review full report at Codecov](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=footer). Last update [2314361...2a4737a](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
",codecov-io,627 days,N,ham1
jmeter544,"I'm not sure it is worth adding yet another Listerer implementation.

It looks like the users would have to copy-paste the configuration if they want to switch between raw and grouped data.",vlsi,-164 days,Y,ham1
jmeter544,"In a parallel universe, there's https://gitlab.com/testload/jmeter-listener (MIT)",vlsi,-164 days,N,ham1
jmeter544,"> I'm not sure it is worth adding yet another Listerer implementation.

IMO this one complements the other one and will be more precise
> 
> It looks like the users would have to copy-paste the configuration if they want to switch between raw and grouped data.

Is this a big deal ? 
Will this really happen ?",pmouawad,-428 days,N,ham1
jmeter544,"> In a parallel universe, there's https://gitlab.com/testload/jmeter-listener (MIT)

Have you tried it ? 
I think it's better to have things in CORE for what seems to be a ""core"" feature, making live metrics available.
Not all users of JMeter are aware of 3rd party plugins.",pmouawad,-428 days,Y,ham1
jmeter544,">Is this a big deal ?

I think it is.
From my perspective, the use case is: ""user wants to send the data to InfluxDB"".
The way to send the data looks more like a configuration option rather than selecting a different listener.

For instance, what if we add `BackendListener that sends percentiles` and `BackendListener that sends errors`?
It does look not user-friendly, especially, taking into account that JMeter does not allow copy-pasting multiple fields.

>IMO this one complements the other one and will be more precise

Having 3-4 ""similar in-core listeners that send data to InfluxDB"" would be confusing.",vlsi,-164 days,Y,ham1
jmeter544,">I think it's better to have things in CORE for what seems to be a ""core"" feature, making live metrics available.

One of the options is to bundle the plugin into the final distribution (like we bundle lots of third-party dependencies).

I have not tried that myself, however, their [wiki says](https://gitlab.com/testload/jmeter-listener/-/wikis/1.-Main) there are reasons to group data before sending:

> Activation of GROUP_BY function (only couple parameters in settings) could be very useful on highload (thousands of transaction per second) because on such intensity InfluxDB/ElasticSearch/Clickhouse can be bottleneck (IOPS). In this mode all Samplers grouping by name and only aggregates",vlsi,-164 days,Y,ham1
jmeter544,"> > I think it's better to have things in CORE for what seems to be a ""core"" feature, making live metrics available.
> 
> One of the options is to bundle the plugin into the final distribution (like we bundle lots of third-party dependencies).
> 
> I have not tried that myself, however, their [wiki says](https://gitlab.com/testload/jmeter-listener/-/wikis/1.-Main) there are reasons to group data before sending:
> 
> > Activation of GROUP_BY function (only couple parameters in settings) could be very useful on highload (thousands of transaction per second) because on such intensity InfluxDB/ElasticSearch/Clickhouse can be bottleneck (IOPS). In this mode all Samplers grouping by name and only aggregates

This plugin has indeed good ideas, but implementation should be improved if embedded in JMeter (https://gitlab.com/testload/jmeter-listener/-/issues).
",pmouawad,-428 days,N,ham1
jmeter544,"Hello @ham1 ,

Regarding this PR, did you have the opportunity to test it on a ""real life"" performance test ? 

What were the use cases ? and related volumes ?

Thanks",pmouawad,-428 days,Y,ham1
jmeter544,"Is there a way to submit the raw requests in batches?
If performance is decent, we'll try this.
",eostermueller,1752 days,N,ham1
jmeter258,"Hello,
Thanks for PR.
I'll look into merging it but what exactly is the 3rd point ? Is it to avoid sending to INflux the Validation results ?
Usually it's better to split PR in this case.

Thanks",pmouawad,-466 days,Y,max3164
jmeter258,"I tried the patch and I don't see annotations displayed.
I tried with:
eventTags = TestTag
And 
eventTags = Test tag for release version=1.2.3

[
<img width=""789"" alt=""screen shot 2017-01-26 at 15 13 04"" src=""https://cloud.githubusercontent.com/assets/3127467/22334124/cd99ecce-e3d9-11e6-903a-19f18532a3c3.png"">
<img width=""352"" alt=""screen shot 2017-01-26 at 15 12 45"" src=""https://cloud.githubusercontent.com/assets/3127467/22334125/cdba8268-e3d9-11e6-8fe4-be27fd0502ad.png"">

](url)",pmouawad,-466 days,N,max3166
jmeter258,"Hi,
You were right.
I have a second possible issue.
Here is what the meanAT shows. It does not look ok to me .
<img width=""1418"" alt=""screen shot 2017-01-26 at 15 51 03"" src=""https://cloud.githubusercontent.com/assets/3127467/22335641/c6c20a2a-e3df-11e6-9540-626affe26f89.png"">

",pmouawad,-466 days,Y,max3168
jmeter258,"This is explained by:
- https://bz.apache.org/bugzilla/show_bug.cgi?id=60650",pmouawad,-466 days,N,max3170
jmeter556,"@vlsi , in this case +1 for switching",pmouawad,-264 days,N,vlsi
jmeter556,"Unless I misunderstand your question It does, JMeter does not start:

- 10.11.16

Stacktrace:

`2020-03-14 22:40:32,223 INFO o.a.j.JMeter: Setting LAF to: com.github.weisj.darklaf.DarkLaf:com.github.weisj.darklaf.theme.DarculaTheme
2020-03-14 22:40:32,309 ERROR o.a.j.JMeter: An error occurred: 
java.lang.UnsatisfiedLinkError: /private/var/folders/72/68dyl2ns1q37l4f0p5yfmg4w0000gn/T/nativeutils61999062531814/libdarklaf-macos.dylib: dlopen(/private/var/folders/72/68dyl2ns1q37l4f0p5yfmg4w0000gn/T/nativeutils61999062531814/libdarklaf-macos.dylib, 1): Symbol not found: _NSAppearanceNameDarkAqua
Referenced from: /private/var/folders/72/68dyl2ns1q37l4f0p5yfmg4w0000gn/T/nativeutils61999062531814/libdarklaf-macos.dylib
Expected in: /System/Library/Frameworks/AppKit.framework/Versions/C/AppKit
in /private/var/folders/72/68dyl2ns1q37l4f0p5yfmg4w0000gn/T/nativeutils61999062531814/libdarklaf-macos.dylib
at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_201]
at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) ~[?:1.8.0_201]
at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824) ~[?:1.8.0_201]
at java.lang.Runtime.load0(Runtime.java:809) ~[?:1.8.0_201]
at java.lang.System.load(System.java:1086) ~[?:1.8.0_201]
at com.github.weisj.darklaf.platform.NativeUtil.loadLibraryFromJar(NativeUtil.java:108) ~[darklaf-native-utils-1.4.1.0.jar:1.4.1.0]
at com.github.weisj.darklaf.platform.macos.JNIDecorationsMacOS.loadLibrary(JNIDecorationsMacOS.java:76) ~[darklaf-macos-1.4.1.0.jar:1.4.1.0]
at com.github.weisj.darklaf.platform.macos.JNIDecorationsMacOS.updateLibrary(JNIDecorationsMacOS.java:65) ~[darklaf-macos-1.4.1.0.jar:1.4.1.0]
at com.github.weisj.darklaf.platform.macos.MacOSDecorationsProvider.initialize(MacOSDecorationsProvider.java:49) ~[darklaf-macos-1.4.1.0.jar:1.4.1.0]
at com.github.weisj.darklaf.platform.Decorations.initialize(Decorations.java:70) ~[darklaf-core-1.4.1.0.jar:1.4.1.0]
at com.github.weisj.darklaf.DarkLaf.setupDecorations(DarkLaf.java:147) ~[darklaf-core-1.4.1.0.jar:1.4.1.0]
at com.github.weisj.darklaf.DarkLaf.getDefaults(DarkLaf.java:120) ~[darklaf-core-1.4.1.0.jar:1.4.1.0]
at javax.swing.UIManager.setLookAndFeel(UIManager.java:539) ~[?:1.8.0_201]
at javax.swing.UIManager.setLookAndFeel(UIManager.java:583) ~[?:1.8.0_201]
at com.github.weisj.darklaf.LafManager.install(LafManager.java:171) ~[darklaf-core-1.4.1.0.jar:1.4.1.0]
at com.github.weisj.darklaf.LafManager.installTheme(LafManager.java:140) ~[darklaf-core-1.4.1.0.jar:1.4.1.0]
at org.apache.jmeter.gui.action.LookAndFeelCommand.activateLookAndFeel(LookAndFeelCommand.java:211) ~[ApacheJMeter_core.jar:5.3-SNAPSHOT]
at org.apache.jmeter.JMeter.startGui(JMeter.java:377) ~[ApacheJMeter_core.jar:5.3-SNAPSHOT]
at org.apache.jmeter.JMeter.start(JMeter.java:544) [ApacheJMeter_core.jar:5.3-SNAPSHOT]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_201]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_201]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_201]
at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_201]
at org.apache.jmeter.NewDriver.main(NewDriver.java:252) [ApacheJMeter.jar:5.3-SNAPSHOT]
`",pmouawad,-264 days,N,vlsi
jmeter556,I sent it to you,pmouawad,-264 days,N,vlsi
jmeter556,@pmouawad I would really appreciate if you could test out [this branch](https://github.com/weisJ/darklaf/pull/72) of darklaf on your machine and tell me whether you are able to load the native library.,weisJ,1662 days,Y,vlsi
jmeter556,"@weisJ I have just tested it and it works now with your last version, Thank you !

@vlsi , thanks for work on this, the result is nice",pmouawad,-264 days,N,vlsi
jmeter556,"@weisJ , I just tested jmeter, I am commenting on https://github.com/weisJ/darklaf/pull/72 now",pmouawad,-264 days,N,vlsi
karaf336,R: cschneider,cschneider,-1435 days,N,rovarga
karaf336,Can you create the PR against master? We only merge to master and then backport.,cschneider,-1435 days,Y,rovarga
karaf336,Apart from that the PR looks good.,cschneider,-1435 days,N,rovarga
karaf336,I also backported to the karaf 4.1.x branch now,cschneider,-1435 days,N,rovarga
karaf160,"> - Use only $(..) instead of both $(...) and `...`

What do you think about:
- Use only ${VAR} instead of both ${VAR} and $VAR
",maggu2810,-170 days,N,lkiesow
karaf160,"Cool :wink: ($0 could be also replaced by ${0}).

I hope that the other ones like this too.
I do not have a strong feeling about that (`...` and $(...); $VAR and ${VAR}), too.
But I think it is good to be consistent.
",maggu2810,-170 days,Y,lkiesow
karaf160,"Changing /bin/sh to /bin/bash means that we can break support one some system (for instance Solaris by default). Is it really required ?
",jbonofre,-1309 days,N,lkiesow
karaf160,"It makes sense. Less than Posix, the purpose is that Karaf scripts work on most Unix system as possible (now they work on Linux, MacOS, Solaris, AIX, *BSD). Let me review your proposal and see how it works on different VM.
Thanks.
",jbonofre,-1309 days,Y,lkiesow
karaf160,"Can you please rebase your PR as it has conflicts with the repo ? Thanks !
",jbonofre,-1309 days,N,lkiesow
karaf160,"No rush, whenever you have time. Thanks !
",jbonofre,-1309 days,N,lkiesow
karaf160,"Sorry, let me double check to merge. Thanks !
",jbonofre,-1309 days,N,lkiesow
karaf1243,"Merged and backported to 4.2.x, thanks!",skitt,-1616 days,N,Sachpat
mesos247,"@mpereira Oh I know what's going on:

> Please make sure Java proto files are generated in ../build/src/java/generated folder.

The site generator expects the build to have been run, at least to the point of proto files being generated.

Try

```
mkdir build
cd build
cmake -DENABLE_JAVA=ON ..
cmake --build . --target mesos-protobufs
```

And then try generating the site again, I think it'll work.",andschwa,-9 days,Y,mpereira
mesos247,"You can use Autotools too, I just don't know what the target is, and don't like to wait around for the entire build 😛",andschwa,-9 days,Y,mpereira
mesos203,"@houht: Was this intended to be a bug report? If you, could you head over to https://issues.apache.org/jira/browse/MESOS and file a ticket there?",bbannier,-217 days,N,houht
phonenix1064,"I am generally not convinced that overriding poolsize from the client side is a good idea.
It is very easy to overload the cluster by specifying a too large poolsize.
IMO it's better to leave this knob in the cluster administrator's hand.",stoty,-330 days,Y,wangchao317
phonenix1064,"Also see my objection from the ticket that this will cause discrepancy between the poolsize used by the client, and by the Index rebuilding initiated by MetaDataObserver.",stoty,-330 days,N,wangchao318
phonenix1064,"I agree @wangchao316 that tuning poolsize is crucial for performance optimization.
It is just my opinion that the tuning should be done on the cluster size, in hbase-site.xml, rather than on the client side.",stoty,-330 days,Y,wangchao321
qpid3,"Hi Emmanuel,

Thanks for this. If you could make a couple of tiny changes I'll get this committed:
- The renamed geronimo-servlet_3.0_spec-1.0.xml spec needs updated with the new dependency info, it still contains the details for the older version (we are working on an actual Maven build, which will allow removing this hideous generation process...).
- Can you add the JIRA reference and attribute yourself in the commit message. As far as I understand it the Pull Request wont be closed if we change the message (which is why the others are still open, I'll need to request that infra close them as we cant). For example:

QPID-5527: Upgrade to Jetty 8

Pull Request from Emmanuel Bourg.
",gemmellr,-186 days,Y,ebourg
qpid3,"This was (accidentally?) closed as a casualty of my request for infra to close the 2 previous stale pull requests that I couldnt (via https://issues.apache.org/jira/browse/INFRA-7253).

Still interested :)
",gemmellr,-186 days,N,ebourg
qpid3,"@gemmellr yes, I closed out all open pull requests.

@ebourg thank you for this patch and your work on this issue. Currently Apache Qpid can not accept pull requests as this is just a mirror. If you could please submit a patch back to the jira project we would appreciate it.
",jfarrell,1 days,Y,ebourg
qpid3,"@jfarrell Understood (and thank you!). I thought I had read long ago that the Pull Request would close automatically so long as we didn't change the hash by altering the message when we commit it. Emmanuel had also raised https://issues.apache.org/jira/browse/QPID-5527 to cover the request, so we can move over there.
",gemmellr,-186 days,Y,ebourg
spark1199,"Merged build triggered. 
",AmplabJenkins,-9 days,N,liancheng
spark1199,"Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/16073/
",AmplabJenkins,-9 days,N,liancheng
spark1199,"Merged build triggered. 
",AmplabJenkins,-9 days,N,liancheng
spark1199,"Aren't we using Scala's BigDecimal? 
",rxin,-10 days,N,liancheng
spark1199,"@rxin we are probably using scala's but this code is unwrapping values that come from hive.
",marmbrus,-8 days,N,liancheng
spark1199,"SGTM. We should probably switch to this: https://issues.apache.org/jira/browse/HIVE-6017 although it is a lower priority task (although for the purpose of API compatibility, maybe we want to make it higher priority0.
",rxin,-10 days,Y,liancheng
spark1199,"All automated tests passed.
Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/16090/
",AmplabJenkins,-9 days,N,liancheng
spark1199,"Merged build finished. 
",AmplabJenkins,-9 days,N,liancheng
spark1199,"Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/16095/
",AmplabJenkins,-9 days,N,liancheng
spark1199,"I'm going to merge this first since the test is most likely a different problem. 
",rxin,-10 days,N,liancheng
storm705,"A few minor nits:

When providing javadoc on variables/methods, be sure to use the following syntax:

`/**`
`* Comment text`
`**/`
`private String variableName;`

Instead of the following:

`// offset state information storage. validate options are storm and kafka`
`private String variableName;`

The first variation will provide fly-over-help within IDE and generated Javadoc files.

Secondly:
Logger instance variables should be private.

Thirdly:
Suggest instance variables on PartitionManager, StaticCoordinator ZkDataStore be made private.
",rmkellogg,-183 days,Y,hsun-cnnxty
storm705,"how will this change affect the format of spout's offset message in zookeeper?
",sweetest,-230 days,N,hsun-cnnxty
storm705,"This seems like it might not be backwards compatible with the existing kafka-spout; i.e., the offsets in ZK are presumably not going to be stored exactly the same as they were before. Is there any plan for supporting migration from the current kafka-spout to this one?
",erikdw,18 days,Y,hsun-cnnxty
storm705,"Furthermore, can you please provide a reference to ""Kafka's consumer offset management api"" in your description?
",erikdw,18 days,N,hsun-cnnxty
storm705,"@hsun-cnnxty : <s>I don't see the reference links in the Description on the PR's Conversation view? Maybe I'm looking in the wrong place? Maybe they should be in comments in the code too?</s>
Yup, was looking in the wrong place, I didn't realize that ""the description"" meant the **JIRA issue**'s Description! 

I'm guessing the following are the links you meant to put:
- https://cwiki.apache.org/confluence/display/KAFKA/Committing+and+fetching+consumer+offsets+in+Kafka
- https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol

I just want it to be very clear that [this PR](https://github.com/apache/storm/pull/705)'s purpose is to change from the kafka spout's consumer offsets being stored in ZooKeeper, to instead being stored directly in Kafka. We should also be clear about the version of Kafka required for such support (0.8.2+). I know the current version of the storm-kafka pom.xml (as of this change) is already referencing 0.8.2.1, but I feel like it should be called out as an explicit requirement in the commit.
",erikdw,18 days,Y,hsun-cnnxty
storm705,"I recommend making your abstraction at the state store, so you would have:

```
public interface StateStore {
public void write(Partition p, long offset);
public long read(Partition p);
}

public class ZkStateStore implements StateStore {
...
}

public class KafkaStateStore implements StateStore {
...
}

/**
Some topologies read from LATEST after a restart, so only memory state is needed.
*/
public class MemoryStateStore implements StateStore {
...
}
```

You would not need different PartitionStateManager for different stores. You would just have:

```
public class PartitionStateManager {
public PartitionStateManager (..., StateStore store) { ... }
public void writeState(...) {
store.write(...);
}
}
```
",choang,32 days,Y,hsun-cnnxty
storm705,"I would make state more concrete, but I suppose your approach is fine.
",choang,32 days,N,hsun-cnnxty
storm705,"@hsun-cnnxty : can you please make your statement a bit more concrete? i.e., what other info is stored in a given kafka topic's consumer state? (other than the offset/partition)
",erikdw,18 days,Y,hsun-cnnxty
storm705,"looks really good. just had a few nits that shouldn't block. I also don't have a vote, so you'll need to find a proper reviewer :)
",choang,32 days,N,hsun-cnnxty
storm705,"`StateStore` implementations could be designed like:

```
public class KafkaStateStore implements StateStore {
public KafkaStateStore(HostPort kafkaBroker, String consumerId) { // consumerId could be topic? I'm don't know enough about the compaction feature
...
}
...
}

public class ZkStateStore implements StateStore {
public ZkStateStore(HostPort zkConnect, String path) {
...
}
}

// future implementations
public class JdbcStateStore implements StateStore {
public JdbcStateStore(String url, String stateTable, String user, String password) {
...
}
}

public class MemoryStateStore implements StateStore {
public MemoryStateStore() {
...
}
}
```

What I'm trying to illustrate are two factors:
1) go away from `StormConf` and `SpoutConfig` because `StateStore` does not need all properties of either objects.
2) the concrete implements of `StateStore` do not need to have the same constructor because your goal is not to have a factory and make it completely config driven. You want to make it so the topology developer can decide what `StateStore` to use.

With the above, it would be pretty easy for someone to implement `MemoryStateStore` and `JdbcStateStore`.
",choang,32 days,Y,hsun-cnnxty
storm705,"this looks good enough
",choang,32 days,N,hsun-cnnxty
storm705,"Any updates on the status of this merge? This update would be very helpful
",AwesomeJohnR,179 days,N,hsun-cnnxty
storm1468,"I'm a little on the fence in terms of squashing the commits of others vs. asking the contributor to do so. There are a lot of situations where spreading out a big patch over multiple commits makes sense and makes the history more consumable. 

A couple of questions:
- How does this preserve authorship in a pull request that has commits from multiple authors?
- How would this work with our current branch model? Specifically, applying a pull request to multiple branches.
",ptgoetz,-119 days,Y,harshach
storm1468,"> It will ask for primary authors and the user who is merging this can input more than one author at the time of merge.

That means it removes authorship information. If we tag a squashed commit as coming from multiple authors, we still wouldn't be able to differentiate what code was contributed by the individual authors.

So if I merged a pull request with multiple authors, the result would be a single commit from me with a message listing the contributing authors, is that correct?
",ptgoetz,-119 days,Y,harshach
storm1468,"1. What's the difference between Spark script vs Kafka script?
Spark script is origin of Kafka and Zeppelin, so unless there're specific improvements, I think picking Spark script is more promising. For example, `trunk` is often not used for git project but Kafka is using that.
2. We're using branches which doesn't fully represent current version for branch. So our script should determine version more smart. One way to determine is looking at pom.xml.
",HeartSaVioR,83 days,Y,harshach
storm1468,"Actually I was the one which claims separated credits from other project. (https://github.com/OpenTSDB/asynchbase/pull/122)

But there was a strong reason to do so, and I think it's not the normal case we can see it often. As I addressed from mailing list, many big Apache projects already used this approach.

If there're cases which squashing really hurts then we can have exceptional case.
",HeartSaVioR,83 days,Y,harshach
storm1468,"@harshach Yeah, I don't know what things Kafka improve from Spark script so I wanted to see the benefit if you know about it. As I commented earlier, just adopting script doesn't work since we use different branch model (master, minor, bugfix) so it should be fully tested (including JIRA integration) before adopting.
",HeartSaVioR,83 days,Y,harshach
storm1468,"@harshach 
I skimmed a bit, and guess determining fix version will not work since branch names we use are different from Spark and Kafka and so on. We can still input them by hand so there's no issue on it but if we modify it to fit for Storm that would be great. (optional)

FYI: Spark script is having same issue.
https://github.com/apache/spark/blob/master/dev/merge_spark_pr.py
",HeartSaVioR,83 days,Y,harshach
storm1468,"> We won't be able capture this in JIRA either. I am not sure how much of this is important to have all the commits from each contributor for a single JIRA which in itself is rare unless its a big patch. It does have ability to give each contributor credit in the commit log.

From a legal perspective it's very important that we be able to track the provenance of all code that lands in an ASF repository and could potentially be released.

For example: 

Bob is a committer. Alice and Charles are not. Alice and Charles collaborate on a patch, both making commits. In the process Charles commits some code that he doesn't have the legal rights to (its proprietary, etc.). Later Bob uses this script to merge the pull request, and squash all the commits. Alice and Charles are listed as authors of the patch, but there is no history regarding how the code that the ASF doesn't have rights to get there. Was it Charles or Alice?

That may seem like an edge case, but one that we should absolutely consider.
",ptgoetz,-119 days,Y,harshach
storm1468,"@harshach The source of the file is referenced here:
https://github.com/apache/storm/pull/1468/files#diff-da45fe3972445a9f82ef768808dd8853R20

I'd like to get clearance that what this script does or enables is okay before proceeding. 
",ptgoetz,-119 days,Y,harshach
storm1468,"I'd really like to go forward with automated tools for developers / committers. What I've stated from dev@ mailing list, many projects already use specific tools for merging, and the merge script originated from Spark is well used for Spark, Kafka, Zeppelin (now TLP).
",HeartSaVioR,83 days,Y,harshach
storm1468,"I'll take a deep look and describe what this script actually does.
",HeartSaVioR,83 days,N,harshach
storm1468,"Here's my understanding regarding this script.

> main()
- get latest branch (highest version) from github mirror
- it assumes that prefix of release branch is `release version`
- get information regarding pull request and events of pull request
- prompt user(committer) to input commit title: default value is title of pull request, and user can replace them
- standardize commit title (via standardize_jira_ref) 
- prompt user to use title as modified vs original
- check that pull request is already merged (closed by asfgit)
- if it is, check merge commit is fetched to local, and cherry-pick to latest branch assuming user wants backport
- prompt user to continue if PR seems to resolve conflicts (by seeing flag from PR information)
- print information of pull request, and commit title, and so on, and prompt user to go on
- merge PR into target branch of PR: get commit hash afterwards
- prompt user to see user wants cherry-pick
- if yes, cherry-pick to latest branch
- prompt user to update associated JIRA
- if yes, resolve issue as fixed with leaving comment

> merge_pr()
- fetch branch which pull request is referring
- fetch and checkout branch which pull request targets (from asf-git)
- merge pull request branch with squash option
- if there're some conflicts, guide user to fix it and mark as resolved (via git add)
- prompt user to input main author: default value is who has most of commits (via extracting authors from pull request branch and sort)
- prompt user to input reviewers: can be blank
- prompt user to see user wants to list all commits into commit message
- construct commit message by
- commit title
- body of pull request if presented
- all authors
- all reviewers if presented
- user if user resolves merge conflict manually
- auto close message for pull request
- list of commits if user want to
- commit with passing main author as author, and commit message
- prompt user to push, or stop
- push changes to remote repository (to asf-git)
- clean temporary branches
- get commit hash and return

> cherry_pick()
- prompt user to input branch name to cherry-pick: default value is latest branch which is passed from main()
- fetch and checkout branch which cherry-pick targets (from asf-git)
- cherry-pick with commit hash
- if there're some conflicts, guide user to fix it and complete cherry-pick manually
- prompt user to push, or stop
- push changes to remote repository (to asf-git)
- clean temporary branches
- get commit hash and return

> resolve_jira_issue()
- prompt user to input JIRA issue ID: default value is extracted value from commit title
- get information of the issue
- check status and stop if issue is already marked as 'Resolved'
- print information of the issue
- prompt user to input comma-separated fix versions
- default values are `unreleased` versions matched to target branch for merge() / cherry_pick()
- develop branch is treated as default version
- mark issue as 'Resolved' with setting fix versions and leaving comment
",HeartSaVioR,83 days,N,harshach
storm1468,"We may have to modify lots of part of script since...
- We don't have develop branch so all about develop branch should be modified. Spark merge script also doesn't have handling with develop branch since they don't have develop branch, too. Maybe adopting spark script would be easier than adopting kafka script.
- Branch policy is not compatible with projects which uses this script. They have branches per version but we just maintain version lines (major, minor, bugfix) so we should do something while determining fix versions from merged branches.
- We're having master and 1.x / 1.0.x branches heavily diverged, so there're often two or more pull requests submitted per one issue. (Committers don't cherry-pick between master and 1.x for storm-core manually since it's easy to see merge conflict.) It should be tested (at least unit test and integration test) individually, and issue should be closed when all of pull requests are checked in. It means that we're having different merging step which other projects don't have.
- Moreover, commit message hook (closing PR) doesn't work if PR is not against master.
- We should update CHANGELOG while merging step. Personally I don't like updating CHANGELOG so opened thread for discussion but we didn't decide something clearly.
- Commit message will contain body of pull request which is free format for now and tends to be meaningless for commit message. We need to guide contributor to write meaningful information. Thanks for Github we can have [body template of the pull request](https://github.com/blog/2111-issue-and-pull-request-templates) which many projects have been using already.

So without arranging our branch policy and merging step, it will be hard to get merge script fit for us.
",HeartSaVioR,83 days,Y,harshach
storm1468,"-1
I am generally opposed to this. Most PRs only have a small number of commits and aren't a problem. For PRs with a large number of commits, it's simple enough to ask the contributor to squash their own commits. 

I'm not sure I see the benefit in adding another script, which we will have to maintain, in order to do something we should rarely be doing. 

Also, I worry that having this script will lead to a sharp rise in totally-squashed PR merges, even when there's not really any benefit (and in fact, loss of authorship info) since some people are likely just going to use the script whenever they're doing a merge.
",knusbaum,50 days,Y,harshach
storm1468,"I'll second @knusbaum's -1. Based on points I made earlier. This has the potntial to automatically destroy code provenance, especially if more than one contributor is involved in a pull request. From a legal perspective, the ASF need to be able to determine the origin of all code changes.

I would recommend any project that uses a variant of this script to double-check with ASF legal that it is okay. I could be totally wrong, but I'd rather play it safe.

I'd also argue that a well thought out series of commits cane make reviews easier. For example, separating maven build changes from code changes. I'd rather we encourage devs to think out the partitioning of commits themselves and squash appropriately if necessary.
",ptgoetz,-119 days,Y,harshach
storm1468,"I'm totally +1 to this approach, even though I think script should be modified to Storm's project style.

Like I said to dev@ mailing list, I have been doing reviewing and merging pending pull requests for weeks and months, and it was painful enough to merge and port back to each branches, even though I ignored cleaning up commits. (Pain is amplified when tiny commit should be merged to all branches.) If I want to clean up commits before merging it should be more painful. CHANGELOG is subject to not in sync among branches, but we need to write it manually because it's hard to filter merge commits to see the change list. (We could just rely on JIRA issues for alternative.)

Regarding commits, I don't want to keep commits like 'kicking travis', 'address review comments', etc. which is not helpful at any chances. For my last 2 years of development of Storm, I didn't utilize individual commit. If something is wrong with recent merge, we rollback the merge, not individual commit. Squashing commits is widely used strategy and already shows success story to many big projects. Even Github provides the squash merge mode (recently rebase mode too) in GUI.

If we want to merge in squashed commit, it should be done in merging process, not reviewing process. For me, ideal review process should be contributor-friendly. While we can't put efforts to only maintain Storm project (by reviewing pull requests, etc.), contributors also can't. Once we create a script which also squashes the commits, we don't need to make contributors bothering with rebasing and squashing commits. If not, all individuals including us should do it just after merger said 'please rebase and squash in order to merge.', which is also bothering for mergers, too. Moreover, there's a chance for contributors to be busy at the moment, and pull request goes stale. Pull requests which need upmerging are the case.

I understand and agree the authorship issue, but we can treat it as exceptional case. Many pull requests are authored by one.

Let's make merging phase as painless, or at least less painful thing.
",HeartSaVioR,83 days,Y,harshach
storm1468,"I'll also point out that the ""if other Apache projects do it, it is oaky"" stance is particularly dangerous. PMC members must understand ASF policy and not rely on what other projects do. If what another project does is wrong, then doing the same thing in our project just introduces liability.
",ptgoetz,-119 days,Y,harshach
storm1468,"I'm okay with automating the merge process, just not the way it is implemented here. Perhaps we shouldmove the discussion to the dev list.
",ptgoetz,-119 days,N,harshach
subversion3,"@woodgood, thanks for the patch. I'm afraid we cannot accept it, because it changes a released API (the lack of a ""@since"" doxygen tag in the docstring means the API was released in 1.0) and would violate our ""100% API/ABI backwards compatibility"" promise.

To pursue an alternative solution please email dev@subversion.apache.org and describe your use-case. Thanks!
",danielshahaf,994 days,Y,woodgood
tinkerpop537,I did some manual tests - seems to work nicely - VOTE +1,spmallette,1 days,N,dkuppitz
tinkerpop537,VOTE +1,okram,94 days,N,dkuppitz
tinkerpop537,"Actually, can you add a test case for this so we know it doesn't break again in the future?",okram,94 days,Y,dkuppitz
tinkerpop537,i was trying to think how we could do it but didn't really come up with anything. we really don't have any tests for `Console` right now that I know of - not sure how we would reasonably do that with JUnit.,spmallette,1 days,N,dkuppitz
tinkerpop537,"Isn't there a distribution verification that does tests? I know it's post-build, but that's better than nothing.",robertdale,502 days,N,dkuppitz
tomcat142,"If there is a lookup with Charset.forName, then would the cache be used at all ? I don't understand this sort of Windows style ""startup time"" optimization, where you hide stuff behind making running actual application code slower. This is often a bad design, so I don't see what this change brings.
-0",rmaucher,-509 days,Y,philwebb
tomcat142,"Besides the 31ms startup time, it's still worse in every way compared to the current code.",rmaucher,-509 days,N,philwebb
tomcat142,"You submitted three items, and indeed I am not convinced by this one. Looking back to the old BZs, there was also there the conclusion that the current code is the best solution for the general Tomcat use case.

However, there is the ""but I only want UTF-8"" argument, in which case the solution is to hardcode UTF-8 and default to Charset.forName for the rest. I think system property configuration is supposed to be removed, but it would be a good solution for this pluggability (it's global, no one really actually needs it, etc).",rmaucher,-509 days,Y,philwebb
tomcat142,"Nooooo. Not an system property ;).

Seriously, I think there is a way that we can improve start-up time without impacting the vast majority of users. Phil's patch is heading in the general direction I was thinking. As ever, there are likely to be trade-offs involved. I hope to be able to have some hard numbers on which to base that discussion soon(ish).",markt-asf,-1848 days,Y,philwebb
tomcat142,See #146 for an alternative approach,markt-asf,-1848 days,N,philwebb
tomcat108,"I have applied the optimisation commit (thanks) but not the ordering changes.

The patch changes the order for undeploy without changing the order for version mapping. This will lead to unexpected behaviour.

The current simple `String` based ordering was selected as a trade off between performance and usability. The type of ordering proposed in this patch was considered too expensive to incur on every request when multiple versions are deployed. To consider such a change, we'd need to see evidence of the performance impact (including GC) on the mapping process of switching to this ordering mechanism. Be aware that mapping code is highly optimised for `String`.

Reviewing the proposed patch I see several things that would need to be fixed in additional to the more architectural issue described above:
- imports from sun.* are not allowed
- the code is Java 8 specific - any fix needs to be available to back-ported to 7.0.x will has to run on Java 6
- the patch does not compile
- `Pattern` instances should be pre-compiled

Finally, the patch only considers purely numeric versions. Many projects include a test string (e.g. RELEASE) in the version number. It would be nice to handle these as well.",markt-asf,-1133 days,Y,koraktor
tomcat108,"The performance of this strategy can be significantly improved by converting the ""version number"" into a value that can more easily be compared against other values. This can be done in several ways, some building on others (i.e. smaller changes) and some with significant changes.

The most obvious performance optimization is to cache the compiled regex `Pattern` objects: you will be doing a lot of tokenizing on `/\./`, so compile it once and use it many times.

The second most obvious performance optimization is to stop using regular expressions entirely. When searching for a single character, it's much faster to write your own tokenizer because regular expressions inherently have more overhead (because they are so flexible). The code is not as straightforward (and I'm happy to see that you provided an easy-to-follow patch, here) but in this case, speed is in fact important as Mark notes: this code will be executed for every request whose URL might match the context.

The third most obvious performance improvement would be to store the tokenized version number as an array and, in `compareTo` simply compare the individual elements of the arrays in each object. Then you don't have to tokenize during each compare.

Finally, instead of re-computing the ""version"" difference every time, a representative value could be built for the version number that can be quickly compared against another value. For example, let's take the versions `1.0.9` and `1.0.10` for example. One strategy would be to normalize each part of the version number to have some kind of ""maximum number of digits."" Let's pick 4-digits and see how to do this:

1. Tokenize `1.0.9` into [1, 0, 9]
2. Convert to string with 4-digits for each version component: `000100000009`
3. Tokenize `1.0.10` into [1, 0, 10]
4. Convert to string with 4-digits for each version component: `000100000010`
5. Compare the strings alphabetically (`String.compareTo`)

Now, simply more steps 1-2 into the constructor of the `ContextName` class and then `ContextName.compareTo` becomes a simple comparison of the normalized version number.

One can also do this with integral values instead of `Strings` and the comparison becomes even faster, but you run the risk of running out of digits if the padding is too wide (e.g. 4-digit padding with a version number like `1.0.9.1` yields `1000200030001` which is `>` `Integer.MAX_VALUE` so you'd have to switch to a `long` value. Comparing `long` values is faster than comparing `String` values, but at some point you run out of digits in a `long` value as well.

Anyway, there is lots of room for improvement, here, and I think it *does* make sense to support ""traditional"" version numbering. Note that this will represent a backward-*in*compatible change, and those relying upon the current alphabetical-ordering will need to adjust their expectations.",ChristopherSchultz,-67 days,Y,koraktor
tomcat108,I think there is a lot of merit in Chris's idea. The concept of some sort of internal pre-computed version string that addresses ordering concerns by zero padding numerical components is one that would be easy to implement and have minimal impact on the existing system.,markt-asf,-1133 days,Y,koraktor
tomcat108,"I'm starting to lean toward requiring this new feature to require a configuration option to enable it, and have it default to `false`. My justification is that it represents a breaking behavioral change that is *just slightly different* than previous behavior as to be unnoticeable until it starts behaving completely unexpectedly.

Can you please add a configuration option for `<Host>` to enable or disable this? Simply swap-out the `compareTo` implementation depending upon the value of that setting.",ChristopherSchultz,-67 days,Y,koraktor
tomcat108,"I agree this should configurable on the Host.
It should be possible to remove the Comparator entirely if the version code is used directly in the Mapper.
I'd like this to be more robust. I'm thinking pad any dot-separated segment to a given length with a suitable character (possible space).",markt-asf,-1133 days,Y,koraktor
tomcat362,"There is lots of good stuff here but also changes to some files that we can't (due to ASF policy w.r.t. 3rd party licenses) change. And as Martin pointed out changing anything that might be considered part of the public API needs careful consideration.
I'm currently trying to figure out the best way to handle this",markt-asf,-2333 days,Y,jbampton
tomcat362,"OK. I have a plan. I have checked out the source branch for the PR locally and am using a visual diff tool to compare this PR with Tomcat. The tool provides tools to easily review the changes in the PR and apply them selectively to Tomcat. I'll apply changes manually. If you can rebase this PR periodically, that will enable this PR to reflect the current TODO list in terms of what needs to be reviewed (and potentially merged). There will be some changes that can't be made in Tomcat. I can direct you to the correct upstream project for those. If you can remove those changes from this PR and submit PRs for the appropriate project(s) that would be very helpful.",markt-asf,-2333 days,Y,jbampton
tomcat362,There were fewer API changes than expected the changes in the PR have been applied excluding the DTD/schema files identified above. If you could rebase the PR and confirm nothing has been missed that would be great.,markt-asf,-2333 days,Y,jbampton
tomcat41,"You should use SELECT ... FOR UPDATE instead of SELECT / UPDATE. Much less code and transactionally-safe. Also, if you use SELECT ... FOR UPDATE, you can use ResultSet.moveToInsertRow to insert a record if necessary.",ChristopherSchultz,-23 days,Y,JohnKiel
tomcat356,"In fact Tomcat has hundreds of such places where simple, safe, and more readable optimizations can be applied. Just open sources in IntelliJ, open Settings/Editor/Inspections/Java/Performarmance and enable all and then run Analyze.
Most hot places are unefficient working with maps like containsKey() then get() call and collections ans StringBuilder without initial capacity.
It would be great just to fix all these problems at once and this will give a real impact on performance",stokito,2015 days,Y,martin-g
tomcat262,"IMO for the branches where Java 8 is minimum we should switch to Java 8 DateTime APIs. 
java.time.format.DateTimeFormatter is thread-safe.",martin-g,-1886 days,Y,gazzyt
tomcat262,Please don't change something like this in Tomcat 9.0 or older. Those releases are far too stable to go around changing the date-related code.,ChristopherSchultz,-1155 days,Y,gazzyt
tomee427,"@danielsoro every time you need to use the method
` new PropertyEditorRegistry().registerDefaults().`
Why not make this method static?
`PropertyEditorRegistry.registerDefaults();`",otaviojava,602 days,N,danielsoro
tomee427,"In`LocalJMXCommand` the PropertyEditorRegistry is held as a field. In other places, this is doing `PropertyEditorRegistry.registerDefaults()` when calling `getValue()`, which has to go an do a bunch of registration each time.

What do we think the lifecycle of a `PropertyEditorRegistry` should be? I did a quick search for references, which throws up `ManagedMBean`, `ActiveMQ5Factory`, `JMSProducerImpl`, and I would have thought it could be tied to the lifecycle of the components using it in each case (i.e. - make it a field).

Thoughts?",jgallimore,356 days,Y,danielsoro
tomee427,"@danielsoro can you resolve the conflicht in `ClaimBean.java` ?

@jgallimore I agree with your thoughs. I also think, that the `PropertyEditorRegistry` should be tied to the lifecycle of the component using it. Should we change this inside this PR or open a new JIRA and do this optimization work in a separate branch / take it as a feature? I would be +1 for the JIRA option.",rzo1,1035 days,Y,danielsoro
tomee427,"Hey @rzo1 
We haven't heard from Daniel.

So if you are willing to help, I suggest you create a new PR, cherry pick what you need from this one and move on.

Does it work for you?",jeanouii,-13 days,Y,danielsoro
tomee427,@jeanouii Ok - i will submit a PR soon,rzo1,1035 days,N,danielsoro
tomee427,Can one of the admins verify this patch?,asf-ci,1376 days,N,danielsoro
tomee427,This PR has been merged: https://github.com/apache/tomee/pull/541 - is this one still needed?,jgallimore,356 days,N,danielsoro
tomee427,@jgallimore Yes. Can be closed.,rzo1,1035 days,N,danielsoro
tomee72,"applied, build in progress on https://ci.apache.org/builders/tomee-trunk-ubuntu/builds/778",rmannibucau,233 days,N,katya-stoycheva
tomee72,"1. not that much, we are not that strong about it cause we all have different habits and thinking about it
2. you can create a new surefire execution for this test with another arquillian container config, that said the embedded testing in openejb-core (which forks by test) should be enough probably
3. you logged it was read only in the assembler, this is enough for now probably",rmannibucau,233 days,Y,katya-stoycheva
tomee72,@katya-stoycheva can you please close this - it was applied. Thank you very much for your contribution :-),AndyGee,-354 days,N,katya-stoycheva
wicket221,I've created https://issues.apache.org/jira/browse/WICKET-6398 for the changelog.,martin-g,-1966 days,N,Jezza
wicket221,"The fix has been applied with https://issues.apache.org/jira/browse/WICKET-6398.
@Jezza Please close this PR! Thank you !",martin-g,-1966 days,N,Jezza
wicket52,"I don't think we need this PR. It could easily be accomplished by overriding newMessageDisplayComponent or by the following change:

https://gist.github.com/jthomerson/6595716
",jthomerson,-83 days,Y,raphw
tomee625,Can one of the admins verify this patch?,asf-ci,257 days,N,Daniel-Dos
tomee625,"@Daniel-Dos @cesarhernandezgt this PR contains changes for 2 different dependencies, (`krazo` from `1.0.0-Beta1` to `1.1.0-M1` and `deltaspike` from `1.9.1` to `1.9.3`), for cases like this, the JIRA issue and the title of the PR should be updated to reflect the actual changes more accurately. 

@Daniel-Dos just from my own curiosity, why don't you include a final release like [Krazo 1.0.0](https://projects.eclipse.org/projects/ee4j.krazo/releases/1.1.0) or wait for [Krazo 1.1.0](https://projects.eclipse.org/projects/ee4j.krazo/releases/1.1.0) that is scheduled for 03/15/2020 (in less than 2 weeks), instead than using a milestone release?

Even better would be to keep the changes on two use different PRs, as you did originally #626 and #625 .

",emecas,276 days,Y,Daniel-Dos
archiva10,"Yep last error is because of non converging spring version
",ebarboni,13 days,Y,carlossg
cloudstack1653,"LGTM
",ustcweizhou,-375 days,N,NuxRo
cloudstack1653,"LGTM, we need to also test for users who connect to VPN on that network (will DNS work for them as well?). @NuxRo can you edit the PR and change the branch to 4.9, we should have this in 4.9 branch as well.
",rhtyd,-187 days,N,NuxRo
cloudstack1653,"@NuxRo if you can change the base-branch of the PR to 4.9, I can initiate some tests on VR/trillian.
",rhtyd,-187 days,N,NuxRo
cloudstack1653,"Thanks @NuxRo can you rebase against latest 4.9, looks like some more commits came in.
",rhtyd,-187 days,N,NuxRo
cloudstack1653,"@NuxRo there are still several commits which have come from master; can you export the commit (git format-patch -1) and reset --hard origin/4.9 and re-apply the commit (git am <patch>) and then git push -f.
",rhtyd,-187 days,N,NuxRo
cloudstack1653,"@NuxRo okay, can you grant me push access on your fork: https://github.com/NuxRo/cloudstack/tree/patch-4 and I can help fix it for you?
",rhtyd,-187 days,N,NuxRo
cloudstack1653,"@NuxRo thanks, fixed the branch, you may remove my access now :)

@blueorangutan package
",rhtyd,-187 days,N,NuxRo
cloudstack1653,"@rhtyd a Jenkins job has been kicked to build packages. I'll keep you posted as I make progress.
",blueorangutan,528 days,N,NuxRo
cloudstack1653,"Packaging result: ✔centos6 ✔centos7 ✔debian repo: http://packages.shapeblue.com/cloudstack/pr/1653
Job ID-94
",blueorangutan,528 days,N,NuxRo
cloudstack1653,"@blueorangutan test matrix
",rhtyd,-187 days,N,NuxRo
cloudstack1653,"@rhtyd a Trillian-Jenkins matrix job (centos6 mgmt + xs56sp1, centos7 mgmt + vmware55u3, ubuntu mgmt + kvmcentos7) has been kicked to run smoke tests against packages at http://packages.shapeblue.com/cloudstack/pr/1653
",blueorangutan,528 days,N,NuxRo
cloudstack1653,"@NuxRo this patch does not appear to break things. However, we want to add a Marvin test case to verify that the fix does not regress again. Does that make sense?

@rhtyd is planning to write the Marvin test case this week. Once he is done and we retest, we will merge this PR.
",jburwell,128 days,Y,NuxRo
cloudstack140,"pinging @llambiel and @pyr @wido LGTM.
",sebgoa,-83 days,N,remibergsma
cloudstack140,"Looks OK to me, waiting for a dead NFS connection is a PITA.
",NuxRo,77 days,N,remibergsma
cloudstack140,"Yes, it is, but we should log somewhere. Now a machine just reboots. We should send something to syslog prior to rebooting.
",wido,115 days,N,remibergsma
cloudstack140,"Something along those lines indeed. So admins can find out WHY the system just went away. Maybe write the log, sleep for 5 seconds and then reboot. Otherwise the logline might not be flushed to the local disk.
",wido,115 days,Y,remibergsma
cloudstack140,"@wido, if this is done as root, writing the log, then calling `fsync` then doing the sysrq will suffice.
",pyr,-17 days,N,remibergsma
cloudstack140,"Would the fsync suffice if we're using network log servers? Ideally the data should be registered remotely, then invoke the sysrq operation.
",NuxRo,77 days,N,remibergsma
cloudstack140,"@NuxRo if you're using network log servers, the UDP packets get sent right away.
",pyr,-17 days,N,remibergsma
cloudstack140,"Cool. Then it seems like we have a deal. It remains to test if the actual log entry survives the sudden reboot.
",NuxRo,77 days,N,remibergsma
cloudstack1258,"@nitin-maharana Thanks. Are there any other api calls that have the same issue?
",remibergsma,-270 days,N,nitin-maharana
cloudstack1258,"@nitin-maharana shouldn't we use the ""recursive"" parameter set to ""true"" too?
",rafaelweingartner,-62 days,N,nitin-maharana
cloudstack1258,"@nitin-maharana thanks for checking.
LGTM
",rafaelweingartner,-62 days,N,nitin-maharana
cloudstack1258,"@nitin-maharana This fix should be against 4.7 IMHO. Can you make a PR for that please? We can still refer to the reviews that have been done. This way both 4.7 and 4.8 will have this fix. Thanks!
",remibergsma,-270 days,Y,nitin-maharana
cxf478,LGTM,coheigea,-1425 days,N,gytis
cxf478,I hope before Xmas.,coheigea,-1425 days,N,gytis
cxf448,The change looks ok but why have you merged it with my previous commit?,deki,96 days,N,rmannibucau
cxf448,I think this change should have an own commit with an own comment because it's not related to the version update of Spring.,deki,96 days,Y,rmannibucau
cxf448,Thanks. You can just force push to your branch next time.,deki,96 days,N,rmannibucau
flink1940,"I added a comment to FLINK-2220. I think the original analysis of the problem was not correct. It is not necessary to check for POJOs whether they override `equals()` and `hashcode()`. Details in FLINK-2220
",fhueske,-429 days,Y,gallenvara
flink1940,"Any update for this PR, @gallenvara?
",fhueske,-429 days,N,gallenvara
flink2883,"Since the capacity limit refers both to input and output queues, we should probably rename `taskmanager.net.max-out-queue-length` to `taskmanager.net.max-queue-length`.",StephanEwen,2 days,Y,uce
flink2883,"Agreed, let's keep it as a debug branch.

I think this is a useful, but not complete, step towards getting rid of the network buffers configuration.
Especially introducing the explicitly ""capacity constrained / back-pressured"" partition type is a good thing, in my opinion.",StephanEwen,2 days,Y,uce
fop60,"FOP-2704 in jira was closed, can you try fop trunk to see if you have a issue",simonsteiner1984,269 days,Y,mpdude
hadoop595,":broken_heart: **-1 overall**






| Vote | Subsystem | Runtime | Comment |
|:----:|----------:|--------:|:--------|
| 0 | reexec | 20 | Docker mode activated. |
||| _ Prechecks _ |
| +1 | @author | 0 | The patch does not contain any @author tags. |
| -1 | test4tests | 0 | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |
||| _ trunk Compile Tests _ |
| +1 | mvninstall | 1098 | trunk passed |
| +1 | compile | 123 | trunk passed |
| +1 | mvnsite | 20 | trunk passed |
| +1 | shadedclient | 1923 | branch has no errors when building and testing our client artifacts. |
||| _ Patch Compile Tests _ |
| +1 | mvninstall | 14 | the patch passed |
| +1 | compile | 100 | the patch passed |
| -1 | cc | 100 | hadoop-hdfs-project_hadoop-hdfs-native-client generated 8 new + 2 unchanged - 0 fixed = 10 total (was 2) |
| +1 | javac | 100 | the patch passed |
| +1 | mvnsite | 16 | the patch passed |
| -1 | whitespace | 0 | The patch 1 line(s) with tabs. |
| +1 | shadedclient | 784 | patch has no errors when building and testing our client artifacts. |
||| _ Other Tests _ |
| +1 | unit | 358 | hadoop-hdfs-native-client in the patch passed. |
| +1 | asflicense | 25 | The patch does not generate ASF License warnings. |
| | | 3370 | |


| Subsystem | Report/Notes |
|----------:|:-------------|
| Docker | Client=17.05.0-ce Server=17.05.0-ce base: https://builds.apache.org/job/hadoop-multibranch/job/PR-595/1/artifact/out/Dockerfile |
| GITHUB PR | https://github.com/apache/hadoop/pull/595 |
| JIRA Issue | HDFS-14304 |
| Optional Tests | dupname asflicense compile cc mvnsite javac unit |
| uname | Linux 2d673d55175a 4.4.0-139-generic #165~14.04.1-Ubuntu SMP Wed Oct 31 10:55:11 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | personality/hadoop.sh |
| git revision | trunk / c0427c8 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_191 |
| cc | https://builds.apache.org/job/hadoop-multibranch/job/PR-595/1/artifact/out/diff-compile-cc-hadoop-hdfs-project_hadoop-hdfs-native-client.txt |
| whitespace | https://builds.apache.org/job/hadoop-multibranch/job/PR-595/1/artifact/out/whitespace-tabs.txt |
| Test Results | https://builds.apache.org/job/hadoop-multibranch/job/PR-595/1/testReport/ |
| Max. process+thread count | 340 (vs. ulimit of 5500) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client U: hadoop-hdfs-project/hadoop-hdfs-native-client |
| Console output | https://builds.apache.org/job/hadoop-multibranch/job/PR-595/1/console |
| Powered by | Apache Yetus 0.9.0 http://yetus.apache.org |


This message was automatically generated.

",hadoop-yetus,802 days,N,sahilTakiar
jackrabbit83,@leachuk Thanks for this PR. To enable to proceed with it please first create an according JIRA ticket and link it from your commit message. Also please rebase and fix the conflict and some issue related to HTTPS has been fixed in https://github.com/apache/jackrabbit/commit/3b2fae841179ee92bfe77a8c7d027fe104c97623 already.,kwin,-271 days,Y,wildone
jackrabbit83,"Thanks for reviewing this. I've closed here as the PR has been superseded by https://github.com/apache/jackrabbit/pull/88 , which includes the new '--insecure' optional parameter.",leachuk,550 days,N,wildone
jclouds64,"Hi! Apologies for the late review. I've been really busy and had no cycles to look at this PR again.

The fix looks good. So the summary is that we need to provide a singleton SSLContext, but also a singleton SSLContext factory. The current approach, however, just addresses this for the `untrusted` context. Would the issue also manifest when a proper, trusted SSLContext is provided? If so, we should have feature parity there, and allow also to set the custom SSLSocketFactory supplier. One option could be to have the Lazy supplier you created implement both suppliers, and allow users to easily use it to inject both consistently where needed?

",nacx,-191 days,Y,roded
jclouds64,@roded We plan to release 2.3.0 later this month. Do you think you can finish this PR soon?,gaul,-1119 days,N,roded
camle3214,"I think changes like this need to be discussed before hand on the dev mailing list or on JIRA issue.

This would configure a build on a Windows machine using GitHub actions. I don't think we need this, at least not in this form.",zregvart,-1201 days,Y,athershehzad
cloudstack1055,"did a mvn build on the pr and manually verified the api call. the private key isn't logged anymore.

before

```
2015-11-10 17:13:18,462 DEBUG [o.a.c.f.j.i.AsyncJobManagerImpl] (API-Job-Executor-3:ctx-74dad1b6 job-799) Executing AsyncJobVO {id:799, userId: 2, accountId: 2, instanceType: None, instanceId: null, cmd: org.apache.cloudstack.api.command.admin.resource.UploadCustomCertificateCmd, cmdInfo: {""sessionkey"":""UZeTJmhKlSkEyt9O1H0QceBb3yI"",""cmdEventType"":""UPLOAD.CUSTOM.CERTIFICATE"",""ctxUserId"":""2"",""signatureversion"":""3"",""httpmethod"":""GET"",""response"":""json"",""domainsuffix"":""abc.com"",""ctxDetails"":""{}"",""certificate"":""123"",""expires"":""2015-11-10T12:04:08+0000"",""ctxAccountId"":""2"",""ctxStartEventId"":""2089"",""privatekey"":""private123""}, cmdVersion: 0, status: IN_PROGRESS, processStatus: 0, resultCode: 0, result: null, initMsid: 233845178473200, completeMsid: null, lastUpdated: null, lastPolled: null, created: null}
```

after

```
2015-11-10 17:22:59,986 DEBUG [o.a.c.f.j.i.AsyncJobManagerImpl] (API-Job-Executor-1:ctx-3c64d80b job-800) Executing AsyncJobVO {id:800, userId: 2, accountId: 2, instanceType: None, instanceId: null, cmd: org.apache.cloudstack.api.command.admin.resource.UploadCustomCertificateCmd, cmdInfo: {""sessionkey"":""9kO_Di2wrf2ekMKCYrveEjCtTg8"",""cmdEventType"":""UPLOAD.CUSTOM.CERTIFICATE"",""ctxUserId"":""2"",""signatureversion"":""3"",""httpmethod"":""GET"",""response"":""json"",""domainsuffix"":""abc.com"",""ctxDetails"":""{}"",""certificate"":""123"",""expires"":""2015-11-10T12:13:49+0000"",""ctxAccountId"":""2"",""ctxStartEventId"":""2091"",}, cmdVersion: 0, status: IN_PROGRESS, processStatus: 0, resultCode: 0, result: null, initMsid: 233845178473200, completeMsid: null, lastUpdated: null, lastPolled: null, created: null}
```

LGTM :+1: 
",karuturi,-1 days,N,DaanHoogland
cloudstack1055,"I'm testing this PR with a 2 KVM host setup to run a set of basic Marvin tests. Will also grep the logs for the private key.
",miguelaferreira,269 days,N,DaanHoogland
cloudstack1055,"@DaanHoogland sent me the below patch the other day and I run some tests on it. Just FYI the results (he rebased later and made a PR). The commit hash obviously doesn't match as it was my temp patch apply commit.

The patch I tested:

```
commit cad68778d8714a5359e3fa79a33d05206e032fea
Author: root <root@cs1.cloud.lan>
Date: Mon Nov 9 12:40:36 2015 +0000

private key security fix received from Daan

diff --git a/api/src/org/apache/cloudstack/api/command/admin/resource/UploadCustomCertificateCmd.java b/api/src/org/apache/cloudstack/api/command/admin/resource/UploadCustomCertificateCmd.java
index e11876a..e8d6cc5 100644
--- a/api/src/org/apache/cloudstack/api/command/admin/resource/UploadCustomCertificateCmd.java
+++ b/api/src/org/apache/cloudstack/api/command/admin/resource/UploadCustomCertificateCmd.java
@@ -32,7 +32,7 @@ import com.cloud.user.Account;
@APICommand(name = ""uploadCustomCertificate"",
responseObject = CustomCertificateResponse.class,
description = ""Uploads a custom certificate for the console proxy VMs to use for SSL. Can be used to upload a single certificate signed by a known CA. Can also be used, through mu
- requestHasSensitiveInfo = false, responseHasSensitiveInfo = false)
+ requestHasSensitiveInfo = true, responseHasSensitiveInfo = false)
public class UploadCustomCertificateCmd extends BaseAsyncCmd {
public static final Logger s_logger = Logger.getLogger(UploadCustomCertificateCmd.class.getName());

diff --git a/utils/src/main/java/com/cloud/utils/StringUtils.java b/utils/src/main/java/com/cloud/utils/StringUtils.java
index c598be8..71cebe1 100644
--- a/utils/src/main/java/com/cloud/utils/StringUtils.java
+++ b/utils/src/main/java/com/cloud/utils/StringUtils.java
@@ -186,7 +186,7 @@ public class StringUtils {
private static final Pattern REGEX_PASSWORD_QUERYSTRING = Pattern.compile(""(&|%26)?[^(&|%26)]*((p|P)assword|accesskey|secretkey)(=|%3D).*?(?=(%26|[&'\""]|$))"");

// removes a password/accesskey/ property from a response json object
- private static final Pattern REGEX_PASSWORD_JSON = Pattern.compile(""\""((p|P)assword|accesskey|secretkey)\"":\\s?\"".*?\"",?"");
+ private static final Pattern REGEX_PASSWORD_JSON = Pattern.compile(""\""((p|P)assword|privatekey|accesskey|secretkey)\"":\\s?\"".*?\"",?"");

private static final Pattern REGEX_PASSWORD_DETAILS = Pattern.compile(""(&|%26)?details(\\[|%5B)\\d*(\\]|%5D)\\.key(=|%3D)((p|P)assword|accesskey|secretkey)(?=(%26|[&'\""]))"");
```

Tests:

```
nosetests --with-marvin --marvin-config=${marvinCfg} -s -a tags=advanced,required_hardware=true \
component/test_vpc_redundant.py \
component/test_routers_iptables_default_policy.py \
component/test_routers_network_ops.py \
component/test_vpc_router_nics.py \
smoke/test_loadbalance.py \
smoke/test_internal_lb.py \
smoke/test_ssvm.py \
smoke/test_network.py
```

Result:

```
[root@cs1 MarvinLogs]# cat test_network_XFGU4E/results.txt 
Create a redundant VPC with two networks with two VMs in each network ... === TestName: test_01_create_redundant_VPC_2tiers_4VMs_4IPs_4PF_ACL | Status : SUCCESS ===
ok
Create a redundant VPC with two networks with two VMs in each network and check default routes ... === TestName: test_02_redundant_VPC_default_routes | Status : SUCCESS ===
ok
Test iptables default INPUT/FORWARD policy on RouterVM ... === TestName: test_02_routervm_iptables_policies | Status : SUCCESS ===
ok
Test iptables default INPUT/FORWARD policies on VPC router ... === TestName: test_01_single_VPC_iptables_policies | Status : SUCCESS ===
ok
Stop existing router, add a PF rule and check we can access the VM ... === TestName: test_isolate_network_FW_PF_default_routes | Status : SUCCESS ===
ok
Test redundant router internals ... === TestName: test_RVR_Network_FW_PF_SSH_default_routes | Status : SUCCESS ===
ok
Create a VPC with two networks with one VM in each network and test nics after destroy ... === TestName: test_01_VPC_nics_after_destroy | Status : SUCCESS ===
ok
Create a VPC with two networks with one VM in each network and test default routes ... === TestName: test_02_VPC_default_routes | Status : SUCCESS ===
ok
Check the password file in the Router VM ... === TestName: test_isolate_network_password_server | Status : SUCCESS ===
ok
Check that the /etc/dhcphosts.txt doesn't contain duplicate IPs ... === TestName: test_router_dhcphosts | Status : SUCCESS ===
ok
Test to create Load balancing rule with source NAT ... === TestName: test_01_create_lb_rule_src_nat | Status : SUCCESS ===
ok
Test to create Load balancing rule with non source NAT ... === TestName: test_02_create_lb_rule_non_nat | Status : SUCCESS ===
ok
Test for assign & removing load balancing rule ... === TestName: test_assign_and_removal_lb | Status : SUCCESS ===
ok
Test to verify access to loadbalancer haproxy admin stats page ... === TestName: test02_internallb_haproxy_stats_on_all_interfaces | Status : SUCCESS ===
ok
Test create, assign, remove of an Internal LB with roundrobin http traffic to 3 vm's ... === TestName: test_01_internallb_roundrobin_1VPC_3VM_HTTP_port80 | Status : SUCCESS ===
ok
Test SSVM Internals ... === TestName: test_03_ssvm_internals | Status : SUCCESS ===
ok
Test CPVM Internals ... === TestName: test_04_cpvm_internals | Status : SUCCESS ===
ok
Test stop SSVM ... === TestName: test_05_stop_ssvm | Status : SUCCESS ===
ok
Test stop CPVM ... === TestName: test_06_stop_cpvm | Status : SUCCESS ===
ok
Test reboot SSVM ... === TestName: test_07_reboot_ssvm | Status : SUCCESS ===
ok
Test reboot CPVM ... === TestName: test_08_reboot_cpvm | Status : SUCCESS ===
ok
Test destroy SSVM ... === TestName: test_09_destroy_ssvm | Status : SUCCESS ===
ok
Test destroy CPVM ... === TestName: test_10_destroy_cpvm | Status : SUCCESS ===
ok
Test for port forwarding on source NAT ... === TestName: test_01_port_fwd_on_src_nat | Status : SUCCESS ===
ok
Test for port forwarding on non source NAT ... === TestName: test_02_port_fwd_on_non_src_nat | Status : SUCCESS ===
ok
Test for reboot router ... === TestName: test_reboot_router | Status : SUCCESS ===
ok
Test for Router rules for network rules on acquired public IP ... === TestName: test_network_rules_acquired_public_ip_1_static_nat_rule | Status : SUCCESS ===
ok
Test for Router rules for network rules on acquired public IP ... === TestName: test_network_rules_acquired_public_ip_2_nat_rule | Status : SUCCESS ===
ok
Test for Router rules for network rules on acquired public IP ... === TestName: test_network_rules_acquired_public_ip_3_Load_Balancer_Rule | Status : SUCCESS ===
ok
----------------------------------------------------------------------
Ran 29 tests in 12467.478s
OK
```

And:

```
nosetests --with-marvin --marvin-config=${marvinCfg} -s -a tags=advanced,required_hardware=false \
smoke/test_routers.py \
smoke/test_network_acl.py \
smoke/test_privategw_acl.py \
smoke/test_reset_vm_on_reboot.py \
smoke/test_vm_life_cycle.py \
smoke/test_vpc_vpn.py \
smoke/test_service_offerings.py \
component/test_vpc_offerings.py \
component/test_vpc_routers.py
```

Result:

```
[root@cs1 MarvinLogs]# cat test_vpc_routers_BFJ8KP/results.txt 
Test router internal advanced zone ... === TestName: test_02_router_internal_adv | Status : SUCCESS ===
ok
Test restart network ... === TestName: test_03_restart_network_cleanup | Status : SUCCESS ===
ok
Test router basic setup ... === TestName: test_05_router_basic | Status : SUCCESS ===
ok
Test router advanced setup ... === TestName: test_06_router_advanced | Status : SUCCESS ===
ok
Test stop router ... === TestName: test_07_stop_router | Status : SUCCESS ===
ok
Test start router ... === TestName: test_08_start_router | Status : SUCCESS ===
ok
Test reboot router ... === TestName: test_09_reboot_router | Status : SUCCESS ===
ok
test_privategw_acl (integration.smoke.test_privategw_acl.TestPrivateGwACL) ... === TestName: test_privategw_acl | Status : SUCCESS ===
ok
Test reset virtual machine on reboot ... === TestName: test_01_reset_vm_on_reboot | Status : SUCCESS ===
ok
Test advanced zone virtual router ... === TestName: test_advZoneVirtualRouter | Status : SUCCESS ===
ok
Test Deploy Virtual Machine ... === TestName: test_deploy_vm | Status : SUCCESS ===
ok
Test Multiple Deploy Virtual Machine ... === Tes",remibergsma,93 days,N,DaanHoogland
cloudstack1055,"Jenkins error seems unrelated:

```
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project cloud-server: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test failed: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
```
",remibergsma,93 days,N,DaanHoogland
cloudstack1055,"JVM died (again!):

```
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.324 sec - in org.apache.cloudstack.privategw.AclOnPrivateGwTest
Running org.apache.cloudstack.networkoffering.CreateNetworkOfferingTest
#
# A fatal error has been detected by the Java Runtime Environment:
#
SUREFIRE-859: # SIGSEGV (0xb) at pc=0xada6f654, pid=21145, tid=4136938304
#
# JRE version: 7.0_25-b15
# Java VM: Java HotSpot(TM) Server VM (23.25-b01 mixed mode linux-x86 )
# Problematic frame:
# C [libnet.so+0x14654] _fini+0x1d0c
#
SUREFIRE-859: # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /home/jenkins/jenkins-slave/workspace/cloudstack-pull-analysis/server/hs_err_pid21145.log
#
SUREFIRE-859: # If you would like to submit a bug report, please visit:
# http://bugreport.sun.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
Aborted
```
",miguelaferreira,269 days,N,DaanHoogland
flink10871,"Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community
to review your pull request. We will use this comment to track the progress of the review.


## Automated Checks
Last check on commit 8257d0ef74983dad3b34aa4fbcbabb9bae8f58b7 (Thu Jan 16 13:41:26 UTC 2020)

**Warnings:**
* No documentation files were touched! Remember to keep the Flink docs up to date!
* **This pull request references an unassigned [Jira ticket](https://issues.apache.org/jira/browse/FLINK-13758).** According to the [code contribution guide](https://flink.apache.org/contributing/contribute-code.html), tickets need to be assigned before starting with the implementation work.


<sub>Mention the bot in a comment to re-run the automated checks.</sub>
## Review Progress

* ❓ 1. The [description] looks good.
* ❓ 2. There is [consensus] that the contribution should go into to Flink.
* ❓ 3. Needs [attention] from.
* ❓ 4. The change fits into the overall [architecture].
* ❓ 5. Overall code [quality] is good.

Please see the [Pull Request Review Guide](https://flink.apache.org/contributing/reviewing-prs.html) for a full explanation of the review process.<details>
The Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>
The @flinkbot bot supports the following commands:

- `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)
- `@flinkbot approve all` to approve all aspects
- `@flinkbot approve-until architecture` to approve everything until `architecture`
- `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention
- `@flinkbot disapprove architecture` to remove an approval you gave earlier
</details>",flinkbot,49 days,N,wangyang0919
flink10871,"<!--
Meta data
Hash:8257d0ef74983dad3b34aa4fbcbabb9bae8f58b7 Status:SUCCESS URL:https://travis-ci.com/flink-ci/flink/builds/144745849 TriggerType:PUSH TriggerID:8257d0ef74983dad3b34aa4fbcbabb9bae8f58b7
Hash:8257d0ef74983dad3b34aa4fbcbabb9bae8f58b7 Status:SUCCESS URL:https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_build/results?buildId=4401 TriggerType:PUSH TriggerID:8257d0ef74983dad3b34aa4fbcbabb9bae8f58b7
Hash:59e2c089f08f2d9ac462b0cc25d9afaf533cca42 Status:SUCCESS URL:https://travis-ci.com/flink-ci/flink/builds/144759987 TriggerType:PUSH TriggerID:59e2c089f08f2d9ac462b0cc25d9afaf533cca42
Hash:59e2c089f08f2d9ac462b0cc25d9afaf533cca42 Status:SUCCESS URL:https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_build/results?buildId=4410 TriggerType:PUSH TriggerID:59e2c089f08f2d9ac462b0cc25d9afaf533cca42
-->
## CI report:

* 8257d0ef74983dad3b34aa4fbcbabb9bae8f58b7 Travis: [SUCCESS](https://travis-ci.com/flink-ci/flink/builds/144745849) Azure: [SUCCESS](https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_build/results?buildId=4401) 
* 59e2c089f08f2d9ac462b0cc25d9afaf533cca42 Travis: [SUCCESS](https://travis-ci.com/flink-ci/flink/builds/144759987) Azure: [SUCCESS](https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_build/results?buildId=4410) 

<details>
<summary>Bot commands</summary>
The @flinkbot bot supports the following commands:

- `@flinkbot run travis` re-run the last Travis build
- `@flinkbot run azure` re-run the last Azure build
</details>",flinkbot,49 days,N,wangyang0920
flink10871,I will merge as soon as Travis gives green.,kl0u,-1258 days,N,wangyang0922
groovy1220,1,danielsun1106,-260 days,N,pditommaso
groovy1220,"Thanks for your contribution!
Could you please submit a JIRA ticket to track the improvement?
",danielsun1106,-260 days,N,pditommaso
groovy1220,"Merged. Thanks!
",danielsun1106,-260 days,N,pditommaso
hbase2858,":confetti_ball: **+1 overall**






| Vote | Subsystem | Runtime | Comment |
|:----:|----------:|--------:|:--------|
| +0 :ok: | reexec | 0m 31s | Docker mode activated. |
| -0 :warning: | yetus | 0m 4s | Unprocessed flag(s): --brief-report-file --spotbugs-strict-precheck --whitespace-eol-ignore-list --whitespace-tabs-ignore-list --quick-hadoopcheck |
||| _ Prechecks _ |
||| _ master Compile Tests _ |
| +1 :green_heart: | mvninstall | 3m 28s | master passed |
| +1 :green_heart: | compile | 1m 1s | master passed |
| +1 :green_heart: | shadedjars | 6m 44s | branch has no errors when building our shaded downstream artifacts. |
| +1 :green_heart: | javadoc | 0m 38s | master passed |
||| _ Patch Compile Tests _ |
| +1 :green_heart: | mvninstall | 3m 34s | the patch passed |
| +1 :green_heart: | compile | 0m 56s | the patch passed |
| +1 :green_heart: | javac | 0m 56s | the patch passed |
| +1 :green_heart: | shadedjars | 6m 36s | patch has no errors when building our shaded downstream artifacts. |
| +1 :green_heart: | javadoc | 0m 36s | the patch passed |
||| _ Other Tests _ |
| +1 :green_heart: | unit | 144m 59s | hbase-server in the patch passed. |
| | | 171m 7s | |


| Subsystem | Report/Notes |
|----------:|:-------------|
| Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/job/PR-2858/3/artifact/yetus-jdk8-hadoop3-check/output/Dockerfile |
| GITHUB PR | https://github.com/apache/hbase/pull/2858 |
| Optional Tests | javac javadoc unit shadedjars compile |
| uname | Linux d57292bf8a88 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | dev-support/hbase-personality.sh |
| git revision | master / 4caab90aa7 |
| Default Java | AdoptOpenJDK-1.8.0_232-b09 |
| Test Results | https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/job/PR-2858/3/testReport/ |
| Max. process+thread count | 4417 (vs. ulimit of 30000) |
| modules | C: hbase-server U: hbase-server |
| Console output | https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/job/PR-2858/3/console |
| versions | git=2.17.1 maven=3.6.3 |
| Powered by | Apache Yetus 0.12.0 https://yetus.apache.org |


This message was automatically generated.

",Apache-HBase,-139 days,N,ZhaoBQ
hbase2858,":confetti_ball: **+1 overall**






| Vote | Subsystem | Runtime | Comment |
|:----:|----------:|--------:|:--------|
| +0 :ok: | reexec | 1m 6s | Docker mode activated. |
| -0 :warning: | yetus | 0m 3s | Unprocessed flag(s): --brief-report-file --spotbugs-strict-precheck --whitespace-eol-ignore-list --whitespace-tabs-ignore-list --quick-hadoopcheck |
||| _ Prechecks _ |
||| _ master Compile Tests _ |
| +1 :green_heart: | mvninstall | 4m 47s | master passed |
| +1 :green_heart: | compile | 1m 13s | master passed |
| +1 :green_heart: | shadedjars | 7m 25s | branch has no errors when building our shaded downstream artifacts. |
| +1 :green_heart: | javadoc | 0m 42s | master passed |
||| _ Patch Compile Tests _ |
| +1 :green_heart: | mvninstall | 4m 33s | the patch passed |
| +1 :green_heart: | compile | 1m 12s | the patch passed |
| +1 :green_heart: | javac | 1m 12s | the patch passed |
| +1 :green_heart: | shadedjars | 7m 29s | patch has no errors when building our shaded downstream artifacts. |
| +1 :green_heart: | javadoc | 0m 40s | the patch passed |
||| _ Other Tests _ |
| +1 :green_heart: | unit | 212m 51s | hbase-server in the patch passed. |
| | | 244m 0s | |


| Subsystem | Report/Notes |
|----------:|:-------------|
| Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/job/PR-2858/3/artifact/yetus-jdk11-hadoop3-check/output/Dockerfile |
| GITHUB PR | https://github.com/apache/hbase/pull/2858 |
| Optional Tests | javac javadoc unit shadedjars compile |
| uname | Linux 09d9fce59be2 4.15.0-128-generic #131-Ubuntu SMP Wed Dec 9 06:57:35 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | dev-support/hbase-personality.sh |
| git revision | master / 4caab90aa7 |
| Default Java | AdoptOpenJDK-11.0.6+10 |
| Test Results | https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/job/PR-2858/3/testReport/ |
| Max. process+thread count | 3601 (vs. ulimit of 30000) |
| modules | C: hbase-server U: hbase-server |
| Console output | https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/job/PR-2858/3/console |
| versions | git=2.17.1 maven=3.6.3 |
| Powered by | Apache Yetus 0.12.0 https://yetus.apache.org |


This message was automatically generated.

",Apache-HBase,-139 days,N,ZhaoBQ
hbase2858,Any further comments/concerns @wchevreuil ?,ndimiduk,-1113 days,N,ZhaoBQ
jena910,The basic interface looks fine as far as I'm able to pick it out amongst the swathe of related changes,rvesse,46 days,N,afs
jmeter632,"My biggest concern with this PR at the moment is its size. Could we add the feature (errorprone) without changing every nag it shows? I tend to get bored when looking at big PRs and either skip parts, or abort looking at it altogether.
Apart from that, I am +1 on adding the feature (if it doesn't break the build when running with Java 8). (Haven't tried it)",FSchumacher,-93 days,Y,vlsi
phonenix1112,"This should be integration testable; however, I saw in the JIRA that you were having issues recreating this. What area according to was giving you issues making an IT for this?",dbwong,-347 days,Y,stoty
poi202,"We have a policy of deprecating code before removing it. I don't think this change is a good idea, even if the code is not really used.",pjfanning,-244 days,Y,MariusVolkhart
poi110,"+1 from me, XWPF is officially still in ""scratchpad"", please add an entry in status.xml which flags this as import change to have it properly documented in the changelog.",centic9,-614 days,Y,pjfanning
hbase977,":broken_heart: **-1 overall** 






| Vote | Subsystem | Runtime | Comment | 
|:----:|----------:|--------:|:--------| 
| +0 :ok: | reexec | 1m 50s | Docker mode activated. | 
||| _ Prechecks _ | 
| +1 :green_heart: | dupname | 0m 0s | No case conflicting files found. | 
| +1 :green_heart: | hbaseanti | 0m 1s | Patch does not have any anti-patterns. | 
| +1 :green_heart: | @author | 0m 0s | The patch does not contain any @author tags. | 
| -0 :warning: | test4tests | 0m 0s | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | 
||| _ master Compile Tests _ | 
| +0 :ok: | mvndep | 0m 35s | Maven dependency ordering for branch | 
| +1 :green_heart: | mvninstall | 5m 58s | master passed | 
| +1 :green_heart: | compile | 1m 29s | master passed | 
| +1 :green_heart: | checkstyle | 2m 8s | master passed | 
| +1 :green_heart: | shadedjars | 5m 5s | branch has no errors when building our shaded downstream artifacts. | 
| +1 :green_heart: | javadoc | 1m 4s | master passed | 
| +0 :ok: | spotbugs | 5m 37s | Used deprecated FindBugs config; considering switching to SpotBugs. | 
| +1 :green_heart: | findbugs | 6m 59s | master passed | 
||| _ Patch Compile Tests _ | 
| +0 :ok: | mvndep | 0m 15s | Maven dependency ordering for patch | 
| +1 :green_heart: | mvninstall | 5m 41s | the patch passed | 
| +1 :green_heart: | compile | 1m 28s | the patch passed | 
| +1 :green_heart: | javac | 1m 28s | the patch passed | 
| +1 :green_heart: | checkstyle | 2m 4s | the patch passed | 
| +1 :green_heart: | whitespace | 0m 0s | The patch has no whitespace issues. | 
| +1 :green_heart: | shadedjars | 5m 8s | patch has no errors when building our shaded downstream artifacts. | 
| +1 :green_heart: | hadoopcheck | 17m 53s | Patch does not cause any errors with Hadoop 2.8.5 2.9.2 or 3.1.2. | 
| +1 :green_heart: | javadoc | 1m 9s | the patch passed | 
| +1 :green_heart: | findbugs | 6m 41s | the patch passed | 
||| _ Other Tests _ | 
| +1 :green_heart: | unit | 1m 54s | hbase-client in the patch passed. | 
| -1 :x: | unit | 192m 22s | hbase-server in the patch failed. | 
| +1 :green_heart: | asflicense | 0m 50s | The patch does not generate ASF License warnings. | 
| | | 268m 1s | | 


| Reason | Tests | 
|-------:|:------| 
| Failed junit tests | hadoop.hbase.master.procedure.TestSCPWithMetaWithReplicasWithoutZKCoordinated | 


| Subsystem | Report/Notes | 
|----------:|:-------------| 
| Docker | Client=19.03.5 Server=19.03.5 base: https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/1/artifact/out/Dockerfile | 
| GITHUB PR | https://github.com/apache/hbase/pull/977 | 
| JIRA Issue | HBASE-23628 | 
| Optional Tests | dupname asflicense javac javadoc unit spotbugs findbugs shadedjars hadoopcheck hbaseanti checkstyle compile | 
| uname | Linux a5f587eff5f0 4.15.0-66-generic #75-Ubuntu SMP Tue Oct 1 05:24:09 UTC 2019 x86_64 GNU/Linux | 
| Build tool | maven | 
| Personality | /home/jenkins/jenkins-slave/workspace/HBase-PreCommit-GitHub-PR_PR-977/out/precommit/personality/provided.sh | 
| git revision | master / 923ba7763e | 
| Default Java | 1.8.0_181 | 
| unit | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/1/artifact/out/patch-unit-hbase-server.txt | 
| Test Results | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/1/testReport/ | 
| Max. process+thread count | 4122 (vs. ulimit of 10000) | 
| modules | C: hbase-client hbase-server U: . | 
| Console output | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/1/console | 
| versions | git=2.11.0 maven=2018-06-17T18:33:14Z) findbugs=3.1.11 | 
| Powered by | Apache Yetus 0.11.1 https://yetus.apache.org | 


This message was automatically generated.",Apache-HBase,-223 days,N,belugabehr
hbase977,":broken_heart: **-1 overall** 






| Vote | Subsystem | Runtime | Comment | 
|:----:|----------:|--------:|:--------| 
| +0 :ok: | reexec | 1m 22s | Docker mode activated. | 
||| _ Prechecks _ | 
| +1 :green_heart: | dupname | 0m 0s | No case conflicting files found. | 
| +1 :green_heart: | hbaseanti | 0m 0s | Patch does not have any anti-patterns. | 
| +1 :green_heart: | @author | 0m 0s | The patch does not contain any @author tags. | 
| -0 :warning: | test4tests | 0m 0s | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | 
||| _ master Compile Tests _ | 
| +0 :ok: | mvndep | 0m 36s | Maven dependency ordering for branch | 
| +1 :green_heart: | mvninstall | 5m 45s | master passed | 
| +1 :green_heart: | compile | 1m 23s | master passed | 
| +1 :green_heart: | checkstyle | 2m 5s | master passed | 
| +1 :green_heart: | shadedjars | 5m 3s | branch has no errors when building our shaded downstream artifacts. | 
| +1 :green_heart: | javadoc | 1m 0s | master passed | 
| +0 :ok: | spotbugs | 4m 52s | Used deprecated FindBugs config; considering switching to SpotBugs. | 
| +1 :green_heart: | findbugs | 6m 4s | master passed | 
||| _ Patch Compile Tests _ | 
| +0 :ok: | mvndep | 0m 17s | Maven dependency ordering for patch | 
| +1 :green_heart: | mvninstall | 5m 43s | the patch passed | 
| +1 :green_heart: | compile | 1m 33s | the patch passed | 
| +1 :green_heart: | javac | 1m 33s | the patch passed | 
| +1 :green_heart: | checkstyle | 2m 8s | the patch passed | 
| +1 :green_heart: | whitespace | 0m 0s | The patch has no whitespace issues. | 
| +1 :green_heart: | shadedjars | 5m 20s | patch has no errors when building our shaded downstream artifacts. | 
| +1 :green_heart: | hadoopcheck | 18m 21s | Patch does not cause any errors with Hadoop 2.8.5 2.9.2 or 3.1.2. | 
| +1 :green_heart: | javadoc | 1m 1s | the patch passed | 
| +1 :green_heart: | findbugs | 6m 43s | the patch passed | 
||| _ Other Tests _ | 
| +1 :green_heart: | unit | 1m 59s | hbase-client in the patch passed. | 
| -1 :x: | unit | 196m 3s | hbase-server in the patch failed. | 
| +1 :green_heart: | asflicense | 1m 22s | The patch does not generate ASF License warnings. | 
| | | 271m 29s | | 


| Reason | Tests | 
|-------:|:------| 
| Failed junit tests | hadoop.hbase.master.TestMasterShutdown | 


| Subsystem | Report/Notes | 
|----------:|:-------------| 
| Docker | Client=19.03.5 Server=19.03.5 base: https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/2/artifact/out/Dockerfile | 
| GITHUB PR | https://github.com/apache/hbase/pull/977 | 
| JIRA Issue | HBASE-23628 | 
| Optional Tests | dupname asflicense javac javadoc unit spotbugs findbugs shadedjars hadoopcheck hbaseanti checkstyle compile | 
| uname | Linux 9c4e51ac674c 4.15.0-70-generic #79-Ubuntu SMP Tue Nov 12 10:36:11 UTC 2019 x86_64 GNU/Linux | 
| Build tool | maven | 
| Personality | /home/jenkins/jenkins-slave/workspace/HBase-PreCommit-GitHub-PR_PR-977/out/precommit/personality/provided.sh | 
| git revision | master / 06eff551c3 | 
| Default Java | 1.8.0_181 | 
| unit | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/2/artifact/out/patch-unit-hbase-server.txt | 
| Test Results | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/2/testReport/ | 
| Max. process+thread count | 4641 (vs. ulimit of 10000) | 
| modules | C: hbase-client hbase-server U: . | 
| Console output | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/2/console | 
| versions | git=2.11.0 maven=2018-06-17T18:33:14Z) findbugs=3.1.11 | 
| Powered by | Apache Yetus 0.11.1 https://yetus.apache.org | 


This message was automatically generated.",Apache-HBase,-223 days,N,belugabehr
tomee454,I reviewed the PR and it fixes the issue.,ivanjunckes,-369 days,N,otaviojava
tomee454,This is not ready. Missing apache header on the new file and there is no test. No idea what this is fixing.,jeanouii,-615 days,N,otaviojava
tomee454,"Looks a lot like we're skipping the current implementation and replacing it. Any reason we're not fixing this over here: https://github.com/apache/geronimo-openapi?

Also, if this is passing the TCK and there is a gap in the TCK testing, it sounds like it would be a good idea to fill that gap.",jgallimore,-246 days,Y,otaviojava
tomee454,@jgallimore you opened a new PR. Can we close this one?,jeanouii,-615 days,N,otaviojava
jena306,"Okay, now I get it. Agreed that number 3 is ""trying too hard"" and on the proposal to provide number 2 and document appropriate usage.",ajs6f,230 days,N,afs
spark18602,"**[Test build #79541 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/79541/testReport)** for PR 18602 at commit [`6f91356`](https://github.com/apache/spark/commit/6f9135645cd767e9d69d98157189c2e7ba08a5cc).
* This patch passes all tests.
* This patch merges cleanly.
* This patch adds no public classes.",SparkQA,101 days,N,jerryshao
spark18602,I commented on the bug.,vanzin,3 days,N,jerryshao
spark10293,"**[Test build #47652 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/47652/consoleFull)** for PR 10293 at commit [`e35d22d`](https://github.com/apache/spark/commit/e35d22db58081261292bb4be9271aa41e9f04ced).
- This patch passes all tests.
- This patch merges cleanly.
- This patch adds no public classes.
",SparkQA,58 days,N,cloud-fan
spark10293,"Overall LGTM
",marmbrus,-74 days,N,cloud-fan
spark10293,"**[Test build #47781 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/47781/consoleFull)** for PR 10293 at commit [`e35d22d`](https://github.com/apache/spark/commit/e35d22db58081261292bb4be9271aa41e9f04ced).
- This patch passes all tests.
- This patch merges cleanly.
- This patch adds no public classes.
",SparkQA,58 days,N,cloud-fan
spark10293,"Okay, I'm going to merge this into master while removing the unnecessary deletion. Thanks.
",marmbrus,-74 days,N,cloud-fan
spark10112,"**[Test build #47096 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/47096/consoleFull)** for PR 10112 at commit [`e89ed9b`](https://github.com/apache/spark/commit/e89ed9b20971fee280904c834b511db91b3bc633).
- This patch **fails Scala style tests**.
- This patch merges cleanly.
- This patch adds no public classes.
",SparkQA,131 days,N,JoshRosen
spark10112,"Can we separate the style check update from the sbt update?
",rxin,-3 days,N,JoshRosen
spark10112,"Since you're upgrading scalastyle, shouldn't you also modify pom.xml?
",vanzin,33 days,N,JoshRosen
spark10112,"**[Test build #47099 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/47099/consoleFull)** for PR 10112 at commit [`40ae332`](https://github.com/apache/spark/commit/40ae332bc3057a15f9265a5f0cbd9fcf47551a09).
- This patch passes all tests.
- This patch merges cleanly.
- This patch adds no public classes.
",SparkQA,131 days,N,JoshRosen
spark10112,"retest this please
",vanzin,33 days,N,JoshRosen
spark10112,"LGTM, btw.
",vanzin,33 days,N,JoshRosen
spark10112,"**[Test build #47146 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/47146/consoleFull)** for PR 10112 at commit [`54c6887`](https://github.com/apache/spark/commit/54c6887a6c4cae459ab48a87ee29ae377d1c3900).
- This patch passes all tests.
- This patch merges cleanly.
- This patch adds no public classes.
",SparkQA,131 days,N,JoshRosen
spark10112,"Sure - go head if it is hard to pull out.
",rxin,-3 days,N,JoshRosen
spark10112,"I've merged this in master.
",rxin,-3 days,N,JoshRosen
storm1094,"OK I am +1 for this then.
",revans2,-563 days,N,arunmahadevan
storm1094,"+1. @arunmahadevan this should go into 1.x-branch as well.
",harshach,-456 days,N,arunmahadevan
storm1094,"Thanks @arunmahadevan merged into master. Can you please open another PR for 1.x-branch. 
",harshach,-456 days,N,arunmahadevan
tinkerpop315,"My inclination is to merge this, but I've started a discussion on this PR on the dev list to make sure the community is ok with this change: https://lists.apache.org/thread.html/Zlxuc3qc7rkc3fq
",spmallette,-449 days,N,analytically
tinkerpop315,"@analytically did you take note of the discussion on the dev mailing list? would you mind going the extra step to update other tests that might be a problem (you're likely to run into them later as you continue with your implementation anyway)?

Also, this is a sufficient enough change imo to warrant an entry to the upgrade documentation. A note in this section:

https://github.com/apache/incubator-tinkerpop/blob/tp31/docs/src/upgrade/release-3.1.x-incubating.asciidoc#graph-database-providers

that explains that graph providers should no longer rely on the test suite to validate that hypens work for property keys would be a worthwhile comment. Please also add an entry to the CHANGELOG 

https://github.com/apache/incubator-tinkerpop/blob/tp31/CHANGELOG.asciidoc#tinkerpop-313-not-officially-released-yet

which would make this PR fully complete. Sorry for the extra effort on this, but it's a good change and should be done ""right"".
",spmallette,-449 days,Y,analytically
tinkerpop315,"@analytically do you think you will be able to come back to this PR at some point?
",spmallette,-449 days,N,analytically
tinkerpop315,"@analytically I see that you made some updates. I think you might need to rebase against tp31 though to resolve conflicts. Other than that, is this ready for review at this point?
",spmallette,-449 days,N,analytically
tinkerpop315,"@analytically did you happen to see my previous comment about rebasing/fixing conflicts on this PR so that we can begin code review/testing on our end? is that something you have some time to come back to soon?
",spmallette,-449 days,N,analytically
tinkerpop315,"something looks strange - why do you have 27 commits on this PR? shouldn't it just be these: 990a76c, 15c06f5, 15c06f5, 5322ee3? 
",spmallette,-449 days,N,analytically
tinkerpop315,"@analytically we're trying to finish reviewing PRs for the upcoming release of 3.2.1 and 3.1.3 - did you happen to see my note above?
",spmallette,-449 days,N,analytically
tinkerpop315,"All tests pass with `docker/build.sh -t -n -i`

VOTE +1
",spmallette,-449 days,N,analytically
tinkerpop315,"VOTE: +1
",dkuppitz,-450 days,N,analytically
tinkerpop315,"LGTM. Is there a JIRA associated with this?

VOTE: +1
",pluradj,-354 days,N,analytically
tinkerpop315,"There wasn't a JIRA that I can remember @pluradj - just something discussed on the mailing list.
",spmallette,-449 days,N,analytically
ant141,"I like the approach of moving the logic for fixing the lat lines to a separate filter. Kudos for adding tests ;-)

A different approach could be to allow an alternative set of filters to be applied before concatenating, i.e. allowing users to define two sets of filters one to apply on each input stream individually and one to be applied to the merged stream. Naming could become an issue (and I've got a long track of picking bad names myself).

I'm not asking you to change your PR, I'd rather like to discuss the different approaches to see which would be best. So what do you think?",bodewig,-2188,Y,TheConstructor
ant36,"ant-unit-1.3 is in the repo (https://github.com/apache/ant/blob/master/lib/optional/ant-antunit-1.3.jar).
It seems that you use an Ant version for building this which does not contain this jar.

If you are building Ant itself I recommend cleaning the environment before (unset ANT_HOME) and bootstrap Ant by itself.",janmaterne,-210,Y,GKFX
ant36,"Looks great, many thanks! I'll merge it to both branches and add `@since` markers.

We'll need to update the manual (probably of zip, jar, war and ear). Also some additional unit tests for `parseLenient` would be good (good old JUnit tests). If you want to take a stab at either, please do so in a fresh PR. Otherwise I'll carve out some time to do it myself.",bodewig,-1031,Y,GKFX
ant37,Great change and I agree it should be fixed in the central location. Many thanks!,bodewig,-994,Y,jaikiran
ant40,"Many thanks.

I've fixed the line-ends to contain line-feeds only as I couldn't merge your patch on a Linux box, therefore the PR now has conflicts. Sorry about that.

As written the tests depend on the timezone of the machine  running the tests:

```
Testcase: testLenientDateTime took 0,006 sec
	FAILED
expected:<1488622440000> but was:<1488618840000>
junit.framework.AssertionFailedError: expected:<1488622440000> but was:<1488618840000>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.tools.ant.util.DateUtilsTest.testLenientDateTime(DateUtilsTest.java:119)
```

this is on GMT+2 and off by an hour.  But as this is off by an hour I think it is more about me being in daylight saving time right now while the date you picked was not (most countries of the EU switch on the last weekends of March and October).",bodewig,-1031,Y,GKFX
ant40,"I haven't found the time to dig deeply into the issue, but it would be good to understand where exactly things turn into unexpected directions.",bodewig,-1031,Y,GKFX
ant41,"Many thanks, merged.

I'd like to add your name to the contributors list, the `contributors,xml` file wants a first and a last name and I'm not completely sure what to put where. :-)",bodewig,-1075,Y,Kui-Liu
ant44,"We've got pretty strict API backwards compatibilty rules, so we cannot change the visibility of any elements that have been part of a release already",bodewig,-1075,Y,Kui-Liu
ant45,"While i agree and prefer returning Optional<?> or an empty List, in this case this would break backwards compatiblity. And therefore this is a no go on the public API.",janmaterne,-254,Y,Kui-Liu
ant47,"Similar to #44 we cannot change a public API and thus will not merge this request.

In this particular case we even once changed it and had to revert the field to be non-final as we broike the Eclipse integration https://github.com/apache/ant/commit/984a03d1ceb6e4b5d194e4d639d0b0fca46d92be",bodewig,-1075,Y,Kui-Liu
ant49,"Thanks @jaikiran, I've added a buch of inline comments.

Overall I'm in favor of this change and if you want to spend the time on fixing the bugzilla issue in a Java5 friendly way for 1.9.x that would certainly be good - bit not something I'd see as an requirement.

The `SymlinkTest` still ensures tests are skipped on platforms that are not Unix (see the `assume` in `setup`). I'm not sure if we can figure out whether Java supports symlinks on the platform easily but it would certainly be good to test things on non-Unix platforms if we claim the task should work.
",bodewig,-994,Y,jaikiran
beam10025,"I'm not a big fan of ifs either but the aim was to have a single production code (for maintenance) in a single module (for users) . And as there were only some located differences (mainly split), a simple if could do. 
Also please note 2 things: discrete versions are used instead of > x in the code to avoid running on non-tested versions. Also we used a very low level ES client (rest client) which is (was?) the only one compatible with all the versions of ES, specially to have a single production code base of ESIO.

If you want, you can submit a refactor PR to improve it. I'll be happy of it. 

",echauchot,-1074,Y,regfaker
beam10025,"@regfaker I'm sorry but the PR below was submitted 1 day before yours and addresses the same (+ the maxRetryTimeout deprecation). 
https://github.com/apache/beam/pull/10010
I need to close one of the PRs and the only valid criteria is the first submitted PR.
I know it can be frustrating, and I'm sorry.
Thanks anyway for your work and I hope you would not be reluctant to submitting other PRs to Beam in the future, the Beam community would love to see your future work.",echauchot,-1074,Y,regfaker
beam10027,"This is fine as-is, should have made clearer that I'm just missing a comment in the translator. I'd also favor a unified style to initialize pipeline options, instead of the two different ways we have now. We can tackle this in a follow-up. Merging.",mxm,-804,Y,dmvk
beam10028,"Tests pass on the release branch. https://gradle.com/s/5jl76y2tkiwmc

So something about this change is causing the error deterministically, as you say. Since it is healthy on `master`, perhaps there are other coupled commits that need to be cherrypicked.",kennknowles,-805,Y,dmvk
beam1003,"I don't see use of `SourceTestUtils` to test split and estimated size. You can take a look on what I did in ElasticsearchIO (https://github.com/jbonofre/incubator-beam/blob/BEAM-425-ELASTICSEARCHIO/sdks/java/io/elasticsearch/src/test/java/org/apache/beam/sdk/io/elasticsearch/ElasticsearchIOTest.java) as example.
",jbonofre,-181,Y,dkulp
beam1003,"`SourceTestUtils` doesn't invoke `splitIntoBundles` per se, but it provides `assertSourcesEqualReferenceSource`. You should call `splitIntoBundles` yourself with various parameters and verify the result using that function.

It also indeed doesn't test `getEstimatedSize` - that has to be simply unit-tested manually.
",jkff,-158,Y,dkulp
beam10038,"> My last two jenkins jobs indicated it took 0s to run, but the logs show that it was aborted after 2 hours. Any idea what could be going on?

It looks like it hit the 2h Jenkins timeout. Should be extended to 3h once https://github.com/apache/beam/pull/10234 is merged and the next seed job runs. (I'll start one in #10234)",udim,-613,Y,chadrik
beam10040,These do not really meet the definition of a release blocker (not a regression from previous release) but given the amount of testing that was done I think they are safe to merge.,tweise,117,Y,mxm
beam10078,"Hi @Akshay-Iyangar, great that you want to contribute to this PR, I hope that @ajothomas won't be mind. Please, let me know when it will be ready for review. I'll be happy to have it merged finally. ",aromanenko-dev,-489,Y,ajothomas
beam10078,"@ajothomas, @aromanenko-dev can you'll have a look? 
Changes made got rid of the `WrappedSnsResponse` to send the exception back to the client. An exception with AWS SNS will should actually throw an exception.
For Response from the SNS client. I'm just returning the `requestElement, statusText and statusCode`
to the users so they can determine how they want to handle the responses at their side.
L
",Akshay-Iyangar,266,Y,ajothomas
beam10078,"@Akshay-Iyangar Thank you for taking care of this! Could you exclude a commit that merges master into feature branch and use rebase instead? 

Also, please squash all **your** commits but keep **Ajo Thomas** commit as well (to respect all credits). 

And finally, please, format all commits with the following pattern - ""[BEAM-8542] Commit message"" (see [Contribution Guide](https://beam.apache.org/contribute/#make-your-change))",aromanenko-dev,-489,Y,ajothomas
beam10078,"I did another review round and see that we still have user API breaking changes there. 
I'm wondering if we can keep the current sync write API and just add new async write API to this? If sync API is really slow and useless then we can deprecate it and remove in the future but I'm against breaking user API changes without deprecation period (usually it's 3 releases for Beam). 
@Akshay-Iyangar wdyt?",aromanenko-dev,-489,Y,ajothomas
beam10078,"@aromanenko-dev - No issues, we can keep both the sync and the async for time-being. Are you ok with me just passing something from the client for using the async in that case so that the sync code need not be changed?",Akshay-Iyangar,266,Y,ajothomas
beam10410,"It looks like [beam_PostCommit_Java_PortabilityApi](https://builds.apache.org/job/beam_PostCommit_Java_PortabilityApi/) has been timing out at 4 hours since #10268 was merged. First failure is here: https://builds.apache.org/job/beam_PostCommit_Java_PortabilityApi/3625/

The run before that took just 50 mins. Will the changes in this PR address that?
",TheNeuralBit,574,Y,lgajowy
beam10410,"It seems as though the artifact server doesn't handle having multiple copies of the same artifact.

I'm seeing this in the worker logs:
```
I 2019-12-18T20:20:07.933280467Z 2019/12/18 20:20:07 Initializing java harness: /opt/apache/beam/boot --id=1 --logging_endpoint=localhost:12370 --control_endpoint=localhost:12371 --artifact_endpoint=localhost:12372 --provision_endpoint=localhost:12373 --semi_persist_dir=/var/opt/google
 
I 2019-12-18T20:20:14.570Z Worker harness starting with: {""options"":{""experiments"":[""beam_fn_api"",""beam_fn_api"",""use_staged_dataflow_worker_jar""],""project"":""apache-beam-testing"",""dataflowJobId"":""2019-12-18_12_18_43-4463651132060001350"",""apiRootUrl"":""https://dataflow.googleapis.com/"",""defaultWorkerLogLevel"":""INFO"",""region"":""us-central1""}} 
I 2019-12-18T20:20:17.204Z Not using conscrypt SSL. Note this is the default Java behavior, but may have reduced performance. To use conscrypt SSL pass pipeline option --experiments=enable_conscrypt_security_provider 
I 2019-12-18T20:20:17.417Z Launched Beam Fn Logging service url: ""localhost:12370""
 
I 2019-12-18T20:20:17.428Z Launched Beam Fn Control service url: ""localhost:12371""
 
I 2019-12-18T20:20:17.432Z Launched Beam Fn Data service url: ""localhost:12371""
 
I 2019-12-18T20:22:10.692155827Z 2019/12/18 20:22:10 Failed to retrieve staged files: failed to dial server at localhost:12372
 
I 2019-12-18T20:22:10.692189943Z 	caused by:
 
I 2019-12-18T20:22:10.692195647Z context deadline exceeded
 
I 2019-12-18T20:22:11.421271885Z 2019/12/18 20:22:11 Initializing java harness: /opt/apache/beam/boot --id=1 --logging_endpoint=localhost:12370 --control_endpoint=localhost:12371 --artifact_endpoint=localhost:12372 --provision_endpoint=localhost:12373 --semi_persist_dir=/var/opt/google
 
I 2019-12-18T20:24:11.428638869Z 2019/12/18 20:24:11 Failed to retrieve staged files: failed to dial server at localhost:12372
 
I 2019-12-18T20:24:11.428673501Z 	caused by:
 
I 2019-12-18T20:24:11.428679088Z context deadline exceeded
 
I 2019-12-18T20:24:23.605267533Z 2019/12/18 20:24:23 Initializing java harness: /opt/apache/beam/boot --id=1 --logging_endpoint=localhost:12370 --control_endpoint=localhost:12371 --artifact_endpoint=localhost:12372 --provision_endpoint=localhost:12373 --semi_persist_dir=/var/opt/google
 
I 2019-12-18T20:26:23.612051533Z 2019/12/18 20:26:23 Failed to retrieve staged files: failed to dial server at localhost:12372
 
I 2019-12-18T20:26:23.612105276Z 	caused by:
 
I 2019-12-18T20:26:23.612111707Z context deadline exceeded
```

and it seems as though the artifact server is failing to start because of a duplicate artifact:
```
2019-12-18 20:44:44.426 GMT
2019/12/18 20:44:44 Starting retrieval proxy on localhost:12372
 
2019-12-18 20:44:44.436 GMT
2019/12/18 20:44:44 Warning: duplicate package kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar:gs://temp-storage-for-end-to-end-tests/pardotest0basictests0testpardoincustomtransform-jenkins-1218201828-52b8a37f/output/results/staging/kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar -> gs://temp-storage-for-end-to-end-tests/pardotest0basictests0testpardoincustomtransform-jenkins-1218201828-52b8a37f/output/results/staging/kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar
2019-12-18 20:44:44.436 GMT
2019/12/18 20:44:44 Warning: duplicate package kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar:gs://temp-storage-for-end-to-end-tests/pardotest0basictests0testpardoincustomtransform-jenkins-1218201828-52b8a37f/output/results/staging/kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar -> gs://temp-storage-for-end-to-end-tests/pardotest0basictests0testpardoincustomtransform-jenkins-1218201828-52b8a37f/output/results/staging/kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar
2019-12-18 20:44:44.436 GMT
2019/12/18 20:44:44 Warning: duplicate package kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar:gs://temp-storage-for-end-to-end-tests/pardotest0basictests0testpardoincustomtransform-jenkins-1218201828-52b8a37f/output/results/staging/kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar -> gs://temp-storage-for-end-to-end-tests/pardotest0basictests0testpardoincustomtransform-jenkins-1218201828-52b8a37f/output/results/staging/kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar
2019-12-18 20:44:44.554 GMT
2019/12/18 20:44:44 Found: name:""dataflow-worker.jar"" permissions:420
2019-12-18 20:44:44.568 GMT
2019/12/18 20:44:44 Found: name:""gradle-worker-t0NPniJD_BNrph4tUjEAcA.jar"" permissions:420
2019-12-18 20:44:44.589 GMT
2019/12/18 20:44:44 Found: name:""beam-runners-google-cloud-dataflow-java-2.19.0-SNAPSHOT-vqQCaAYDJbCaZ0oA4mNI5g.jar"" permissions:420
2019-12-18 20:44:44.609 GMT
2019/12/18 20:44:44 Found: name:""beam-runners-google-cloud-dataflow-java-2.19.0-SNAPSHOT-tests-M5hsuzpWA1C_SievTBu8Iw.jar"" permissions:420
2019-12-18 20:44:44.627 GMT
2019/12/18 20:44:44 Found: name:""beam-sdks-java-io-google-cloud-platform-2.19.0-SNAPSHOT-gF7joDLdfBhIkT6CNOm0Jw.jar"" permissions:420
2019-12-18 20:44:44.642 GMT
2019/12/18 20:44:44 Found: name:""beam-sdks-java-io-google-cloud-platform-2.19.0-SNAPSHOT-tests-22xEYN8ClmCrqJbqZ7bR6g.jar"" permissions:420
2019-12-18 20:44:44.660 GMT
2019/12/18 20:44:44 Found: name:""beam-sdks-java-test-utils-2.19.0-SNAPSHOT-tests-_s4rMauR1JmL2T1vdRmCpQ.jar"" permissions:420
2019-12-18 20:44:44.678 GMT
2019/12/18 20:44:44 Found: name:""beam-sdks-java-test-utils-2.19.0-SNAPSHOT-MB4nyV8X4YHcKD-5k8L3uQ.jar"" permissions:420
2019-12-18 20:44:44.694 GMT
2019/12/18 20:44:44 Found: name:""beam-sdks-java-extensions-google-cloud-platform-core-2.19.0-SNAPSHOT-xG4ii-F4pQDibsy1ddSgOw.jar"" permissions:420
2019-12-18 20:44:44.713 GMT
2019/12/18 20:44:44 Found: name:""beam-sdks-java-extensions-google-cloud-platform-core-2.19.0-SNAPSHOT-tests-V7CS-3V7KfLhfJpUNIUVqA.jar"" permissions:420
2019-12-18 20:44:44.734 GMT
2019/12/18 20:44:44 Found: name:""beam-runners-core-construction-java-2.19.0-SNAPSHOT-_qToI7zqjDySCS_yxe3L3Q.jar"" permissions:420
2019-12-18 20:44:44.752 GMT
2019/12/18 20:44:44 Found: name:""beam-sdks-java-extensions-protobuf-2.19.0-SNAPSHOT-vubRi4-OFW-I0LQ7KprTXg.jar"" permissions:420
2019-12-18 20:44:44.768 GMT
2019/12/18 20:44:44 Found: name:""beam-runners-direct-java-2.19.0-SNAPSHOT-MY3GpZvnewQkmr_lsV1-WA.jar"" permissions:420
2019-12-18 20:44:44.794 GMT
2019/12/18 20:44:44 Found: name:""beam-sdks-java-io-common-2.19.0-SNAPSHOT-tests-6KdNrj9GD_69MH2GU1U5PQ.jar"" permissions:420
2019-12-18 20:44:44.813 GMT
2019/12/18 20:44:44 Found: name:""beam-sdks-java-io-common-2.19.0-SNAPSHOT-jyHKmvgVAwC95_AE4Ob6LQ.jar"" permissions:420
2019-12-18 20:44:44.831 GMT
2019/12/18 20:44:44 Found: name:""beam-sdks-java-core-2.19.0-SNAPSHOT-tests-DJ4CnSGwjIRcevjig0iA3g.jar"" permissions:420
2019-12-18 20:44:44.849 GMT
2019/12/18 20:44:44 Found: name:""beam-sdks-java-core-2.19.0-SNAPSHOT-9x5vxaFdu-Ny5rvW8SAy6Q.jar"" permissions:420
2019-12-18 20:44:44.866 GMT
2019/12/18 20:44:44 Found: name:""hamcrest-library-2.1-PvgiE9wlzQ4mXcFvbiXZXA.jar"" permissions:420
2019-12-18 20:44:44.954 GMT
2019/12/18 20:44:44 Found: name:""guava-testlib-25.1-jre-VHcJjyv-_w8Il-qpUZrmtg.jar"" permissions:420
2019-12-18 20:44:44.977 GMT
2019/12/18 20:44:44 Found: name:""junit-quickcheck-core-0.8-EgEvYI3gIP_d2fBpDZJ42w.jar"" permissions:420
2019-12-18 20:44:45.001 GMT
2019/12/18 20:44:45 Found: name:""powermock-module-junit4-2.0.2-rbQRTz39L0xkwlZMsTIavw.jar"" permissions:420
2019-12-18 20:44:45.025 GMT
2019/12/18 20:44:45 Found: name:""grpc-all-1.17.1-ObISw2fPGxSCJVcuI5PXVg.jar"" permissions:420
2019-12-18 20:44:45.046 GMT
2019/12/18 20:44:45 Found: name:""grpc-testing-1.17.1-ZCm5QWshyeL2B8wGncO7OQ.jar"" permissions:420
2019-12-18 20:44:45.072 GMT
2019/12/18 20:44:45 Found: name:""powermock-module-junit4-common-2.0.2-El4eEVG3F1FJspjaZ06JzA.jar"" permissions:420
2019-12-18 20:44:45.092 GMT
2019/12/18 20:44:45 Found: name:""junit-4.13-beta-3-SR8V8LeBEDL0Hb3Yr3GZaA.jar"" permissions:420
2019-12-18 20:44:45.115 GMT
2019/12/18 20:44:45 Found: name:""hamcrest-core-2.1-wOGHxn46eRDV1UiUlkzBlQ.jar"" permissions:420
2019-12-18 20:44:45.153 GMT
2019/12/18 20:44:45 Found: name:""beam-model-pipeline-2.19.0-SNAPSHOT-y_W94tehet9I9TeRqo3PEA.jar"" permissions:420
2019-12-18 20:44:45.179 GMT
2019/12/18 20:44:45 Found: name:""beam-model-job-management-2.19.0-SNAPSHOT-JRNZ_ELrzeoUfantairRlw.jar"" permissions:420
2019-12-18 20:44:45.202 GMT
2019/12/18 20:44:45 Found: name:""beam-vendor-bytebuddy-1_9_3-0.1-uWJWmyXQR6o6a6AbF7r0TA.jar"" permissions:420
2019-12-18 20:44:45.232 GMT
2019/12/18 20:44:45 Found: name:""beam-vendor-guava-26_0-jre-0.1-6QokGG9WNL5MyYZ3G5HWjA.jar"" permissions:420
2019-12-18 20:44:45.252 GMT
2019/12/18 20:44:45 Found: name:""google-cloud-bigquerystorage-0.79.0-alpha-6UpzWBLaf_wow6RacZONWA.jar"" permissions:420
2019-12-18 20:44:45.280 GMT
2019/12/18 20:44:45 Found: name:""bigtable-client-core-1.8.0-fFSIJpUcSXpWhLZcfslZzQ.jar"" permissions:420
2019-12-18 20:44:45.296 GMT
2019/12/18 20:44:45 Found: name:""google-cloud-spanner-1.6.0-SshEQyw4IrAiG9CeGXOlkw.jar"" permissions:420
2019-12-18 20:44:45.372 GMT
2019/12/18 20:44:45 Found: name:""google-cloud-bigtable-0.73.0-alpha-eB5GY2vHSW64cCChZOS18w.jar"" permissions:420
2019-12-18 20:44:45.409 GMT
2019/12/18 20:44:45 Found: name:""google-cloud-bigtable-admin-0.73.0-alpha-tf8RW5s2Pg4xX_7h-hnKCA.jar"" permissions:420
2019-12-18 20:44:45.427 GMT
2019/12/18 20:44:45 Found: name:""google-cloud-core-grpc-1.61.0-0jZ_NlVzGDZOmvFmfTqJwg.jar"" permissions:420
2019-12-18 20:44:45.451 GMT
2019/12/18 20:44:45 Found: name:""gax-grpc-1.38.0-3ClsPaT9rq8Z5k4YOIfuXg.jar"" permissions:420
2019-12-18 20:44:45.475 GMT
2019/12/18 20:44:45 Found: name:""google-cloud-bigquery-1.28.0-Kl2D-E5AKdJ_R0IOB4xEZA.jar"" permissions:420
2019-12-18 20:44:45.494 GMT
2019/12/18 20:44:45 Found: name:""google-cloud-core-http-1.55.0-BNHYMDYs4SQW4PB1djuVig.jar"" permissions:420
2019-12-18 20:44:45.513 GMT
2019/12/18 20:44:45 Found: name:""google-cloud-core-1.61.0-IlSrOl7G255S3mvdat3lmw.jar"" permissions:420
2019-12-18 20:44:45.530 GMT
2019/12/18 20:44:45 Found: name:""gax-httpjson-0.52.0-4ATeMZWHBufEm6llIAqk-g.jar"" permissions:420
2019-12-18 20:44:45.547 GMT
2019/12/18 20:44:45 Found: name:""gax-1.38.0-FCo0lAFSkhHsRCXTmZ2K0g.jar"" permissions:420
2019-12-18 20:44:45.567 GMT
2019/12/18 20:44:45 Found: name:""grpc-alts-1.17.1-utvCNzI1dYk3drhIBG-azw.jar"" permissions:420
2019-12-18 20:44:45.584 GMT
2019/12/18 20:44:45 Found: name:""google-auth-library-oauth2-http-0.12.0-uhKYUO6-pQeD5qoEqSlXPQ.jar"" permissions:420
2019-12-18 20:44:45.599 GMT
2019/12/18 20:44:45 Found: name:""google-api-services-clouddebugger-v2-rev20181114-1.28.0-U-z8PFQLYxPCTBDukg6SIw.jar"" permissions:420
2019-12-18 20:44:45.621 GMT
2019/12/18 20:44:45 Found: name:""google-api-services-dataflow-v1b3-rev20190927-1.28.0-ZgWxFdWAP6FEyoWz8lP5uw.jar"" permissions:420
2019-12-18 20:44:45.643 GMT
2019/12/18 20:44:45 Found: name:""gcsio-1.9.16-iBTmtmP7wvXHVlGNaBCuzg.jar"" permissions:420
2019-12-18 20:44:45.656 GMT
2019/12/18 20:44:45 Found: name:""util-1.9.16-jjyXrI5LEGRyIaXFKj9tvA.jar"" permissions:420
2019-12-18 20:44:45.687 GMT
2019/12/18 20:44:45 Found: name:""google-api-services-storage-v1-rev20181109-1.28.0-DJqyY-K3Ol9Ssdd-OqNoYg.jar"" permissions:420
2019-12-18 20:44:45.713 GMT
2019/12/18 20:44:45 Found: name:""google-api-services-cloudresourcemanager-v1-rev20181015-1.28.0-XNoKjKa8eomfOz0s-YI0iQ.jar"" permissions:420
2019-12-18 20:44:45.733 GMT
2019/12/18 20:44:45 Found: name:""google-api-services-bigquery-v2-rev20181221-1.28.0-R0jhsGj7UgWAkFOqtc-S2g.jar"" permissions:420
2019-12-18 20:44:45.753 GMT
2019/12/18 20:44:45 Found: name:""google-api-services-pubsub-v1-rev20181213-1.28.0-I5vY-LAnGjEk3u56f0tU0w.jar"" permissions:420
2019-12-18 20:44:45.768 GMT
2019/12/18 20:44:45 Found: name:""datastore-v1-proto-client-1.6.3-yZQu0PLVT3TnUT_wNpicFA.jar"" permissions:420
2019-12-18 20:44:45.820 GMT
2019/12/18 20:44:45 Found: name:""google-api-client-java6-1.28.0-HAodKBbdqyrFdBPEZvVcaA.jar"" permissions:420
2019-12-18 20:44:45.841 GMT
2019/12/18 20:44:45 Found: name:""google-api-client-jackson2-1.28.0--W6_uwURyqtauUTVXwhXaw.jar"" permissions:420
2019-12-18 20:44:45.875 GMT
2019/12/18 20:44:45 Found: name:""google-api-client-1.28.0-SDG80NMNEzgrOvUKy7v1Pg.jar"" permissions:420
2019-12-18 20:44:45.898 GMT
2019/12/18 20:44:45 Found: name:""google-http-client-jackson2-1.28.0-Yyuo_SNjVh_9zfbnsFTr7A.jar"" permissions:420
2019-12-18 20:44:45.916 GMT
2019/12/18 20:44:45 Found: name:""google-oauth-client-java6-1.28.0-goLBdHjMZJZLuwqWvoQzGQ.jar"" permissions:420
2019-12-18 20:44:45.932 GMT
2019/12/18 20:44:45 Found: name:""google-oauth-client-1.28.0-uwS8kMzmcLlyBTJ2BAb7yg.jar"" permissions:420
2019-12-18 20:44:45.949 GMT
2019/12/18 20:44:45 Found: name:""google-http-client-apache-2.0.0-2uE2vYvcJczYvnh6THKxTw.jar"" permissions:420
2019-12-18 20:44:45.969 GMT
2019/12/18 20:44:45 Found: name:""google-http-client-protobuf-1.28.0-TVX5WOhxCLsZmjp1E38R9g.jar"" permissions:420
2019-12-18 20:44:45.994 GMT
2019/12/18 20:44:45 Found: name:""google-http-client-jackson-1.28.0-JE8m9mp3cvILoTTSsuOGLA.jar"" permissions:420
2019-12-18 20:44:46.013 GMT
2019/12/18 20:44:46 Found: name:""google-http-client-appengine-1.27.0-TdVXGCdmlhycAZxqyyGADQ.jar"" permissions:420
2019-12-18 20:44:46.039 GMT
2019/12/18 20:44:46 Found: name:""google-http-client-1.28.0-d1m2BjJFwtcUlfahuoccNA.jar"" permissions:420
2019-12-18 20:44:46.069 GMT
2019/12/18 20:44:46 Found: name:""grpc-auth-1.17.1-FGjOS4ligPUTMy8WuF05hg.jar"" permissions:420
2019-12-18 20:44:46.092 GMT
2019/12/18 20:44:46 Found: name:""grpc-netty-1.17.1-4r8YjiC7Mk6Ki3FAZghiEQ.jar"" permissions:420
2019-12-18 20:44:46.115 GMT
2019/12/18 20:44:46 Found: name:""grpc-google-cloud-pubsub-v1-1.43.0-nl20guInYAxfIy09hLdNYQ.jar"" permissions:420
2019-12-18 20:44:46.239 GMT
2019/12/18 20:44:46 Found: name:""grpc-google-cloud-bigquerystorage-v1beta1-0.44.0-SsBroVH9fPiss71kHPKsmQ.jar"" permissions:420
2019-12-18 20:44:46.270 GMT
2019/12/18 20:44:46 Found: name:""grpc-google-common-protos-1.12.0-kY92YHJ3__3_EfGz7q7QJQ.jar"" permissions:420
2019-12-18 20:44:46.288 GMT
2019/12/18 20:44:46 Found: name:""grpc-google-cloud-bigtable-v2-0.38.0-q6Se3msUfLxCDeBbiqj1tA.jar"" permissions:420
2019-12-18 20:44:46.304 GMT
2019/12/18 20:44:46 Found: name:""grpc-google-cloud-bigtable-admin-v2-0.38.0-73d39v9pMyfVcnsTMrtv7Q.jar"" permissions:420
2019-12-18 20:44:46.327 GMT
2019/12/18 20:44:46 Found: name:""grpc-google-cloud-spanner-v1-1.6.0-DC0Nq4gVV4LS1Oe0H3HegA.jar"" permissions:420
2019-12-18 20:44:46.346 GMT
2019/12/18 20:44:46 Found: name:""grpc-google-cloud-spanner-admin-database-v1-1.6.0-7T0gXklb71lKsYSs9DFN0w.jar"" permissions:420
2019-12-18 20:44:46.371 GMT
2019/12/18 20:44:46 Found: name:""grpc-google-cloud-spanner-admin-instance-v1-1.6.0-CrMYysm29EPdTKEjGVeLIw.jar"" permissions:420
2019-12-18 20:44:46.417 GMT
2019/12/18 20:44:46 Found: name:""grpc-grpclb-1.17.1-VaGwdjH7d-oSQhPxT2IdGA.jar"" permissions:420
2019-12-18 20:44:46.448 GMT
2019/12/18 20:44:46 Found: name:""grpc-stub-1.17.1-q57ZbJwKOpHX7YVm-7htNw.jar"" permissions:420
2019-12-18 20:44:46.477 GMT
2019/12/18 20:44:46 Found: name:""grpc-protobuf-1.17.1-gkIppisCtEE4HD6U0fv4tA.jar"" permissions:420
2019-12-18 20:44:46.501 GMT
2019/12/18 20:44:46 Found: name:""grpc-netty-shaded-1.17.1-wHeHRt2qhBMCJwd7qhVwvw.jar"" permissions:420
2019-12-18 20:44:46.515 GMT
2019/12/18 20:44:46 Found: name:""opencensus-contrib-grpc-util-0.17.0-iAypGz9jPIeblJhNhFTA7Q.jar"" permissions:420
2019-12-18 20:44:46.546 GMT
2019/12/18 20:44:46 Found: name:""grpc-okhttp-1.17.1-Ym33a1M8OEav6F7N12g25A.jar"" permissions:420
2019-12-18 20:44:46.569 GMT
2019/12/18 20:44:46 Found: name:""grpc-protobuf-nano-1.17.1-v769mz-qvJS69e3CdEG1cw.jar"" permissions:420
2019-12-18 20:44:46.585 GMT
2019/12/18 20:44:46 Found: name:""grpc-protobuf-lite-1.17.1-Bl91ZSPClD_iVyq27vy-Tw.jar"" permissions:420
2019-12-18 20:44:46.601 GMT
2019/12/18 20:44:46 Found: name:""grpc-core-1.17.1-Qs78YGV9O_x9ZproeRWUWg.jar"" permissions:420
2019-12-18 20:44:46.624 GMT
2019/12/18 20:44:46 Found: name:""opencensus-contrib-http-util-0.18.0-rhXNw-oEIYyewUcIytLblA.jar"" permissions:420
2019-12-18 20:44:46.641 GMT
2019/12/18 20:44:46 Found: name:""proto-google-cloud-datastore-v1-0.44.0-elmwKzdsUHaqeLHDNgnRRA.jar"" permissions:420
2019-12-18 20:44:46.658 GMT
2019/12/18 20:44:46 Found: name:""proto-google-cloud-bigquerystorage-v1beta1-0.83.0-AN9CMtFFGiHEo_KfhDFXKw.jar"" permissions:420
2019-12-18 20:44:46.683 GMT
2019/12/18 20:44:46 Found: name:""proto-google-cloud-bigtable-v2-0.44.0-3UqoMk6S7yLqYAP51vS80A.jar"" permissions:420
2019-12-18 20:44:46.699 GMT
2019/12/18 20:44:46 Found: name:""proto-google-cloud-pubsub-v1-1.43.0-H05P5I68_0NNGLjnpRu-Ow.jar"" permissions:420
2019-12-18 20:44:46.716 GMT
2019/12/18 20:44:46 Found: name:""proto-google-cloud-spanner-admin-database-v1-1.6.0-oJs46XQ701jPN_K3eNCk6g.jar"" permissions:420
2019-12-18 20:44:46.730 GMT
2019/12/18 20:44:46 Found: name:""proto-google-cloud-bigtable-admin-v2-0.38.0-giJ--Dr2jXieoiK-g0ip0Q.jar"" permissions:420
2019-12-18 20:44:46.746 GMT
2019/12/18 20:44:46 Found: name:""proto-google-cloud-spanner-admin-instance-v1-1.6.0-k_koDbxapZBjr-nrHQW8Iw.jar"" permissions:420
2019-12-18 20:44:46.770 GMT
2019/12/18 20:44:46 Found: name:""proto-google-iam-v1-0.12.0-KtsSGk0GwozxZp-QSDLgQQ.jar"" permissions:420
2019-12-18 20:44:46.788 GMT
2019/12/18 20:44:46 Found: name:""proto-google-cloud-spanner-v1-1.6.0-ZxGDeokcBrWzu3ayFr2ydA.jar"" permissions:420
2019-12-18 20:44:46.818 GMT
2019/12/18 20:44:46 Found: name:""api-common-1.8.1-g5ubgp_2pxctZAsz-8Lhsw.jar"" permissions:420
2019-12-18 20:44:46.833 GMT
2019/12/18 20:44:46 Found: name:""protobuf-java-util-3.6.0-92eguGRlzdJ7ZLzQCUAmgQ.jar"" permissions:420
2019-12-18 20:44:46.854 GMT
2019/12/18 20:44:46 Found: name:""guava-25.1-jre-2jg4hH0QmsQ18NPtSuHHlA.jar"" permissions:420
2019-12-18 20:44:46.871 GMT
2019/12/18 20:44:46 Found: name:""google-extensions-0.3.1-6lp0wo6CspsO1VNozDOfXA.jar"" permissions:420
2019-12-18 20:44:46.959 GMT
2019/12/18 20:44:46 Found: name:""flogger-system-backend-0.3.1-Z8I0E_LCTwJSgLdOjwU9TA.jar"" permissions:420
2019-12-18 20:44:46.978 GMT
2019/12/18 20:44:46 Found: name:""flogger-0.3.1-CRV_dWA6DF7OAjvQud_tJg.jar"" permissions:420
2019-12-18 20:44:46.998 GMT
2019/12/18 20:44:46 Found: name:""jsr305-3.0.2-3YOsy4mTY8MrB9ehsuTOQA.jar"" permissions:420
2019-12-18 20:44:47.017 GMT
2019/12/18 20:44:47 Found: name:""jackson-databind-2.9.10-_0PXnGJLD31GVUL-5mSEdA.jar"" permissions:420
2019-12-18 20:44:47.037 GMT
2019/12/18 20:44:47 Found: name:""jackson-dataformat-yaml-2.9.10-6-zFtnuWh0wIBoFR_YnQtQ.jar"" permissions:420
2019-12-18 20:44:47.236 GMT
2019/12/18 20:44:47 Found: name:""jackson-core-2.9.10-1i2bHR2D3VU-Z4vI_Oj4CQ.jar"" permissions:420
2019-12-18 20:44:47.291 GMT
2019/12/18 20:44:47 Found: name:""jackson-annotations-2.9.10-JsK297xwTMrcZMg5leD_fw.jar"" permissions:420
2019-12-18 20:44:47.306 GMT
2019/12/18 20:44:47 Found: name:""avro-1.8.2-EDleWlceGh9hE0EfJ20v6g.jar"" permissions:420
2019-12-18 20:44:47.322 GMT
2019/12/18 20:44:47 Found: name:""avro-1.8.2-tests-KC1ltAJzRHVcxtiYWfFpvg.jar"" permissions:420
2019-12-18 20:44:47.336 GMT
2019/12/18 20:44:47 Found: name:""slf4j-jdk14-1.7.25-lEPyQGtDduRgOFY4C9MlVA.jar"" permissions:420
2019-12-18 20:44:47.354 GMT
2019/12/18 20:44:47 Found: name:""metrics-core-3.1.2-uLLedSRzIqDAN0IPVwjlkg.jar"" permissions:420
2019-12-18 20:44:47.383 GMT
2019/12/18 20:44:47 Found: name:""slf4j-api-1.7.25-yq_jdq-3CG3L7nn3gDlMow.jar"" permissions:420
2019-12-18 20:44:47.412 GMT
2019/12/18 20:44:47 Found: name:""snappy-java-1.1.4-SFNwbMuGq13aaoKVzeS1Tw.jar"" permissions:420
2019-12-18 20:44:47.427 GMT
2019/12/18 20:44:47 Found: name:""joda-time-2.10.3-x9d0qCHsaxqSPYJWPWV-Kw.jar"" permissions:420
2019-12-18 20:44:47.461 GMT
2019/12/18 20:44:47 Found: name:""xz-1.8-X5ghJ-DehbeFxLKrrSGqLg.jar"" permissions:420
2019-12-18 20:44:47.477 GMT
2019/12/18 20:44:47 Found: name:""powermock-api-mockito2-2.0.2-shnnF2D5x8By8gIRGXFhZw.jar"" permissions:420
2019-12-18 20:44:47.492 GMT
2019/12/18 20:44:47 Found: name:""mockito-core-3.0.0-7tOnWjwKUa3vSvwalF3i1w.jar"" permissions:420
2019-12-18 20:44:47.513 GMT
2019/12/18 20:44:47 Found: name:""kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar"" permissions:420
2019-12-18 20:44:47.528 GMT
2019/12/18 20:44:47 Found: name:""kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar"" permissions:420
2019-12-18 20:44:47.550 GMT
2019/12/18 20:44:47 Found: name:""kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar"" permissions:420
2019-12-18 20:44:47.577 GMT
2019/12/18 20:44:47 Found: name:""kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar"" permissions:420
2019-12-18 20:44:47.592 GMT
2019/12/18 20:44:47 Found: name:""zstd-jni-1.3.8-3-bSgd5MmkqGHlwXyzvze_1w.jar"" permissions:420
2019-12-18 20:44:47.609 GMT
2019/12/18 20:44:47 Found: name:""google-auth-library-credentials-0.12.0-oX-2Z1dxImK9WyCyPRaKfg.jar"" permissions:420
2019-12-18 20:44:47.635 GMT
2019/12/18 20:44:47 Found: name:""beam-vendor-grpc-1_21_0-0.1-wUUJvysMX0DMT0c1hpzFYQ.jar"" permissions:420
2019-12-18 20:44:47.651 GMT
2019/12/18 20:44:47 Found: name:""google-cloud-dataflow-java-proto-library-all-0.5.160304-dqDYFED9AirNnNfC_1P6Ag.jar"" permissions:420
2019-12-18 20:44:47.665 GMT
2019/12/18 20:44:47 Found: name:""hamcrest-2.1-oTm8x8sMLv9-n5czpY1b3Q.jar"" permissions:420
2019-12-18 20:44:47.685 GMT
2019/12/18 20:44:47 Found: name:""error_prone_annotations-2.0.15-npnuyJrjs2dF75GdvUBIvQ.jar"" permissions:420
2019-12-18 20:44:47.702 GMT
2019/12/18 20:44:47 Found: name:""jackson-mapper-asl-1.9.13-F1D5wzk1L8S3KNYbVxcWEw.jar"" permissions:420
2019-12-18 20:44:47.742 GMT
2019/12/18 20:44:47 Found: name:""jackson-core-asl-1.9.13-MZxJpDBOP6n-PNjc_ACdNw.jar"" permissions:420
2019-12-18 20:44:47.761 GMT
2019/12/18 20:44:47 Found: name:""paranamer-2.7-VweilzYySf_-OOgYnNb5yw.jar"" permissions:420
2019-12-18 20:44:47.780 GMT
2019/12/18 20:44:47 Found: name:""commons-compress-1.19-_ol7ztQ0aEULeFtmwc_0VQ.jar"" permissions:420
2019-12-18 20:44:47.794 GMT
2019/12/18 20:44:47 Found: name:""snakeyaml-1.23-ZOyL0mttUDSofsscjODv3A.jar"" permissions:420
2019-12-18 20:44:47.812 GMT
2019/12/18 20:44:47 Found: name:""checker-qual-2.0.0-lP4a92wQAG-8W5iBgLcb8A.jar"" permissions:420
2019-12-18 20:44:47.827 GMT
2019/12/18 20:44:47 Found: name:""j2objc-annotations-1.1-Sa4yBLsLubKsdwYmQfSm1w.jar"" permissions:420
2019-12-18 20:44:47.842 GMT
2019/12/18 20:44:47 Found: name:""powermock-api-support-2.0.2-cmvt0hCp-TF3N42XWM5ZWQ.jar"" permissions:420
2019-12-18 20:44:47.943 GMT
2019/12/18 20:44:47 Found: name:""powermock-core-2.0.2-N-uIY2bG_VrLlX25F5TknA.jar"" permissions:420
2019-12-18 20:44:47.962 GMT
2019/12/18 20:44:47 Found: name:""powermock-reflect-2.0.2-WpXXbgjnSFeT4du1aQDZ2w.jar"" permissions:420
2019-12-18 20:44:47.979 GMT
2019/12/18 20:44:47 Found: name:""byte-buddy-1.9.10-QusrhKJgPcRx1AnckpJw7Q.jar"" permissions:420
2019-12-18 20:44:47.998 GMT
2019/12/18 20:44:47 Found: name:""byte-buddy-agent-1.9.10-BCkqVrRfNOFZg1RAm6NEQw.jar"" permissions:420
2019-12-18 20:44:48.013 GMT
2019/12/18 20:44:48 Found: name:""objenesis-3.0.1-J5YZBgYnQmdzV6gwCA2MQQ.jar"" permissions:420
2019-12-18 20:44:48.031 GMT
2019/12/18 20:44:48 Found: name:""reflectasm-1.07-shaded-IELCIoQPsh49E873w5ZYMQ.jar"" permissions:420
2019-12-18 20:44:48.074 GMT
2019/12/18 20:44:48 Found: name:""minlog-1.2-98-99jtn3wu_pMfLJgiFvA.jar"" permissions:420
2019-12-18 20:44:48.097 GMT
2019/12/18 20:44:48 Found: name:""javaruntype-1.3-rOez6Sa6vLqndviPQ7_noQ.jar"" permissions:420
2019-12-18 20:44:48.128 GMT
2019/12/18 20:44:48 Found: name:""ognl-3.1.12-aq6HFAi1aHiNX6qcImOsmw.jar"" permissions:420
2019-12-18 20:44:48.142 GMT
2019/12/18 20:44:48 Found: name:""generics-resolver-2.0.1-VrpA4CuIscdXm0wF6oMTaQ.jar"" permissions:420
2019-12-18 20:44:48.183 GMT
2019/12/18 20:44:48 Found: name:""netty-codec-http2-4.1.30.Final-xWX9W6lm-ZlAY32qJIyUKw.jar"" permissions:420
2019-12-18 20:44:48.198 GMT
2019/12/18 20:44:48 Found: name:""netty-handler-4.1.30.Final-zZAD5jGo-RpbSIfZ0lRurQ.jar"" permissions:420
2019-12-18 20:44:48.214 GMT
2019/12/18 20:44:48 Found: name:""netty-tcnative-boringssl-static-2.0.17.Final-YPiL4Pp2vC0bifcXJZEEQQ.jar"" permissions:420
2019-12-18 20:44:48.234 GMT
2019/12/18 20:44:48 Found: name:""proto-google-common-protos-1.17.0-ZA4dWNbtwl5-FIMyB6cNug.jar"" permissions:420
2019-12-18 20:44:48.252 GMT
2019/12/18 20:44:48 Found: name:""protobuf-java-3.6.0-NDZs0P9eJxX9OraYDJl91w.jar"" permissions:420
2019-12-18 20:44:48.310 GMT
2019/12/18 20:44:48 Found: name:""classgraph-4.8.56-OJxxWYNOeeT7dCzKVFyoag.jar"" permissions:420
2019-12-18 20:44:48.330 GMT
2019/12/18 20:44:48 Found: name:""auto-value-annotations-1.6.3-xXovAdCH5p9GU93RsQLguA.jar"" permissions:420
2019-12-18 20:44:48.353 GMT
2019/12/18 20:44:48 Found: name:""opencensus-contrib-grpc-metrics-0.17.0-ezV91dXAiuVASofLaT_ZTg.jar"" permissions:420
2019-12-18 20:44:48.364 GMT
2019/12/18 20:44:48 Found: name:""opencensus-api-0.18.0-BZQH5cDopLnYFcO8ZEinaQ.jar"" permissions:420
2019-12-18 20:44:48.385 GMT
2019/12/18 20:44:48 Found: name:""javax.annotation-api-1.3.2-KrGXPu__qiruxH1QueQLnQ.jar"" permissions:420
2019-12-18 20:44:48.401 GMT
2019/12/18 20:44:48 Found: name:""animal-sniffer-annotations-1.17-fKEIt5DParXb9UIsx58NiQ.jar"" permissions:420
2019-12-18 20:44:48.424 GMT
2019/12/18 20:44:48 Found: name:""asm-4.0-Mi2PiMURGvYS34OMAZHNfg.jar"" permissions:420
2019-12-18 20:44:48.441 GMT
2019/12/18 20:44:48 Found: name:""antlr-runtime-3.1.2-dpdEvr2kPK4dWHj9fUOv1A.jar"" permissions:420
2019-12-18 20:44:48.464 GMT
2019/12/18 20:44:48 Found: name:""javassist-3.24.0-GA-I_0i0NCJ3frFZ6Oc4MsakQ.jar"" permissions:420
2019-12-18 20:44:48.479 GMT
2019/12/18 20:44:48 Found: name:""threetenbp-1.3.3-bEXFSgaAYiXSdUtR-98IjQ.jar"" permissions:420
2019-12-18 20:44:48.510 GMT
2019/12/18 20:44:48 Found: name:""httpclient-4.5.5-l-flsTVHa30lpasx4epJIg.jar"" permissions:420
2019-12-18 20:44:48.525 GMT
2019/12/18 20:44:48 Found: name:""commons-logging-1.2-BAtLTY6siG9rSio70vMbAA.jar"" permissions:420
2019-12-18 20:44:48.570 GMT
2019/12/18 20:44:48 Found: name:""grpc-context-1.17.1-fI9fpai7msIsYMyyTxPe7A.jar"" permissions:420
2019-12-18 20:44:48.589 GMT
2019/12/18 20:44:48 Found: name:""gson-2.7-UTSiNQ9YiQ_7nbC0AEcZXQ.jar"" permissions:420
2019-12-18 20:44:48.603 GMT
2019/12/18 20:44:48 Found: name:""netty-handler-proxy-4.1.30.Final-lCIjvadnkbjk1NVm_E895w.jar"" permissions:420
2019-12-18 20:44:48.623 GMT
2019/12/18 20:44:48 Found: name:""netty-codec-http-4.1.30.Final-AwxItf4Cqo2SMwSuXhxusg.jar"" permissions:420
2019-12-18 20:44:48.673 GMT
2019/12/18 20:44:48 Found: name:""netty-codec-socks-4.1.30.Final-1WHnwq75shvN3dCiM86_2g.jar"" permissions:420
2019-12-18 20:44:48.687 GMT
2019/12/18 20:44:48 Found: name:""netty-codec-4.1.30.Final-x4vcMCRY_egm-VkASqrGgQ.jar"" permissions:420
2019-12-18 20:44:48.708 GMT
2019/12/18 20:44:48 Found: name:""netty-transport-4.1.30.Final-x3QJT81qSgvsv4fipL84Ug.jar"" permissions:420
2019-12-18 20:44:48.732 GMT
2019/12/18 20:44:48 Found: name:""netty-buffer-4.1.30.Final-mYBmhCN5yGpDEs5G7F6ejg.jar"" permissions:420
2019-12-18 20:44:48.759 GMT
2019/12/18 20:44:48 Found: name:""args4j-2.33-Cm1RX3axXSnjzVKd6TGXOQ.jar"" permissions:420
2019-12-18 20:44:48.772 GMT
2019/12/18 20:44:48 Found: name:""postgresql-42.2.2-rq7ipFbyabSdeBJdb0kvXQ.jar"" permissions:420
2019-12-18 20:44:48.790 GMT
2019/12/18 20:44:48 Found: name:""commons-lang3-3.6-XRj2i1Ei_TmMEY31OrTPVQ.jar"" permissions:420
```",lukecwik,-501,Y,lgajowy
beam10412,"Errors appear before warnings. `FnApiLogRecordHandlerTest.test_context` failed.
edit: And there are 2 pytest runs per suite (with and without xdist)
edit: And it seems that Jenkins did not collect test results (""Took 0 ms on master""), otherwise you'd see the failed tests on the summary page.",udim,-571,Y,kamilwu
beam10413,"@alexvanboxel 
1. I'm not entirely sure I understand the use case. Why do options need to be copied, while other constructs (such as schemas) don't?

2. I disagree for a couple of reasons:
   * A FieldType can be nullable, and there's no reason not to support null FieldTypes in options. By returning null for non-existent fields, you make it hard to distinguish between a null value and an invalid get call (i.e. passing an option that is not part of the schema). This feels like the sort of behavior that seems simpler, but actually makes things more complex (e.g. the fact that Java Maps return null for non-existent values is often considered a mistaken design, for this very reason. It's hard to distinguish between a missing value and an explicit null stored in a map).

   * Options are no different than schemas - there is a static list of options for each field, so I don't see why this is so different.

I also want to understand better why we can't just use Row here (revisiting Brian's question). You mentioned using dots in field names, but AFAIK protobuf also prohibits dots. It's true that dots are often used in option specifications in proto, but that's just to set individual fields of a message. In Beam, we really should model those as a single row-valued option, not as multiple individual options. I.e. (from the proto docs):

    option (my_method_option).foo = 567;
    option (my_method_option).bar = ""Some string"";

should translate to a single Beam option named ""my_method_option"" not to two separate options.

dots are also used to represent package names of course. Is this the main reason you need to support dots?",reuvenlax,82,Y,alexvanboxel
beam10413,"Some thoughts:

I know your use case for options involves mapping specific input types to schemas, however we need to make sure that the actual mechanism is more generic than that. We have many uses for field metadata (or options) inside of Beam, completely unrelated to any option concept on input data types. I would not like to make a design choice that precludes nullable FieldTypes now, because we may very well find that we need nullable field types.

That being said: I notice that you haven't removed the FieldType metadata in this PR. Is that planned for a future PR? I don't think we want both metadata and options in the protocol - Options seemed to me to be a better-typed version of metadata.

I don't personally think that the hasOption code is too bad, and it's definitely better than constraining the type system for Options in a way that we quite likely will regret. That being said, you've already added the getValueOrDefault method. So you can still write this code:

   .getFields()
        .forEach(
            f -> {
              Integer pkSequence = f.getOptions().getValueOrDefault(""vptech.data.contract.v1.primary_key"", null);
              if (pkSequence != null) {
                primaryKeyMap.put(pkSequence, dataSchema.indexOf(f.getName()));
              }
            });

A tiny bit more verbose, but much more explicit about what you are trying to do. (FYI the reason I don't suggest allowing getValue to return an Optional<T> - even though it might be the ""better"" solution - is because the rest of the code base does not use Optional, and I want things to stay consistent).

About the removeOption builder method. I'm struggling with this a bit, since it feels like a weird pattern to put remove into a builder. I'm also not sure whether copying a a Schema field with just a subset of options is a common use case or not. Generally we should start off by keeping such things out of the core Beam API until we are convinced that there are multiple users who need them. I think you can still accomplish the same thing by simply adding a new Options.Builder.setOptions override that takes in a Map.

newField = newField.withOptions(Options.builder().setOptions(
     Maps.filterKeys(
        oldField.getOptions().getAllOptions(),
        n -> !n.equals(optionToRemove))));

This code is probably a bit more annoying for you to write. If we are convinced that this is a common use case then I'm ok with adding removeOption, but if it's specific to your use case then I think we're better off adding the new setOptions method and using that.


",reuvenlax,82,Y,alexvanboxel
beam10413,"Taking another look. I'm fine with metadata being in a separate PR - it
makes it easier to review. I just wanted to ensure that metadata was still
in the plan.

On Sun, Mar 15, 2020 at 4:24 AM Alex Van Boxel <notifications@github.com>
wrote:

> I think I addresses now all the issues in the latest fixup.
>
> Talking about field metadata: yes this will be another PR. I didn't want
> this all in one PR. This is the groundwork to get a decent API in. I have
> at least 2 related PR's (proto and avro support) waiting before tackling
> removal of metadata though.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/beam/pull/10413#issuecomment-599197547>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AFAYJVLQZ6BGMQQ37KJ65QTRHS3HPANCNFSM4J4M5UTA>
> .
>
",reuvenlax,82,Y,alexvanboxel
beam10413,"@alexvanboxel I'm still not 100% sure that I agree that options are not schemas, and I don't love making SchemaVerification public. However I think it will be easier to iterate on those issues in isolation once the basic support is in, so I'm approving for now.

LGTM",reuvenlax,82,Y,alexvanboxel
beam10418,"Thanks for the contribution.

Tyson, could you create a JIRA account as per the [contribution guide](https://beam.apache.org/contribute/#share-your-intent) for sharing your intent. Then I can add you as a contributor to the project which would allow you to assign JIRAs to yourself (specifically BEAM-9014 which I created for this change). Note that all PRs should have an accompanying JIRA associated with them.",lukecwik,-1390,Y,tysonjh
beam10424,"Please don't get me the wrong way - I really think this is a good change
and I believe that in my comments I suggested improvements not only to
style but also to functionality (template vs generating code using
system.out.println). I wouldn't say those are subjective. What is more, in
my opinion, after a review round or two we could get even more value from
this pr.

The reason why I think merging this should wait for the reviewer's approval
(not necessarily mine) is that it would prevent missunderstandings like the
one we have right now. I agree this is not ""mission critical"" code but I
still think we shouldn't judge that on our own.



pt., 20 gru 2019, 14:44 użytkownik Maximilian Michels <
notifications@github.com> napisał:

> I thought I had addressed all your comments. That's why I merged. The only
> open discussion point seems to resolve about code style, which can be
> subjective. I definitely always check if any blockers are still open before
> merging. I merged fast here, though there was sufficient time for the
> reviewers.
>
> Please consider, this is not a mission-critical code path. I'm not
> changing anything related to the Beam runtime. This just generates docs
> which always became outdated before. I'm essentially doing this on the side
> and if the review discussion is only about code style, I don't see a reason
> to block this in case of an isolated class, which is solely used to
> generate a table in the documentation.
>
> While I'm also guilty of this occasionally, it is worth to take a step
> back and take a look at the value of a change, instead of insisting on
> details, which might make it better subjectively, but do not provide more
> value, or even increase the maintenance costs. I believe I've just recently
> reviewed some of your PRs, where I tried to live up to that standards.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/beam/pull/10424?email_source=notifications&email_token=AAOXWDMNZQVKZD2DBUCOW43QZTD4VA5CNFSM4J5HDT7KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHM6RRQ#issuecomment-567929030>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAOXWDOZ6JLH5ZS5V2KZ7FLQZTD4VANCNFSM4J5HDT7A>
> .
>
",lgajowy,497,Y,mxm
beam10424,"@mxm thanks for working in this, it is a good improvement.

However, regarding the merge if this PR, I would prefer more time is given to allow for comments and consideration of comments. We should wait for an approval, not just assume everything was reviewed and addressed.

There is no reason to rush this type of change.
",tweise,117,Y,mxm
flink9864,Thanks @JingsongLi for the update. I suppose the purpose of introducing these abstractions is to support writing partitions to different external systems other than Hive. Can we have a summary about what a user/developer needs to implement in order to achieve that?,lirui-apache,583,Y,JingsongLi
flink9865,"> @JingsongLi I noticed that the ""getScalarFunction"" method in ""ScalarSqlFunction"" of blink planner was replaced by ""makeFunction"" method. I think using ""getScalarFunction"" to get the initial ScalarFunction object and check its language type makes more sense here so I re-add the method. Please take a look at this PR to make sure it does not cause any side effects to blink planner :).

LGTM, it is reasonable, can you just use scala val to scalarFunction just like `TableSqlFunction`?",JingsongLi,-482,Y,WeiZhong94
flink9867,"My point is that, in my opinion, this configuration (and also [the configuration for the default delimeter](https://github.com/apache/flink/pull/2219#discussion_r70475544)) is not in demand by users.
Instead of exposing this parameter outside of Flink to operators of Flink cluster, you can still make it configurable but internally. So when metrics register themselves, they would define which delimeter to use. End users probably won't need to customize this.

While in case of Prometheus reporter, `.` is [invalid character in metric name](https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels). InfluxDB reporter just followed this convention (I don't remember exact reason why I didn't use `.`). It may worth to use `_` for all reporters (for unification, as some common ground), but I understand that this would be an incompatible change.

But thinking more generally, I find that the `(CharacterFilter, delimeter)` pair is just a concrete implementation of more generic api (something like `MetricScopeFormatter` that can become more specific (and different) to some metric reporters.

To summarise, my personal cons against this change are:
 * the configuration is not requested by users. Less configuration exposed by Flink - less things to maintain as public api and more freedom to change implementation later;
 * even if the config is exposed, actual correct values that would work in practice are limited (cannot be any character) and specific to actual metrics system;
 * for some reporters, the scope formatting can required to be more flexible (compared to current implementation based on `CharacterFilter` and `delimeter`).",1u0,1604,Y,zentol
flink987,"Looks good in general.

Any reason that the tests cannot be part of `flink-tests`? Just thinking that we may reduce our maven project sprawl a bit ;-)
",StephanEwen,2,Y,rmetzger
flink9874,"@twalthr Thanks a lot for your comments. However, I don't agree with you on some points:

> -1 for this PR.

When you post your review comments in the PR, it has already indicated that changes are required for this PR. If this is for my previous +1, it's also not necessary. Just as I said ""+1 from my side"", it only represents that there is no problems from my side. 

+1 is only for the part which I'm pretty sure, for the other part which I'm not familiar with, I already ping you. That's the reason why I also ping you to review this change as I think you are more familiar with the changes of this PR.

> 
> First of all, it adds tests that don't focus on what should actually be tested.
> 
> Second, it merges the entire table config into global job parameters. This does not fit into the big picture of FLIP-59. Where global job config should just be a single key under the table config and execution config.

FLIP-59 is still under discussion and it has not been accepted yet. Although it's a meaningful proposal from my side, it has been inactive for more than one month. Is there any plan to commit it to release 1.10? if not, we should follow the current status. Besides, before FLIP-59 is accepted and committed, any configuration added via TableConfig is global job parameters and so this change is also self-contained and meaningful from this point of view, you can see Blink planner already implemented it in this way. The details could be found [here](https://github.com/apache/flink/blob/d32af521cbe83f88cd0b822c4d752a1b5102c47c/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala#L289).

Best,
Dian",dianfu,-579,Y,WeiZhong94
flink9874,"Hi @dianfu,

sorry if my `-1` was too direct. It should neither offend you nor the contributor. It is simply an expression that the current status of the PR cannot be merged due to the issues that I mentioned.

FLIP-59 is still under discussion because it takes longer than expected to update the ConfigOption infrastructure. It is still targeted for 1.10. As you might have seen on the mailing list, we still try to reach consensus on FLIP-54 and split it into multiple parts such as FLIP-77.

The solution in this PR makes it difficult to validate the configuration in the future as it encourages users to add arbitrary keys to the table config. We should clearly separate `global job configuration` (which are arbtirary user-defined properties available to functions) and `configuration` (which are Flink-defined properties available to Flink).

For example, we could have a separate `Configuration` member variable in TableConfig for user-defined properties for clear separation. What do you think?",twalthr,-1672,Y,WeiZhong94
flink9874,"Hi @twalthr, 

Thanks a lot for your reply. Regarding adding the `global job configuration`, as it will introduce new API interfaces and it will be better to discuss it in the ML or the mentioned FLIPs. The aim of this PR is to align the legacy planner with the blink planner(maybe we should change the title of this PR to make it more clear). So how about limiting the scope of this PR to aligning the two planners and discussing the API changes in the FLIP discussion? 

Best,
Dian",dianfu,-579,Y,WeiZhong94
flink9874,"Hi @dianfu @WeiZhong94 as far as I know, @twalthr  is on vacation. So I agree that we can limit the scope  of this PR. If there are any improvement we can open new PRs before 1.10 release. 

What do you think? 

@hequn8128 is there any suggestion from your side? :)

Best,
Jincheng",sunjincheng121,-788,Y,WeiZhong94
flink9874,"Thank you all for the discussion! 

I think the concern raised by @twalthr is reasonable. However, aligning the two planners first is a good idea and thus we can unblock the coming related PRs. 

What do you think?",hequn8128,-662,Y,WeiZhong94
flink988,"Why isn't rebalance implied when the 2 operators don't have the same parallelism and partitioning is not defined? If you don't specify the partitioning (which defaults to forward) means that you don't specifically care, in this case implied rebalance is the most natural thing.

It's arguable that if the user specifically says forward then we give an error but otherwise I this this just hurts the developers.
",gyfora,8,Y,aljoscha
flink988,"If I understand correctly, this also this changes the semantics that we execute programs without sinks, and also topology branches which don't end in sinks. I personally don't like the fact that the each branch in the processing graph needs to end in a sink, it is rather artificial.
",gyfora,8,Y,aljoscha
flink988,"It would be good to get some feedback from the others as well, but in general my arguments are the following:
1. Getting exceptions after non-parallel sources just because you didn't rebalance (or in any other case), is very confusing for the user and seems unintuitive. Therefore I vote for fixing the current behaviour and keeping rebalance implied.
2. There is probably a good reason why a user implements an operator (maybe some outside world communication), so not executing it (for instance no sink attached) will lead to incorrect behaviour. Also we cannot force someone to always use a sink in this case as they might need some special behaviour implemented by some other operator.
",gyfora,8,Y,aljoscha
flink988,"+1 for rebalancing automatically between operators of different DOP. The batch API does the same. But it should really be ""rebalance"", not a form of forward that typically creates skew.

I am somewhat indifferent to the sink-or-no-sink question. The batch API requires sinks strictly, and it makes total sense there. For streaming, it may be different.

If we require sinks, we should have a simple function `sink()` or so, which marks the operation as sink, so we don't force people to implement a _discarding sink_ or so unnecessarily.
",StephanEwen,-27,Y,aljoscha
flink9881,"@kl0u thanks for opening this pull requests. The purpose of it looks good to me. My concern contains

1. For providing configuration, so far we use name pattern `XxxConfiguration` such as `RestConfiguration`/`JobMasterConfiguration` and so on. `ExecutionParameterProvider` looks diverge from them.

2. `fromConfiguration` is so far implemented as static methods of the `XxxConfiguration`. I wonder whether or not we have to introduce a `ExecutionParameterProviderBuilder` which doesn't follow a builder pattern as described in our [code style guide](https://lists.apache.org/x/thread.html/58e7ed148ff1df7acfacc038a5f07a3a74547caf6674c959ea6f91b4@%3Cdev.flink.apache.org%3E).

3. Since we decide to make `Configuration` the unique view of configurations in Flink, `fromCommandLine` looks like a common pattern. Shall we try to induce how to parse a `CommandLine` to `Configuration` so that it guides following development?",tisonkun,1027,Y,kl0u
flink9881,"Thanks for your update @kl0u !

After an investigation I actually think that the `Builder` is more like a `Factory` and here is a [patch](https://gist.github.com/TisonKun/4d574a118bb5ce00fb9ae8cd24724319) you can make use of. For a typical builder you can checkout `MiniClusterConfiguration.Builder` and so on. I'm open to learn from you opinion also :P

For comment `3`, think a bit more I would prefer defer so-call bulletproof scheme until we have more instances so that we can smoothly induce from those instances instead of imagine too much right now.


",tisonkun,1027,Y,kl0u
flink9882,"Thanks for contributing to Flink! ☕️ 

I don't think this will work, however, since the setter for case classes doesn't actually modify the case class and Flink only has a reference to the unmodified case class. When using the Scala API, the type analysis stack should correctly analyze case classes and use the `CaseClassSerializer` for them. (The `TypeExtractor` is only used for the Java API)",aljoscha,-1913,Y,ElliotVilhelm
ant13,"We've voted to keep 1.9.x as an LTS branch for Java5 compatible versions of Ant and move master to Java8 as baseline. 
",bodewig,-427,N,nhojpatrick
jmeter483,"@ham1 , do you think the PR is good to go?
",vlsi,-164 days,N,ham1
karaf1221,Tested succesfully thanks !,jbonofre,-2268 days,N,rmannibucau