{
  "Can one of the admins verify this patch?": "N",
  "The example/demo/test will be part of Wicket from now on so we have to adapt it (package names, versions, etc.).\nI guess the example will be only in 7.x but the just improvement in BaseWicketTester will be in 6.x as well.\n": "Y",
  "Merged build finished. \n": "N",
  "Thanks @NuxRo can you rebase against latest 4.9, looks like some more commits came in.\n": "N",
  "Thanks again for your work, @ropalka": "N",
  "**[Test build #47096 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/47096/consoleFull)** for PR 10112 at commit [`e89ed9b`](https://github.com/apache/spark/commit/e89ed9b20971fee280904c834b511db91b3bc633).\n- This patch **fails Scala style tests**.\n- This patch merges cleanly.\n- This patch adds no public classes.\n": "N",
  "@reta hmm, both case are interesting but my point was that normal scoped beans are required to be proxied whereas others not. That said it also depends how CDI beans are proxies so maybe saner to test with a custom fake proxy not copying annotation since I suspect several CDI impl will copy them to not break frameworks, wdyt?": "Y",
  "@woodgood, thanks for the patch. I'm afraid we cannot accept it, because it changes a released API (the lack of a \"@since\" doxygen tag in the docstring means the API was released in 1.0) and would violate our \"100% API/ABI backwards compatibility\" promise.\n\nTo pursue an alternative solution please email dev@subversion.apache.org and describe your use-case. Thanks!\n": "Y",
  "FOP-2704 in jira was closed, can you try fop trunk to see if you have a issue": "Y",
  "Actually I was the one which claims separated credits from other project. (https://github.com/OpenTSDB/asynchbase/pull/122)\n\nBut there was a strong reason to do so, and I think it's not the normal case we can see it often. As I addressed from mailing list, many big Apache projects already used this approach.\n\nIf there're cases which squashing really hurts then we can have exceptional case.\n": "Y",
  "The performance of this strategy can be significantly improved by converting the \"version number\" into a value that can more easily be compared against other values. This can be done in several ways, some building on others (i.e. smaller changes) and some with significant changes.\n\nThe most obvious performance optimization is to cache the compiled regex `Pattern` objects: you will be doing a lot of tokenizing on `/\\./`, so compile it once and use it many times.\n\nThe second most obvious performance optimization is to stop using regular expressions entirely. When searching for a single character, it's much faster to write your own tokenizer because regular expressions inherently have more overhead (because they are so flexible). The code is not as straightforward (and I'm happy to see that you provided an easy-to-follow patch, here) but in this case, speed is in fact important as Mark notes: this code will be executed for every request whose URL might match the context.\n\nThe third most obvious performance improvement would be to store the tokenized version number as an array and, in `compareTo` simply compare the individual elements of the arrays in each object. Then you don't have to tokenize during each compare.\n\nFinally, instead of re-computing the \"version\" difference every time, a representative value could be built for the version number that can be quickly compared against another value. For example, let's take the versions `1.0.9` and `1.0.10` for example. One strategy would be to normalize each part of the version number to have some kind of \"maximum number of digits.\" Let's pick 4-digits and see how to do this:\n\n1. Tokenize `1.0.9` into [1, 0, 9]\n2. Convert to string with 4-digits for each version component: `000100000009`\n3. Tokenize `1.0.10` into [1, 0, 10]\n4. Convert to string with 4-digits for each version component: `000100000010`\n5. Compare the strings alphabetically (`String.compareTo`)\n\nNow, simply more steps 1-2 into the constructor of the `ContextName` class and then `ContextName.compareTo` becomes a simple comparison of the normalized version number.\n\nOne can also do this with integral values instead of `Strings` and the comparison becomes even faster, but you run the risk of running out of digits if the padding is too wide (e.g. 4-digit padding with a version number like `1.0.9.1` yields `1000200030001` which is `>` `Integer.MAX_VALUE` so you'd have to switch to a `long` value. Comparing `long` values is faster than comparing `String` values, but at some point you run out of digits in a `long` value as well.\n\nAnyway, there is lots of room for improvement, here, and I think it *does* make sense to support \"traditional\" version numbering. Note that this will represent a backward-*in*compatible change, and those relying upon the current alphabetical-ordering will need to adjust their expectations.": "Y",
  "One comment I would make regarding this, if we start adding `isAssignableFrom` checks for every possible type then the performance is going to start to suffer. It may be that there needs to be an extensible codec mechanism added instead.": "Y",
  "This PR can't be responsible for the failing tests as the sources or test files are not touched.\n\nFailing tests are:\nsrc.tests.antunit.types.resources.test_xml.testhttpurl2\nsrc.tests.antunit.taskdefs.get-test_xml.testTemporaryRedirect\nsrc.tests.antunit.taskdefs.get-test_xml.testNestedResources\nsrc.tests.antunit.taskdefs.get-test_xml.testInfiniteRedirect\nsrc.tests.antunit.taskdefs.gunzip-test_xml.testWithNonFileResourceToDir\nsrc.tests.antunit.taskdefs.gunzip-test_xml.testWithNonFileResourceToFile\n": "Y",
  "@pmouawad I would really appreciate if you could test out [this branch](https://github.com/weisJ/darklaf/pull/72) of darklaf on your machine and tell me whether you are able to load the native library.": "Y",
  "In a parallel universe, there's https://gitlab.com/testload/jmeter-listener (MIT)": "N",
  "In`LocalJMXCommand` the PropertyEditorRegistry is held as a field. In other places, this is doing `PropertyEditorRegistry.registerDefaults()` when calling `getValue()`, which has to go an do a bunch of registration each time.\n\nWhat do we think the lifecycle of a `PropertyEditorRegistry` should be? I did a quick search for references, which throws up `ManagedMBean`, `ActiveMQ5Factory`, `JMSProducerImpl`, and I would have thought it could be tied to the lifecycle of the components using it in each case (i.e. - make it a field).\n\nThoughts?": "Y",
  "Taking another look. I'm fine with metadata being in a separate PR - it\nmakes it easier to review. I just wanted to ensure that metadata was still\nin the plan.\n\nOn Sun, Mar 15, 2020 at 4:24 AM Alex Van Boxel <notifications@github.com>\nwrote:\n\n> I think I addresses now all the issues in the latest fixup.\n>\n> Talking about field metadata: yes this will be another PR. I didn't want\n> this all in one PR. This is the groundwork to get a decent API in. I have\n> at least 2 related PR's (proto and avro support) waiting before tackling\n> removal of metadata though.\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/apache/beam/pull/10413#issuecomment-599197547>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFAYJVLQZ6BGMQQ37KJ65QTRHS3HPANCNFSM4J4M5UTA>\n> .\n>\n": "Y",
  "Is there a way to submit the raw requests in batches?\nIf performance is decent, we'll try this.\n": "N",
  "you are very right about the bean cache of course. It is a problem known for years... that and the stupid synchronization mechanism inside the Introspector. If you now can tell me the performance impact of this change, then I will soon be able to give my ok. \n": "Y",
  "Agreed, let's keep it as a debug branch.\n\nI think this is a useful, but not complete, step towards getting rid of the network buffers configuration.\nEspecially introducing the explicitly \"capacity constrained / back-pressured\" partition type is a good thing, in my opinion.": "Y",
  "> R: @aaltay\n> PTAL.\n> \n> Let's talk offline to see if we need to create release branch and tags for a release.\n\nChange LGTM. I am assuming you are planning to release this soon.\n\n@kennknowles - Do we use branches or tags for releasing vendored dependencies? The release of this sub-directory will be similar to that. (I believe we do not do anything special other than a vote on dev list.)": "Y",
  "@NuxRo there are still several commits which have come from master; can you export the commit (git format-patch -1) and reset --hard origin/4.9 and re-apply the commit (git am <patch>) and then git push -f.\n": "N",
  "@ajothomas, @aromanenko-dev can you'll have a look? \r\nChanges made got rid of the `WrappedSnsResponse` to send the exception back to the client. An exception with AWS SNS will should actually throw an exception.\r\nFor Response from the SNS client. I'm just returning the `requestElement, statusText and statusCode`\r\nto the users so they can determine how they want to handle the responses at their side.\r\nL\r\n": "Y",
  "@kl0u thanks for opening this pull requests. The purpose of it looks good to me. My concern contains\r\n\r\n1. For providing configuration, so far we use name pattern `XxxConfiguration` such as `RestConfiguration`/`JobMasterConfiguration` and so on. `ExecutionParameterProvider` looks diverge from them.\r\n\r\n2. `fromConfiguration` is so far implemented as static methods of the `XxxConfiguration`. I wonder whether or not we have to introduce a `ExecutionParameterProviderBuilder` which doesn't follow a builder pattern as described in our [code style guide](https://lists.apache.org/x/thread.html/58e7ed148ff1df7acfacc038a5f07a3a74547caf6674c959ea6f91b4@%3Cdev.flink.apache.org%3E).\r\n\r\n3. Since we decide to make `Configuration` the unique view of configurations in Flink, `fromCommandLine` looks like a common pattern. Shall we try to induce how to parse a `CommandLine` to `Configuration` so that it guides following development?": "Y",
  "I added some more comments. I could not find in that test anywhere the notion of checking that elements are not late, but properly interleaved with the watermarks.\n\nIs there a test that checks that the reader does not let LongMax watermarks pass through? Or that the split generating task does not emit a long-max watermark on exit?\n\nAlso, is there a test that tests the interplay between the split-generator and the reader?\n\nThat would be important to have, I think.\n": "Y",
  "@wido, if this is done as root, writing the log, then calling `fsync` then doing the sysrq will suffice.\n": "N",
  "I sent it to you": "N",
  "I think there is a lot of merit in Chris's idea. The concept of some sort of internal pre-computed version string that addresses ordering concerns by zero padding numerical components is one that would be easy to implement and have minimal impact on the existing system.": "Y",
  "When I compile your PR with ```ant download_jars clean install```, I get the following:\n```\ncompile-mongodb:\n[javac] Compiling 9 source files to /xxx/jmeter/build/protocol/mongodb\n[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/config/MongoSourceElement.java:117: error: cannot find symbol\n[javac] .autoConnectRetry(getAutoConnectRetry())\n[javac] ^\n[javac] symbol: method autoConnectRetry(boolean)\n[javac] location: class Builder\n[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/config/MongoSourceElement.java:130: error: no suitable constructor found for WriteConcern(int,int,boolean,boolean,boolean)\n[javac] builder.writeConcern(new WriteConcern(\n[javac] ^\n[javac] constructor WriteConcern.WriteConcern() is not applicable\n[javac] (actual and formal argument lists differ in length)\n[javac] constructor WriteConcern.WriteConcern(int) is not applicable\n[javac] (actual and formal argument lists differ in length)\n[javac] constructor WriteConcern.WriteConcern(String) is not applicable\n[javac] (actual and formal argument lists differ in length)\n[javac] constructor WriteConcern.WriteConcern(int,int) is not applicable\n[javac] (actual and formal argument lists differ in length)\n[javac] constructor WriteConcern.WriteConcern(boolean) is not applicable\n[javac] (actual and formal argument lists differ in length)\n[javac] constructor WriteConcern.WriteConcern(int,int,boolean) is not applicable\n[javac] (actual and formal argument lists differ in length)\n[javac] constructor WriteConcern.WriteConcern(int,int,boolean,boolean) is not applicable\n[javac] (actual and formal argument lists differ in length)\n[javac] constructor WriteConcern.WriteConcern(String,int,boolean,boolean) is not applicable\n[javac] (actual and formal argument lists differ in length)\n[javac] constructor WriteConcern.WriteConcern(Object,Integer,Boolean,Boolean) is not applicable\n[javac] (actual and formal argument lists differ in length)\n[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/mongo/MongoDB.java:53: error: cannot find symbol\n[javac] boolean authenticated = db.isAuthenticated();\n[javac] ^\n[javac] symbol: method isAuthenticated()\n[javac] location: variable db of type DB\n[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/mongo/MongoDB.java:57: error: cannot find symbol\n[javac] authenticated = db.authenticate(username, password.toCharArray());\n[javac] ^\n[javac] symbol: method authenticate(String,char[])\n[javac] location: variable db of type DB\n[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/sampler/MongoScriptRunner.java:55: error: cannot find symbol\n[javac] db.requestStart();\n[javac] ^\n[javac] symbol: method requestStart()\n[javac] location: variable db of type DB\n[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/sampler/MongoScriptRunner.java:57: error: cannot find symbol\n[javac] db.requestEnsureConnection();\n[javac] ^\n[javac] symbol: method requestEnsureConnection()\n[javac] location: variable db of type DB\n[javac] /xxx/jmeter/src/protocol/mongodb/org/apache/jmeter/protocol/mongodb/sampler/MongoScriptRunner.java:66: error: cannot find symbol\n[javac] db.requestDone();\n[javac] ^\n[javac] symbol: method requestDone()\n[javac] location: variable db of type DB\n[javac] Note: Some input files use or override a deprecated API.\n[javac] Note: Recompile with -Xlint:deprecation for details.\n[javac] 7 errors\n```\nThat means, that the current code doesn't work with the new jars.\n\nIf you want to update the code the new jars, go ahead, but as of JMeter 3.0 the sampler has been deprecated and it should have been removed with version 3.1 (which obviously didn't happen).\n\nIt would be interesting to see your JSR223 sampler. I suspect it handles all of the connection setup and client usage itself. Maybe it would be a good idea to discuss your use case and solution on the mailing list.": "Y",
  ">I think it's better to have things in CORE for what seems to be a \"core\" feature, making live metrics available.\n\nOne of the options is to bundle the plugin into the final distribution (like we bundle lots of third-party dependencies).\n\nI have not tried that myself, however, their [wiki says](https://gitlab.com/testload/jmeter-listener/-/wikis/1.-Main) there are reasons to group data before sending:\n\n> Activation of GROUP_BY function (only couple parameters in settings) could be very useful on highload (thousands of transaction per second) because on such intensity InfluxDB/ElasticSearch/Clickhouse can be bottleneck (IOPS). In this mode all Samplers grouping by name and only aggregates": "Y",
  "I would still duplicate a WordCount copy into the Spark runner like I did in https://github.com/apache/incubator-beam/pull/539 because it's widely used in the runner's unit tests.\nMaybe this could be removed after the runner is mature enough to rely only on the RunnableOnService tests.\nAnd like I also said in https://github.com/apache/incubator-beam/pull/539, transitive dependency is your enemy here, I can't come up with something better than adding Spark provided/runtime dependencies.\n\nThis could be resolved by removing the provided scope on spark dependencies from the Spark runner, but I don't think that's a good idea. Looping in @jbonofre WDYT ? this could make the Spark runner Jar become very heavy.. and what about different Spark distributions on clusters ?\n": "Y",
  "Similar to #44 we cannot change a public API and thus will not merge this request.\r\n\r\nIn this particular case we even once changed it and had to revert the field to be non-final as we broike the Eclipse integration https://github.com/apache/ant/commit/984a03d1ceb6e4b5d194e4d639d0b0fca46d92be": "Y",
  "Since you're upgrading scalastyle, shouldn't you also modify pom.xml?\n": "N",
  "Thanks koushik. I tend to agree with you on this one. I will add it to my merge list. Thanks...": "N",
  "@NuxRo if you can change the base-branch of the PR to 4.9, I can initiate some tests on VR/trillian.\n": "N",
  "Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/16095/\n": "N",
  "@vlsi , in this case +1 for switching": "N",
  "I'm okay with automating the merge process, just not the way it is implemented here. Perhaps we shouldmove the discussion to the dev list.\n": "N",
  "LGTM, we need to also test for users who connect to VPN on that network (will DNS work for them as well?). @NuxRo can you edit the PR and change the branch to 4.9, we should have this in 4.9 branch as well.\n": "N",
  "applied, build in progress on https://ci.apache.org/builders/tomee-trunk-ubuntu/builds/778": "N",
  "+1. @arunmahadevan this should go into 1.x-branch as well.\n": "N",
  ":broken_heart: **-1 overall**\n\n\n\n\n\n\n| Vote | Subsystem | Runtime | Comment |\n|:----:|----------:|--------:|:--------|\n| 0 | reexec | 20 | Docker mode activated. |\n||| _ Prechecks _ |\n| +1 | @author | 0 | The patch does not contain any @author tags. |\n| -1 | test4tests | 0 | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. |\n||| _ trunk Compile Tests _ |\n| +1 | mvninstall | 1098 | trunk passed |\n| +1 | compile | 123 | trunk passed |\n| +1 | mvnsite | 20 | trunk passed |\n| +1 | shadedclient | 1923 | branch has no errors when building and testing our client artifacts. |\n||| _ Patch Compile Tests _ |\n| +1 | mvninstall | 14 | the patch passed |\n| +1 | compile | 100 | the patch passed |\n| -1 | cc | 100 | hadoop-hdfs-project_hadoop-hdfs-native-client generated 8 new + 2 unchanged - 0 fixed = 10 total (was 2) |\n| +1 | javac | 100 | the patch passed |\n| +1 | mvnsite | 16 | the patch passed |\n| -1 | whitespace | 0 | The patch 1 line(s) with tabs. |\n| +1 | shadedclient | 784 | patch has no errors when building and testing our client artifacts. |\n||| _ Other Tests _ |\n| +1 | unit | 358 | hadoop-hdfs-native-client in the patch passed. |\n| +1 | asflicense | 25 | The patch does not generate ASF License warnings. |\n| | | 3370 | |\n\n\n| Subsystem | Report/Notes |\n|----------:|:-------------|\n| Docker | Client=17.05.0-ce Server=17.05.0-ce base: https://builds.apache.org/job/hadoop-multibranch/job/PR-595/1/artifact/out/Dockerfile |\n| GITHUB PR | https://github.com/apache/hadoop/pull/595 |\n| JIRA Issue | HDFS-14304 |\n| Optional Tests | dupname asflicense compile cc mvnsite javac unit |\n| uname | Linux 2d673d55175a 4.4.0-139-generic #165~14.04.1-Ubuntu SMP Wed Oct 31 10:55:11 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | personality/hadoop.sh |\n| git revision | trunk / c0427c8 |\n| maven | version: Apache Maven 3.3.9 |\n| Default Java | 1.8.0_191 |\n| cc | https://builds.apache.org/job/hadoop-multibranch/job/PR-595/1/artifact/out/diff-compile-cc-hadoop-hdfs-project_hadoop-hdfs-native-client.txt |\n| whitespace | https://builds.apache.org/job/hadoop-multibranch/job/PR-595/1/artifact/out/whitespace-tabs.txt |\n| Test Results | https://builds.apache.org/job/hadoop-multibranch/job/PR-595/1/testReport/ |\n| Max. process+thread count | 340 (vs. ulimit of 5500) |\n| modules | C: hadoop-hdfs-project/hadoop-hdfs-native-client U: hadoop-hdfs-project/hadoop-hdfs-native-client |\n| Console output | https://builds.apache.org/job/hadoop-multibranch/job/PR-595/1/console |\n| Powered by | Apache Yetus 0.9.0 http://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n": "N",
  "did a mvn build on the pr and manually verified the api call. the private key isn't logged anymore.\n\nbefore\n\n```\n2015-11-10 17:13:18,462 DEBUG [o.a.c.f.j.i.AsyncJobManagerImpl] (API-Job-Executor-3:ctx-74dad1b6 job-799) Executing AsyncJobVO {id:799, userId: 2, accountId: 2, instanceType: None, instanceId: null, cmd: org.apache.cloudstack.api.command.admin.resource.UploadCustomCertificateCmd, cmdInfo: {\"sessionkey\":\"UZeTJmhKlSkEyt9O1H0QceBb3yI\",\"cmdEventType\":\"UPLOAD.CUSTOM.CERTIFICATE\",\"ctxUserId\":\"2\",\"signatureversion\":\"3\",\"httpmethod\":\"GET\",\"response\":\"json\",\"domainsuffix\":\"abc.com\",\"ctxDetails\":\"{}\",\"certificate\":\"123\",\"expires\":\"2015-11-10T12:04:08+0000\",\"ctxAccountId\":\"2\",\"ctxStartEventId\":\"2089\",\"privatekey\":\"private123\"}, cmdVersion: 0, status: IN_PROGRESS, processStatus: 0, resultCode: 0, result: null, initMsid: 233845178473200, completeMsid: null, lastUpdated: null, lastPolled: null, created: null}\n```\n\nafter\n\n```\n2015-11-10 17:22:59,986 DEBUG [o.a.c.f.j.i.AsyncJobManagerImpl] (API-Job-Executor-1:ctx-3c64d80b job-800) Executing AsyncJobVO {id:800, userId: 2, accountId: 2, instanceType: None, instanceId: null, cmd: org.apache.cloudstack.api.command.admin.resource.UploadCustomCertificateCmd, cmdInfo: {\"sessionkey\":\"9kO_Di2wrf2ekMKCYrveEjCtTg8\",\"cmdEventType\":\"UPLOAD.CUSTOM.CERTIFICATE\",\"ctxUserId\":\"2\",\"signatureversion\":\"3\",\"httpmethod\":\"GET\",\"response\":\"json\",\"domainsuffix\":\"abc.com\",\"ctxDetails\":\"{}\",\"certificate\":\"123\",\"expires\":\"2015-11-10T12:13:49+0000\",\"ctxAccountId\":\"2\",\"ctxStartEventId\":\"2091\",}, cmdVersion: 0, status: IN_PROGRESS, processStatus: 0, resultCode: 0, result: null, initMsid: 233845178473200, completeMsid: null, lastUpdated: null, lastPolled: null, created: null}\n```\n\nLGTM :+1: \n": "N",
  "R: cschneider": "N",
  "This is fine as-is, should have made clearer that I'm just missing a comment in the translator. I'd also favor a unified style to initialize pipeline options, instead of the two different ways we have now. We can tackle this in a follow-up. Merging.": "Y",
  "Re: Map1Iterator -- yes is it similar (and older). `Iter` works on any iterators, not just `ExtendedIterator`.\n\nRe: naming of search - your choice. It is alwayss difficult to choose between compatibility and creating legacy.\n": "Y",
  "Errors appear before warnings. `FnApiLogRecordHandlerTest.test_context` failed.\r\nedit: And there are 2 pytest runs per suite (with and without xdist)\r\nedit: And it seems that Jenkins did not collect test results (\"Took 0 ms on master\"), otherwise you'd see the failed tests on the summary page.": "Y",
  "JVM died (again!):\n\n```\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.324 sec - in org.apache.cloudstack.privategw.AclOnPrivateGwTest\nRunning org.apache.cloudstack.networkoffering.CreateNetworkOfferingTest\n#\n# A fatal error has been detected by the Java Runtime Environment:\n#\nSUREFIRE-859: # SIGSEGV (0xb) at pc=0xada6f654, pid=21145, tid=4136938304\n#\n# JRE version: 7.0_25-b15\n# Java VM: Java HotSpot(TM) Server VM (23.25-b01 mixed mode linux-x86 )\n# Problematic frame:\n# C [libnet.so+0x14654] _fini+0x1d0c\n#\nSUREFIRE-859: # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\n#\n# An error report file with more information is saved as:\n# /home/jenkins/jenkins-slave/workspace/cloudstack-pull-analysis/server/hs_err_pid21145.log\n#\nSUREFIRE-859: # If you would like to submit a bug report, please visit:\n# http://bugreport.sun.com/bugreport/crash.jsp\n# The crash happened outside the Java Virtual Machine in native code.\n# See problematic frame for where to report the bug.\n#\nAborted\n```\n": "N",
  "Changing /bin/sh to /bin/bash means that we can break support one some system (for instance Solaris by default). Is it really required ?\n": "N",
  "LGTM, \n\n\n+1. merging. (Attempting to do it from IntelliJ, FWIW)\n\nThat NoVersionAttributeException means that the code to detect changes in a file while open didn't get an etag in the GET response. Ozone needs to fix that. In the meantime, you can set a different change detection policy for your buckets, so disable that logic": "Y",
  "ClassInfo#remove is fine, adding it to InvokerHelper#removeClass is probably fine too. Doing this to Eval is probably working out, since you rarely will create a class within the eval and then using meta programming on it. For the other three this is different, but adding a flushCaches we could have. Of course if we use Closeable, we could use close instead. Adding this to GroovyShell and GroovyScriptEngine would be actually a good idea I think. GroovyClassLoader is, because it is extending URLClassloader, already closeable... so it could go there even without adding a new interface.\n": "Y",
  "LGTM\n\ntag:easypr": "N",
  "Please don't get me the wrong way - I really think this is a good change\nand I believe that in my comments I suggested improvements not only to\nstyle but also to functionality (template vs generating code using\nsystem.out.println). I wouldn't say those are subjective. What is more, in\nmy opinion, after a review round or two we could get even more value from\nthis pr.\n\nThe reason why I think merging this should wait for the reviewer's approval\n(not necessarily mine) is that it would prevent missunderstandings like the\none we have right now. I agree this is not \"mission critical\" code but I\nstill think we shouldn't judge that on our own.\n\n\n\npt., 20 gru 2019, 14:44 użytkownik Maximilian Michels <\nnotifications@github.com> napisał:\n\n> I thought I had addressed all your comments. That's why I merged. The only\n> open discussion point seems to resolve about code style, which can be\n> subjective. I definitely always check if any blockers are still open before\n> merging. I merged fast here, though there was sufficient time for the\n> reviewers.\n>\n> Please consider, this is not a mission-critical code path. I'm not\n> changing anything related to the Beam runtime. This just generates docs\n> which always became outdated before. I'm essentially doing this on the side\n> and if the review discussion is only about code style, I don't see a reason\n> to block this in case of an isolated class, which is solely used to\n> generate a table in the documentation.\n>\n> While I'm also guilty of this occasionally, it is worth to take a step\n> back and take a look at the value of a change, instead of insisting on\n> details, which might make it better subjectively, but do not provide more\n> value, or even increase the maintenance costs. I believe I've just recently\n> reviewed some of your PRs, where I tried to live up to that standards.\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/apache/beam/pull/10424?email_source=notifications&email_token=AAOXWDMNZQVKZD2DBUCOW43QZTD4VA5CNFSM4J5HDT7KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHM6RRQ#issuecomment-567929030>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAOXWDOZ6JLH5ZS5V2KZ7FLQZTD4VANCNFSM4J5HDT7A>\n> .\n>\n": "Y",
  "I agree @wangchao316 that tuning poolsize is crucial for performance optimization.\nIt is just my opinion that the tuning should be done on the cluster size, in hbase-site.xml, rather than on the client side.": "Y",
  "@mpereira Oh I know what's going on:\n\n> Please make sure Java proto files are generated in ../build/src/java/generated folder.\n\nThe site generator expects the build to have been run, at least to the point of proto files being generated.\n\nTry\n\n```\nmkdir build\ncd build\ncmake -DENABLE_JAVA=ON ..\ncmake --build . --target mesos-protobufs\n```\n\nAnd then try generating the site again, I think it'll work.": "Y",
  "**[Test build #47652 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/47652/consoleFull)** for PR 10293 at commit [`e35d22d`](https://github.com/apache/spark/commit/e35d22db58081261292bb4be9271aa41e9f04ced).\n- This patch passes all tests.\n- This patch merges cleanly.\n- This patch adds no public classes.\n": "N",
  "I will merge as soon as Travis gives green.": "N",
  "> Maybe @amiara514 would have a comment as I understood he is using jena-text via Java code? \n\n@osma : Not exactly, I execute Sparql queries via java code which involve jena-text. I don't manipulate it at this level.\n": "N",
  "Isn't there a distribution verification that does tests? I know it's post-build, but that's better than nothing.": "N",
  "@jgallimore you opened a new PR. Can we close this one?": "N",
  "Just a quick comment (I didn't review all code): Why does this touch the AlignedWindowOperator tests? I would like to keep this commit as small as possible because we're dealing with sensitive stuff where I'd like to clearly separate things.\n\nIn `OneInputStreamOperatorTestHarness` and `KeyedOneInputStreamOperatorTestHarness`, restricting the time provider parameter to a `TestTimeServiceProvider` does not change anything, right? So I think we can leave it as is. Also in `OneInputStreamOperatorTestHarness` the additional `TimeCharacteristic` parameter is only useful for one specific test so I think it would be better to instead expose the `StreamConfig` and set the parameter there for the one test to keep the number of constructors manageable. \n": "Y",
  "@jeanouii Ok - i will submit a PR soon": "N",
  "@danielsoro every time you need to use the method\n` new PropertyEditorRegistry().registerDefaults().`\nWhy not make this method static?\n`PropertyEditorRegistry.registerDefaults();`": "N",
  "Thanks for your update @kl0u !\r\n\r\nAfter an investigation I actually think that the `Builder` is more like a `Factory` and here is a [patch](https://gist.github.com/TisonKun/4d574a118bb5ce00fb9ae8cd24724319) you can make use of. For a typical builder you can checkout `MiniClusterConfiguration.Builder` and so on. I'm open to learn from you opinion also :P\r\n\r\nFor comment `3`, think a bit more I would prefer defer so-call bulletproof scheme until we have more instances so that we can smoothly induce from those instances instead of imagine too much right now.\r\n\r\n\r\n": "Y",
  "@gemmellr yes, I closed out all open pull requests.\n\n@ebourg thank you for this patch and your work on this issue. Currently Apache Qpid can not accept pull requests as this is just a mirror. If you could please submit a patch back to the jira project we would appreciate it.\n": "Y",
  "All in all some minor change requests, otherwise this seems good.\n": "N",
  "@regfaker I'm sorry but the PR below was submitted 1 day before yours and addresses the same (+ the maxRetryTimeout deprecation). \r\nhttps://github.com/apache/beam/pull/10010\r\nI need to close one of the PRs and the only valid criteria is the first submitted PR.\r\nI know it can be frustrating, and I'm sorry.\r\nThanks anyway for your work and I hope you would not be reluctant to submitting other PRs to Beam in the future, the Beam community would love to see your future work.": "Y",
  "The basic interface looks fine as far as I'm able to pick it out amongst the swathe of related changes": "N",
  "Maybe revive old idea of extracting MongoDB components into external plugin? Custom plugin detaches version from core JMeter, which means some users can use older version of plugin, while rest of JMeter can be upgraded to newer.": "Y",
  "@aromanenko-dev - No issues, we can keep both the sync and the async for time-being. Are you ok with me just passing something from the client for using the async in that case so that the sync code need not be changed?": "Y",
  "Optimization applied to '2112_Spring' branch, which is designated to be merged into the final 2.0.0 release.": "N",
  "Thanks for your contribution!\nCould you please submit a JIRA ticket to track the improvement?\n": "N",
  "@Akshay-Iyangar Thank you for taking care of this! Could you exclude a commit that merges master into feature branch and use rebase instead? \r\n\r\nAlso, please squash all **your** commits but keep **Ajo Thomas** commit as well (to respect all credits). \r\n\r\nAnd finally, please, format all commits with the following pattern - \"[BEAM-8542] Commit message\" (see [Contribution Guide](https://beam.apache.org/contribute/#make-your-change))": "Y",
  "-1\nI am generally opposed to this. Most PRs only have a small number of commits and aren't a problem. For PRs with a large number of commits, it's simple enough to ask the contributor to squash their own commits. \n\nI'm not sure I see the benefit in adding another script, which we will have to maintain, in order to do something we should rarely be doing. \n\nAlso, I worry that having this script will lead to a sharp rise in totally-squashed PR merges, even when there's not really any benefit (and in fact, loss of authorship info) since some people are likely just going to use the script whenever they're doing a merge.\n": "Y",
  "Packaging result: ✔centos6 ✔centos7 ✔debian repo: http://packages.shapeblue.com/cloudstack/pr/1653\nJob ID-94\n": "N",
  "`Iter` is also a good choice because it provides access to transform-behavior as well as many other forms of iterator-processing. `Map1Iterator` is more of an \"implementation\" class. As the code evolves, I would expect `Iter` to last longer and be better supported.\n": "Y",
  "**[Test build #47099 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/47099/consoleFull)** for PR 10112 at commit [`40ae332`](https://github.com/apache/spark/commit/40ae332bc3057a15f9265a5f0cbd9fcf47551a09).\n- This patch passes all tests.\n- This patch merges cleanly.\n- This patch adds no public classes.\n": "N",
  "Yes, it is, but we should log somewhere. Now a machine just reboots. We should send something to syslog prior to rebooting.\n": "N",
  "I'm not a big fan of ifs either but the aim was to have a single production code (for maintenance) in a single module (for users) . And as there were only some located differences (mainly split), a simple if could do. \r\nAlso please note 2 things: discrete versions are used instead of > x in the code to avoid running on non-tested versions. Also we used a very low level ES client (rest client) which is (was?) the only one compatible with all the versions of ES, specially to have a single production code base of ESIO.\r\n\r\nIf you want, you can submit a refactor PR to improve it. I'll be happy of it. \r\n\r\n": "Y",
  "Cool :wink: ($0 could be also replaced by ${0}).\n\nI hope that the other ones like this too.\nI do not have a strong feeling about that (`...` and $(...); $VAR and ${VAR}), too.\nBut I think it is good to be consistent.\n": "Y",
  "VOTE +1": "N",
  ":confetti_ball: **+1 overall**\n\n\n\n\n\n\n\n\n\n\n\n\n\n| Vote | Subsystem | Runtime | Comment |\n\n|:----:|----------:|--------:|:--------|\n\n| +0 :ok: | reexec | 1m 30s | Docker mode activated. |\n\n||| _ Prechecks _ |\n\n| +1 :green_heart: | dupname | 0m 0s | No case conflicting files found. |\n\n| +1 :green_heart: | hbaseanti | 0m 0s | Patch does not have any anti-patterns. |\n\n| +1 :green_heart: | @author | 0m 0s | The patch does not contain any @author tags. |\n\n||| _ master Compile Tests _ |\n\n| +1 :green_heart: | mvninstall | 4m 9s | master passed |\n\n| +1 :green_heart: | checkstyle | 1m 17s | master passed |\n\n| +1 :green_heart: | spotbugs | 2m 9s | master passed |\n\n||| _ Patch Compile Tests _ |\n\n| +1 :green_heart: | mvninstall | 3m 46s | the patch passed |\n\n| +1 :green_heart: | checkstyle | 1m 12s | the patch passed |\n\n| +1 :green_heart: | whitespace | 0m 0s | The patch has no whitespace issues. |\n\n| +1 :green_heart: | hadoopcheck | 12m 23s | Patch does not cause any errors with Hadoop 3.1.2 3.2.1. |\n\n| +1 :green_heart: | spotbugs | 2m 16s | the patch passed |\n\n||| _ Other Tests _ |\n\n| +1 :green_heart: | asflicense | 0m 13s | The patch does not generate ASF License warnings. |\n\n| | | 36m 20s | |\n\n\n\n\n\n| Subsystem | Report/Notes |\n\n|----------:|:-------------|\n\n| Docker | Client=19.03.8 Server=19.03.8 base: https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-1711/1/artifact/yetus-general-check/output/Dockerfile |\n\n| GITHUB PR | https://github.com/apache/hbase/pull/1711 |\n\n| Optional Tests | dupname asflicense spotbugs hadoopcheck hbaseanti checkstyle |\n\n| uname | Linux 42bd3610f948 4.15.0-74-generic #84-Ubuntu SMP Thu Dec 19 08:06:28 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |\n\n| Build tool | maven |\n\n| Personality | dev-support/hbase-personality.sh |\n\n| git revision | master / 5e32e08782 |\n\n| Max. process+thread count | 84 (vs. ulimit of 12500) |\n\n| modules | C: hbase-server U: hbase-server |\n\n| Console output | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-1711/1/console |\n\n| versions | git=2.17.1 maven=(cecedd343002696d0abb50b32b541b8a6ba2883f) spotbugs=3.1.12 |\n\n| Powered by | Apache Yetus 0.11.1 https://yetus.apache.org |\n\n\n\n\n\nThis message was automatically generated.": "N",
  "+1 for rebalancing automatically between operators of different DOP. The batch API does the same. But it should really be \"rebalance\", not a form of forward that typically creates skew.\n\nI am somewhat indifferent to the sink-or-no-sink question. The batch API requires sinks strictly, and it makes total sense there. For streaming, it may be different.\n\nIf we require sinks, we should have a simple function `sink()` or so, which marks the operation as sink, so we don't force people to implement a _discarding sink_ or so unnecessarily.\n": "Y",
  ">Is this a big deal ?\n\nI think it is.\nFrom my perspective, the use case is: \"user wants to send the data to InfluxDB\".\nThe way to send the data looks more like a configuration option rather than selecting a different listener.\n\nFor instance, what if we add `BackendListener that sends percentiles` and `BackendListener that sends errors`?\nIt does look not user-friendly, especially, taking into account that JMeter does not allow copy-pasting multiple fields.\n\n>IMO this one complements the other one and will be more precise\n\nHaving 3-4 \"similar in-core listeners that send data to InfluxDB\" would be confusing.": "Y",
  "I'll also point out that the \"if other Apache projects do it, it is oaky\" stance is particularly dangerous. PMC members must understand ASF policy and not rely on what other projects do. If what another project does is wrong, then doing the same thing in our project just introduces liability.\n": "Y",
  "**[Test build #47146 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/47146/consoleFull)** for PR 10112 at commit [`54c6887`](https://github.com/apache/spark/commit/54c6887a6c4cae459ab48a87ee29ae377d1c3900).\n- This patch passes all tests.\n- This patch merges cleanly.\n- This patch adds no public classes.\n": "N",
  "something looks strange - why do you have 27 commits on this PR? shouldn't it just be these: 990a76c, 15c06f5, 15c06f5, 5322ee3? \n": "N",
  "OK, either way. I was suggesting that it's fine to make this example consistent with the others, even if test/train isn't the point of this example. if you do proceed, read the contributing wiki above to see how to make a JIRA, and then investigate whether there are other similar examples to change like the one I mentioned.": "Y",
  "Tests pass on the release branch. https://gradle.com/s/5jl76y2tkiwmc\r\n\r\nSo something about this change is causing the error deterministically, as you say. Since it is healthy on `master`, perhaps there are other coupled commits that need to be cherrypicked.": "Y",
  "I'm going to merge this first since the test is most likely a different problem. \n": "N",
  "> Also at same time not sure if it hides bad JSON setting or other json related behind. Should we fail totally in this case?\n\n\n\nWe would still get an error in that case [from here](https://github.com/apache/tinkerpop/blob/master/gremlin-dotnet/src/Gremlin.Net/Driver/Connection.cs#L136) where the `receivedMsg` is also accessed without a null check. If `receivedMsg` is `null` then `TryParseResponseMessage` will throw a `NullReferenceException` which will be caught by the try/catch in `Parse` and we notify the response handler about the exception.\n\nI think that we should however ensure that we're not getting an exception in a `catch` clause.": "N",
  "i was trying to think how we could do it but didn't really come up with anything. we really don't have any tests for `Console` right now that I know of - not sure how we would reasonably do that with JUnit.": "N",
  "@analytically we're trying to finish reviewing PRs for the upcoming release of 3.2.1 and 3.1.3 - did you happen to see my note above?\n": "N",
  "@analytically did you take note of the discussion on the dev mailing list? would you mind going the extra step to update other tests that might be a problem (you're likely to run into them later as you continue with your implementation anyway)?\n\nAlso, this is a sufficient enough change imo to warrant an entry to the upgrade documentation. A note in this section:\n\nhttps://github.com/apache/incubator-tinkerpop/blob/tp31/docs/src/upgrade/release-3.1.x-incubating.asciidoc#graph-database-providers\n\nthat explains that graph providers should no longer rely on the test suite to validate that hypens work for property keys would be a worthwhile comment. Please also add an entry to the CHANGELOG \n\nhttps://github.com/apache/incubator-tinkerpop/blob/tp31/CHANGELOG.asciidoc#tinkerpop-313-not-officially-released-yet\n\nwhich would make this PR fully complete. Sorry for the extra effort on this, but it's a good change and should be done \"right\".\n": "Y",
  "I have tried 3.5.0-SNAPSHOT and miss a lot of jakarta.* packages. When do you plan to move to jakarta namespaces?\nThe change will not be backward compatible. What is the use of making it backward compatible?\nI would create 4.0.0-SNAPSHOT and start jakarta namespace from there and keep 3.x.x javax namespace.\n\nI am getting java.lang.NoClassDefFoundError: javax.annotation.Resource when using 3.5.0-SNAPSHOT.": "Y",
  "Thanks - merged.\n\n\n\nWe can see what happens when run in the Apache Software Foundation account.": "N",
  "This isn't quite how we report issues or suggest changes. Please read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark first.\n\nThis is just an example of getting metrics, so the test/train split isn't as important. Still it is showing evaluating metrics and another example in isotonic_regression_example.py does do a test/train split. You might follow its example. Look for other similar examples that should be standardized in this way.": "Y",
  "Many thanks, merged.\r\n\r\nI'd like to add your name to the contributors list, the `contributors,xml` file wants a first and a last name and I'm not completely sure what to put where. :-)": "Y",
  "Thanks for opening this, I’ll take a closer look on Monday. Unfortunately there isn’t a great way to automate this process. The migration was done via a custom tool I wrote and there were clearly edge cases I missed.": "N",
  "<!--\nMeta data\n{\n\"version\" : 1,\n\"metaDataEntries\" : [ {\n\"hash\" : \"195d03891a9c5cef972527218e8f1be6693ff695\",\n\"status\" : \"DELETED\",\n\"url\" : \"https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=17738\",\n\"triggerID\" : \"195d03891a9c5cef972527218e8f1be6693ff695\",\n\"triggerType\" : \"PUSH\"\n}, {\n\"hash\" : \"7b43e89b82548db8d911c7ef04a56846d153dd15\",\n\"status\" : \"FAILURE\",\n\"url\" : \"https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=17843\",\n\"triggerID\" : \"7b43e89b82548db8d911c7ef04a56846d153dd15\",\n\"triggerType\" : \"PUSH\"\n} ]\n}-->\n## CI report:\n\n* 7b43e89b82548db8d911c7ef04a56846d153dd15 Azure: [FAILURE](https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=17843) \n\n<details>\n<summary>Bot commands</summary>\nThe @flinkbot bot supports the following commands:\n\n- `@flinkbot run travis` re-run the last Travis build\n- `@flinkbot run azure` re-run the last Azure build\n</details>": "N",
  "1. What's the difference between Spark script vs Kafka script?\nSpark script is origin of Kafka and Zeppelin, so unless there're specific improvements, I think picking Spark script is more promising. For example, `trunk` is often not used for git project but Kafka is using that.\n2. We're using branches which doesn't fully represent current version for branch. So our script should determine version more smart. One way to determine is looking at pom.xml.\n": "Y",
  "Hi Emmanuel,\n\nThanks for this. If you could make a couple of tiny changes I'll get this committed:\n- The renamed geronimo-servlet_3.0_spec-1.0.xml spec needs updated with the new dependency info, it still contains the details for the older version (we are working on an actual Maven build, which will allow removing this hideous generation process...).\n- Can you add the JIRA reference and attribute yourself in the commit message. As far as I understand it the Pull Request wont be closed if we change the message (which is why the others are still open, I'll need to request that infra close them as we cant). For example:\n\nQPID-5527: Upgrade to Jetty 8\n\nPull Request from Emmanuel Bourg.\n": "Y",
  "@rxin we are probably using scala's but this code is unwrapping values that come from hive.\n": "N",
  "Maybe we should split this into smaller chunks. I think we all agree about `ClassInfo#remove` and adding it to `InvokerHelper#removeClass`, so that could be one PR.\nUsing `flushCaches/close` in `GroovyShell`, `GroovyScriptEngine`, and `GroovyClassLoader` seems reasonable to me too. The latter would just do a `classCache.values().forEach(InvokerHelper::removeClass)`, right?\n`GroovyShell` and `GroovyScriptEngine` would probably just have to `close()` their `GroovyClassLoader` from their `close()` methods then, but since they are both extensible public API, adding the `Closeable` interface would probably have to wait for 3.x.\n": "Y",
  "Apart from that the PR looks good.": "N",
  "Done: https://github.com/apache/beam/releases/tag/jupyterlab-sidepanel-v1.0.0\n\nIt worked using a personal access token instead of an ssh key (https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/creating-a-personal-access-token#using-a-token-on-the-command-line). Not sure why.": "Y",
  "Looks a lot like we're skipping the current implementation and replacing it. Any reason we're not fixing this over here: https://github.com/apache/geronimo-openapi?\n\nAlso, if this is passing the TCK and there is a gap in the TCK testing, it sounds like it would be a good idea to fill that gap.": "Y",
  "I'm testing this PR with a 2 KVM host setup to run a set of basic Marvin tests. Will also grep the logs for the private key.\n": "N",
  "I compile and `run()` script classes manually. When compiling, I keep track of the classes that are generated for a script and manually unload them using `InvokerHelper.removeClass()` afterward. Then I have to use reflection to remove them from `globalClassSet`/`globalClassValue`. So what I really need from this PR is the added `ClassInfo.remove()` method and its being called from `InvokerHelper.removeClass()`.\n": "Y",
  "Looks good in general.\n\nAny reason that the tests cannot be part of `flink-tests`? Just thinking that we may reduce our maven project sprawl a bit ;-)\n": "Y",
  "That is fine. @milamberspace would you mind just doing a force push again to kick off jenkins. I have seen other jenkins runs passing so I think jenkins is just not happy when it has a lot of load.": "Y",
  "I commented on the bug.": "N",
  "While my point of view on things is of a Spark (+YARN) cluster, I'm starting to get the feeling that there are a lot of interest in \"out-of-the-box\" packaging..\n\nLet me raise that in the mailing list to get people's thought on this, and I might change the build to either compile or use profiles or something.\n": "Y",
  "Thanks for your contribution, but I don't think this is the correct solution.\nIf we only update the jars, the code will not compile anymore. There are a lot of missing and deprecated methods and classes. I tried to update our usage of the api to the current one, but that is not possible without a lot of work. (The authentication mechanism has changed and the eval mechanism, that we depend on has been deprecated/removed).\nAs the mongo sampler has been deprecated with JMeter 3.0. So a better patch would be to remove the client jars altogether.": "Y",
  "> > I think it's better to have things in CORE for what seems to be a \"core\" feature, making live metrics available.\n> \n> One of the options is to bundle the plugin into the final distribution (like we bundle lots of third-party dependencies).\n> \n> I have not tried that myself, however, their [wiki says](https://gitlab.com/testload/jmeter-listener/-/wikis/1.-Main) there are reasons to group data before sending:\n> \n> > Activation of GROUP_BY function (only couple parameters in settings) could be very useful on highload (thousands of transaction per second) because on such intensity InfluxDB/ElasticSearch/Clickhouse can be bottleneck (IOPS). In this mode all Samplers grouping by name and only aggregates\n\nThis plugin has indeed good ideas, but implementation should be improved if embedded in JMeter (https://gitlab.com/testload/jmeter-listener/-/issues).\n": "N",
  "@peihe anything new here ? because https://github.com/apache/incubator-beam/pull/539 is passing tests now - but like you said, it doesn't eliminate code duplication.\n\nI don't see this working if the runner doesn't have a `compile` scope dependency on the engine, and at least for Spark, I'm not sure it's the best way to go.\n\nPinging @jbonofre: from your experience with customers, is Spark usually provided ? \n": "Y",
  "Aren't we using Scala's BigDecimal? \n": "N",
  "Looks good. Would you mind squashing this down to one commit please (it retains your name rather than us squash it which doesn't).": "Y",
  "@blueorangutan test matrix\n": "N",
  "Tool \"Copyright (c) 2013-2018 Mozilla Foundation\" (https://github.com/validator/validator/blob/master/src/nu/validator/client/SimpleCommandLineValidator.java), so license seems to be ok.\n\nRuns fine. Errors are printed to the console.\n\nIt seems that the tool does not support writing a report to file.\nCould you try to create a report file with Ant (&lt;redirector>)? So this target would behave like the other report-targets.": "Y",
  "SGTM. We should probably switch to this: https://issues.apache.org/jira/browse/HIVE-6017 although it is a lower priority task (although for the purpose of API compatibility, maybe we want to make it higher priority0.\n": "Y",
  "I agree this should configurable on the Host.\nIt should be possible to remove the Comparator entirely if the version code is used directly in the Mapper.\nI'd like this to be more robust. I'm thinking pad any dot-separated segment to a given length with a suitable character (possible space).": "Y",
  "Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community\nto review your pull request. We will use this comment to track the progress of the review.\n\n\n## Automated Checks\nLast check on commit 195d03891a9c5cef972527218e8f1be6693ff695 (Sat May 08 07:57:45 UTC 2021)\n\n✅no warnings\n\n<sub>Mention the bot in a comment to re-run the automated checks.</sub>\n## Review Progress\n\n* ❓ 1. The [description] looks good.\n* ❓ 2. There is [consensus] that the contribution should go into to Flink.\n* ❓ 3. Needs [attention] from.\n* ❓ 4. The change fits into the overall [architecture].\n* ❓ 5. Overall code [quality] is good.\n\nPlease see the [Pull Request Review Guide](https://flink.apache.org/contributing/reviewing-prs.html) for a full explanation of the review process.<details>\nThe Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>\nThe @flinkbot bot supports the following commands:\n\n- `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)\n- `@flinkbot approve all` to approve all aspects\n- `@flinkbot approve-until architecture` to approve everything until `architecture`\n- `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention\n- `@flinkbot disapprove architecture` to remove an approval you gave earlier\n</details>": "N",
  "My biggest concern with this PR at the moment is its size. Could we add the feature (errorprone) without changing every nag it shows? I tend to get bored when looking at big PRs and either skip parts, or abort looking at it altogether.\nApart from that, I am +1 on adding the feature (if it doesn't break the build when running with Java 8). (Haven't tried it)": "Y",
  "👍 I think this is better than reverting the three commits.": "N",
  "I've merged this in master.\n": "N",
  "Can you please rebase your PR as it has conflicts with the repo ? Thanks !\n": "N",
  "Thanks @jaikiran, I've added a buch of inline comments.\r\n\r\nOverall I'm in favor of this change and if you want to spend the time on fixing the bugzilla issue in a Java5 friendly way for 1.9.x that would certainly be good - bit not something I'd see as an requirement.\r\n\r\nThe `SymlinkTest` still ensures tests are skipped on platforms that are not Unix (see the `assume` in `setup`). I'm not sure if we can figure out whether Java supports symlinks on the platform easily but it would certainly be good to test things on non-Unix platforms if we claim the task should work.\r\n": "Y",
  "Overall LGTM\n": "N",
  "There were fewer API changes than expected the changes in the PR have been applied excluding the DTD/schema files identified above. If you could rebase the PR and confirm nothing has been missed that would be great.": "Y",
  "Actually, let me take a step back and understand a few things deeper, first.\nWho actually generates the watermarks (in ingestion time)? The operator that creates the file splits, or the operator that reads the splits?\n\nIf the configuration is set to IngestionTime, will the operator that creates the file splits emit a final LongMax watermark? Is that one passing through by the split-reading operator? Is there a test that test that specific scenario? (I believe it was the initially reported bug).\n": "Y",
  "@ropalka thank you, LGTM, I have a few minor comments which I could address myself if you prefer, otherwise - I think we are good to go": "N",
  "closing in favor of #1250": "N",
  ":broken_heart: **-1 overall** \n\n\n\n\n\n\n| Vote | Subsystem | Runtime | Comment | \n|:----:|----------:|--------:|:--------| \n| +0 :ok: | reexec | 1m 50s | Docker mode activated. | \n||| _ Prechecks _ | \n| +1 :green_heart: | dupname | 0m 0s | No case conflicting files found. | \n| +1 :green_heart: | hbaseanti | 0m 1s | Patch does not have any anti-patterns. | \n| +1 :green_heart: | @author | 0m 0s | The patch does not contain any @author tags. | \n| -0 :warning: | test4tests | 0m 0s | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | \n||| _ master Compile Tests _ | \n| +0 :ok: | mvndep | 0m 35s | Maven dependency ordering for branch | \n| +1 :green_heart: | mvninstall | 5m 58s | master passed | \n| +1 :green_heart: | compile | 1m 29s | master passed | \n| +1 :green_heart: | checkstyle | 2m 8s | master passed | \n| +1 :green_heart: | shadedjars | 5m 5s | branch has no errors when building our shaded downstream artifacts. | \n| +1 :green_heart: | javadoc | 1m 4s | master passed | \n| +0 :ok: | spotbugs | 5m 37s | Used deprecated FindBugs config; considering switching to SpotBugs. | \n| +1 :green_heart: | findbugs | 6m 59s | master passed | \n||| _ Patch Compile Tests _ | \n| +0 :ok: | mvndep | 0m 15s | Maven dependency ordering for patch | \n| +1 :green_heart: | mvninstall | 5m 41s | the patch passed | \n| +1 :green_heart: | compile | 1m 28s | the patch passed | \n| +1 :green_heart: | javac | 1m 28s | the patch passed | \n| +1 :green_heart: | checkstyle | 2m 4s | the patch passed | \n| +1 :green_heart: | whitespace | 0m 0s | The patch has no whitespace issues. | \n| +1 :green_heart: | shadedjars | 5m 8s | patch has no errors when building our shaded downstream artifacts. | \n| +1 :green_heart: | hadoopcheck | 17m 53s | Patch does not cause any errors with Hadoop 2.8.5 2.9.2 or 3.1.2. | \n| +1 :green_heart: | javadoc | 1m 9s | the patch passed | \n| +1 :green_heart: | findbugs | 6m 41s | the patch passed | \n||| _ Other Tests _ | \n| +1 :green_heart: | unit | 1m 54s | hbase-client in the patch passed. | \n| -1 :x: | unit | 192m 22s | hbase-server in the patch failed. | \n| +1 :green_heart: | asflicense | 0m 50s | The patch does not generate ASF License warnings. | \n| | | 268m 1s | | \n\n\n| Reason | Tests | \n|-------:|:------| \n| Failed junit tests | hadoop.hbase.master.procedure.TestSCPWithMetaWithReplicasWithoutZKCoordinated | \n\n\n| Subsystem | Report/Notes | \n|----------:|:-------------| \n| Docker | Client=19.03.5 Server=19.03.5 base: https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/1/artifact/out/Dockerfile | \n| GITHUB PR | https://github.com/apache/hbase/pull/977 | \n| JIRA Issue | HBASE-23628 | \n| Optional Tests | dupname asflicense javac javadoc unit spotbugs findbugs shadedjars hadoopcheck hbaseanti checkstyle compile | \n| uname | Linux a5f587eff5f0 4.15.0-66-generic #75-Ubuntu SMP Tue Oct 1 05:24:09 UTC 2019 x86_64 GNU/Linux | \n| Build tool | maven | \n| Personality | /home/jenkins/jenkins-slave/workspace/HBase-PreCommit-GitHub-PR_PR-977/out/precommit/personality/provided.sh | \n| git revision | master / 923ba7763e | \n| Default Java | 1.8.0_181 | \n| unit | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/1/artifact/out/patch-unit-hbase-server.txt | \n| Test Results | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/1/testReport/ | \n| Max. process+thread count | 4122 (vs. ulimit of 10000) | \n| modules | C: hbase-client hbase-server U: . | \n| Console output | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/1/console | \n| versions | git=2.11.0 maven=2018-06-17T18:33:14Z) findbugs=3.1.11 | \n| Powered by | Apache Yetus 0.11.1 https://yetus.apache.org | \n\n\nThis message was automatically generated.": "N",
  "> In a parallel universe, there's https://gitlab.com/testload/jmeter-listener (MIT)\n\nHave you tried it ? \nI think it's better to have things in CORE for what seems to be a \"core\" feature, making live metrics available.\nNot all users of JMeter are aware of 3rd party plugins.": "Y",
  "@milamberspace can you do a force push again. I have pushed fixes to Jenkins and Travis this morning, so with a new push we should be able to get this green. Thx...": "N",
  "@nitin-maharana Thanks. Are there any other api calls that have the same issue?\n": "N",
  "@danielsoro can you resolve the conflicht in `ClaimBean.java` ?\n\n@jgallimore I agree with your thoughs. I also think, that the `PropertyEditorRegistry` should be tied to the lifecycle of the component using it. Should we change this inside this PR or open a new JIRA and do this optimization work in a separate branch / take it as a feature? I would be +1 for the JIRA option.": "Y",
  "@hsun-cnnxty : <s>I don't see the reference links in the Description on the PR's Conversation view? Maybe I'm looking in the wrong place? Maybe they should be in comments in the code too?</s>\nYup, was looking in the wrong place, I didn't realize that \"the description\" meant the **JIRA issue**'s Description! \n\nI'm guessing the following are the links you meant to put:\n- https://cwiki.apache.org/confluence/display/KAFKA/Committing+and+fetching+consumer+offsets+in+Kafka\n- https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol\n\nI just want it to be very clear that [this PR](https://github.com/apache/storm/pull/705)'s purpose is to change from the kafka spout's consumer offsets being stored in ZooKeeper, to instead being stored directly in Kafka. We should also be clear about the version of Kafka required for such support (0.8.2+). I know the current version of the storm-kafka pom.xml (as of this change) is already referencing 0.8.2.1, but I feel like it should be called out as an explicit requirement in the commit.\n": "Y",
  "Okay, I'm going to merge this into master while removing the unnecessary deletion. Thanks.\n": "N",
  "@NuxRo this patch does not appear to break things. However, we want to add a Marvin test case to verify that the fix does not regress again. Does that make sense?\n\n@rhtyd is planning to write the Marvin test case this week. Once he is done and we retest, we will merge this PR.\n": "Y",
  "@rhtyd a Trillian-Jenkins matrix job (centos6 mgmt + xs56sp1, centos7 mgmt + vmware55u3, ubuntu mgmt + kvmcentos7) has been kicked to run smoke tests against packages at http://packages.shapeblue.com/cloudstack/pr/1653\n": "N",
  "If I understand correctly, this also this changes the semantics that we execute programs without sinks, and also topology branches which don't end in sinks. I personally don't like the fact that the each branch in the processing graph needs to end in a sink, it is rather artificial.\n": "Y",
  "retest this please\n": "N",
  "Hi Esteban,\n\nthanks for the pull request. :)\n\nYou talking about getXZYPath() has me wondering if your fix will work with Java 6 (because Groovy still supports that)?\n\nThanks,\nPascal": "Y",
  ":broken_heart: **-1 overall** \n\n\n\n\n\n\n| Vote | Subsystem | Runtime | Comment | \n|:----:|----------:|--------:|:--------| \n| +0 :ok: | reexec | 1m 22s | Docker mode activated. | \n||| _ Prechecks _ | \n| +1 :green_heart: | dupname | 0m 0s | No case conflicting files found. | \n| +1 :green_heart: | hbaseanti | 0m 0s | Patch does not have any anti-patterns. | \n| +1 :green_heart: | @author | 0m 0s | The patch does not contain any @author tags. | \n| -0 :warning: | test4tests | 0m 0s | The patch doesn't appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. | \n||| _ master Compile Tests _ | \n| +0 :ok: | mvndep | 0m 36s | Maven dependency ordering for branch | \n| +1 :green_heart: | mvninstall | 5m 45s | master passed | \n| +1 :green_heart: | compile | 1m 23s | master passed | \n| +1 :green_heart: | checkstyle | 2m 5s | master passed | \n| +1 :green_heart: | shadedjars | 5m 3s | branch has no errors when building our shaded downstream artifacts. | \n| +1 :green_heart: | javadoc | 1m 0s | master passed | \n| +0 :ok: | spotbugs | 4m 52s | Used deprecated FindBugs config; considering switching to SpotBugs. | \n| +1 :green_heart: | findbugs | 6m 4s | master passed | \n||| _ Patch Compile Tests _ | \n| +0 :ok: | mvndep | 0m 17s | Maven dependency ordering for patch | \n| +1 :green_heart: | mvninstall | 5m 43s | the patch passed | \n| +1 :green_heart: | compile | 1m 33s | the patch passed | \n| +1 :green_heart: | javac | 1m 33s | the patch passed | \n| +1 :green_heart: | checkstyle | 2m 8s | the patch passed | \n| +1 :green_heart: | whitespace | 0m 0s | The patch has no whitespace issues. | \n| +1 :green_heart: | shadedjars | 5m 20s | patch has no errors when building our shaded downstream artifacts. | \n| +1 :green_heart: | hadoopcheck | 18m 21s | Patch does not cause any errors with Hadoop 2.8.5 2.9.2 or 3.1.2. | \n| +1 :green_heart: | javadoc | 1m 1s | the patch passed | \n| +1 :green_heart: | findbugs | 6m 43s | the patch passed | \n||| _ Other Tests _ | \n| +1 :green_heart: | unit | 1m 59s | hbase-client in the patch passed. | \n| -1 :x: | unit | 196m 3s | hbase-server in the patch failed. | \n| +1 :green_heart: | asflicense | 1m 22s | The patch does not generate ASF License warnings. | \n| | | 271m 29s | | \n\n\n| Reason | Tests | \n|-------:|:------| \n| Failed junit tests | hadoop.hbase.master.TestMasterShutdown | \n\n\n| Subsystem | Report/Notes | \n|----------:|:-------------| \n| Docker | Client=19.03.5 Server=19.03.5 base: https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/2/artifact/out/Dockerfile | \n| GITHUB PR | https://github.com/apache/hbase/pull/977 | \n| JIRA Issue | HBASE-23628 | \n| Optional Tests | dupname asflicense javac javadoc unit spotbugs findbugs shadedjars hadoopcheck hbaseanti checkstyle compile | \n| uname | Linux 9c4e51ac674c 4.15.0-70-generic #79-Ubuntu SMP Tue Nov 12 10:36:11 UTC 2019 x86_64 GNU/Linux | \n| Build tool | maven | \n| Personality | /home/jenkins/jenkins-slave/workspace/HBase-PreCommit-GitHub-PR_PR-977/out/precommit/personality/provided.sh | \n| git revision | master / 06eff551c3 | \n| Default Java | 1.8.0_181 | \n| unit | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/2/artifact/out/patch-unit-hbase-server.txt | \n| Test Results | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/2/testReport/ | \n| Max. process+thread count | 4641 (vs. ulimit of 10000) | \n| modules | C: hbase-client hbase-server U: . | \n| Console output | https://builds.apache.org/job/HBase-PreCommit-GitHub-PR/job/PR-977/2/console | \n| versions | git=2.11.0 maven=2018-06-17T18:33:14Z) findbugs=3.1.11 | \n| Powered by | Apache Yetus 0.11.1 https://yetus.apache.org | \n\n\nThis message was automatically generated.": "N",
  "Merged build triggered. \n": "N",
  "I can see 2 enhancements to this class \n1) let 'random' be a final field \n2) use Random.nextInt(int bound) instead, to improve code readability \n\nI'm using this inspiration to update the class in the 'v2' development branch. Thx! \n\n```java \nprivate static final int NUMBER_CHARACTERS = 10; \nprivate static final String CHARACTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890\"; \n\nprivate final Random random = new Random(); \n\n@Override \npublic String generateRandomCode() { \nfinal StringBuilder buf = new StringBuilder(NUMBER_CHARACTERS); \nfor (int i = 0; i < NUMBER_CHARACTERS; i++) { \nfinal int pos = random.nextInt(CHARACTERS.length()); \nbuf.append(CHARACTERS.charAt(pos)); \n} \nreturn buf.toString(); \n} \n```": "Y",
  "@harshach \nI skimmed a bit, and guess determining fix version will not work since branch names we use are different from Spark and Kafka and so on. We can still input them by hand so there's no issue on it but if we modify it to fit for Storm that would be great. (optional)\n\nFYI: Spark script is having same issue.\nhttps://github.com/apache/spark/blob/master/dev/merge_spark_pr.py\n": "Y",
  "I'm not sure it is worth adding yet another Listerer implementation.\n\nIt looks like the users would have to copy-paste the configuration if they want to switch between raw and grouped data.": "Y",
  "@swill @milamberspace weird, we've lost our Travis integration with PRs here. https://travis-ci.org/apache/cloudstack/ says \"The repository at apache/cloudstack was not found\"": "N",
  "Furthermore, can you please provide a reference to \"Kafka's consumer offset management api\" in your description?\n": "N",
  "In fact Tomcat has hundreds of such places where simple, safe, and more readable optimizations can be applied. Just open sources in IntelliJ, open Settings/Editor/Inspections/Java/Performarmance and enable all and then run Analyze.\nMost hot places are unefficient working with maps like containsKey() then get() call and collections ans StringBuilder without initial capacity.\nIt would be great just to fix all these problems at once and this will give a real impact on performance": "Y",
  "The fix has been applied with https://issues.apache.org/jira/browse/WICKET-6398.\n@Jezza Please close this PR! Thank you !": "N",
  "#1515 has now been merged (sorry for the delay). Once you have a chance to fix the merge conflicts, I can get this merged. Thx...": "N",
  "> My last two jenkins jobs indicated it took 0s to run, but the logs show that it was aborted after 2 hours. Any idea what could be going on?\r\n\r\nIt looks like it hit the 2h Jenkins timeout. Should be extended to 3h once https://github.com/apache/beam/pull/10234 is merged and the next seed job runs. (I'll start one in #10234)": "Y",
  "> It will ask for primary authors and the user who is merging this can input more than one author at the time of merge.\n\nThat means it removes authorship information. If we tag a squashed commit as coming from multiple authors, we still wouldn't be able to differentiate what code was contributed by the individual authors.\n\nSo if I merged a pull request with multiple authors, the result would be a single commit from me with a message listing the contributing authors, is that correct?\n": "Y",
  "**[Test build #47781 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/47781/consoleFull)** for PR 10293 at commit [`e35d22d`](https://github.com/apache/spark/commit/e35d22db58081261292bb4be9271aa41e9f04ced).\n- This patch passes all tests.\n- This patch merges cleanly.\n- This patch adds no public classes.\n": "N",
  "Could this please be merged then?\n": "N",
  "TDB (1 and 2) tests can run into issues. Memory mapped files on MS Windows do not get deleted from the file system until the JVM process exits This is a long standing Java bug.\n\n\n\nThe tests for Windows try to reuse database but it still leaves a significant over head.\n\n\n\nAslo, any crashes have before left dead directories (JUnit clean up not happening fully?).\n\n\n\nIf it causes problems, then just have Linux and MacOS - we have MS Windows testing on Jenkins . Adding MacOS to the GH action CI is still a step forward.": "Y",
  "+1 to the change itself, but I feel it does not solve GROOVY-7646, because that should have not been happening even without you removing classes from the cache actively. I can for example change the eval script to contain a class besides the script class and then it should blow up again. Furthermore, Eval just uses GroovyShell. So you should get the same problem with GroovyShell too. So in my opinion your fix is not addressing the underlying issue\n": "Y",
  "hi alfusainey\n\ni just had a closer look at your lastest patch.\nwhile i definitely see that your current approach makes things easy from a developer\npoint of view, it somehow defeats the purpose of having json-based remoting if\nthe complete json string is finally just converted to xml. that's the biggest concern\nthat i currently have with your approach.\n\napart from that, just a kind reminder with respect to the diffs: i would be really glad\nif you could make sure that your patches don't contain modifications unrelated to\nyour work.. that also applies for the import statements.\n\nkind regards\nangela\n\nFrom: Alfusainey <notifications@github.com<mailto:notifications@github.com>>\nReply-To: apache/jackrabbit <reply@reply.github.com<mailto:reply@reply.github.com>>\nDate: Sunday 25 May 2014 02:36\nTo: apache/jackrabbit <jackrabbit@noreply.github.com<mailto:jackrabbit@noreply.github.com>>\nCc: anchela <angela@apache.org<mailto:angela@apache.org>>\nSubject: Re: [jackrabbit] XML content import for remoting server (#19)\n\nReply to this email directly or view it on GitHubhttps://github.com/apache/jackrabbit/pull/19#issuecomment-44108291.\n": "Y",
  "I think this one is ok at this point - note the creation of https://issues.apache.org/jira/browse/TINKERPOP-2082 for other GLVs \n\nVOTE +1": "N",
  "> @JingsongLi I noticed that the \"getScalarFunction\" method in \"ScalarSqlFunction\" of blink planner was replaced by \"makeFunction\" method. I think using \"getScalarFunction\" to get the initial ScalarFunction object and check its language type makes more sense here so I re-add the method. Please take a look at this PR to make sure it does not cause any side effects to blink planner :).\r\n\r\nLGTM, it is reasonable, can you just use scala val to scalarFunction just like `TableSqlFunction`?": "Y",
  "Looks great, many thanks! I'll merge it to both branches and add `@since` markers.\r\n\r\nWe'll need to update the manual (probably of zip, jar, war and ear). Also some additional unit tests for `parseLenient` would be good (good old JUnit tests). If you want to take a stab at either, please do so in a fresh PR. Otherwise I'll carve out some time to do it myself.": "Y",
  "@DaanHoogland sent me the below patch the other day and I run some tests on it. Just FYI the results (he rebased later and made a PR). The commit hash obviously doesn't match as it was my temp patch apply commit.\n\nThe patch I tested:\n\n```\ncommit cad68778d8714a5359e3fa79a33d05206e032fea\nAuthor: root <root@cs1.cloud.lan>\nDate: Mon Nov 9 12:40:36 2015 +0000\n\nprivate key security fix received from Daan\n\ndiff --git a/api/src/org/apache/cloudstack/api/command/admin/resource/UploadCustomCertificateCmd.java b/api/src/org/apache/cloudstack/api/command/admin/resource/UploadCustomCertificateCmd.java\nindex e11876a..e8d6cc5 100644\n--- a/api/src/org/apache/cloudstack/api/command/admin/resource/UploadCustomCertificateCmd.java\n+++ b/api/src/org/apache/cloudstack/api/command/admin/resource/UploadCustomCertificateCmd.java\n@@ -32,7 +32,7 @@ import com.cloud.user.Account;\n@APICommand(name = \"uploadCustomCertificate\",\nresponseObject = CustomCertificateResponse.class,\ndescription = \"Uploads a custom certificate for the console proxy VMs to use for SSL. Can be used to upload a single certificate signed by a known CA. Can also be used, through mu\n- requestHasSensitiveInfo = false, responseHasSensitiveInfo = false)\n+ requestHasSensitiveInfo = true, responseHasSensitiveInfo = false)\npublic class UploadCustomCertificateCmd extends BaseAsyncCmd {\npublic static final Logger s_logger = Logger.getLogger(UploadCustomCertificateCmd.class.getName());\n\ndiff --git a/utils/src/main/java/com/cloud/utils/StringUtils.java b/utils/src/main/java/com/cloud/utils/StringUtils.java\nindex c598be8..71cebe1 100644\n--- a/utils/src/main/java/com/cloud/utils/StringUtils.java\n+++ b/utils/src/main/java/com/cloud/utils/StringUtils.java\n@@ -186,7 +186,7 @@ public class StringUtils {\nprivate static final Pattern REGEX_PASSWORD_QUERYSTRING = Pattern.compile(\"(&|%26)?[^(&|%26)]*((p|P)assword|accesskey|secretkey)(=|%3D).*?(?=(%26|[&'\\\"]|$))\");\n\n// removes a password/accesskey/ property from a response json object\n- private static final Pattern REGEX_PASSWORD_JSON = Pattern.compile(\"\\\"((p|P)assword|accesskey|secretkey)\\\":\\\\s?\\\".*?\\\",?\");\n+ private static final Pattern REGEX_PASSWORD_JSON = Pattern.compile(\"\\\"((p|P)assword|privatekey|accesskey|secretkey)\\\":\\\\s?\\\".*?\\\",?\");\n\nprivate static final Pattern REGEX_PASSWORD_DETAILS = Pattern.compile(\"(&|%26)?details(\\\\[|%5B)\\\\d*(\\\\]|%5D)\\\\.key(=|%3D)((p|P)assword|accesskey|secretkey)(?=(%26|[&'\\\"]))\");\n```\n\nTests:\n\n```\nnosetests --with-marvin --marvin-config=${marvinCfg} -s -a tags=advanced,required_hardware=true \\\ncomponent/test_vpc_redundant.py \\\ncomponent/test_routers_iptables_default_policy.py \\\ncomponent/test_routers_network_ops.py \\\ncomponent/test_vpc_router_nics.py \\\nsmoke/test_loadbalance.py \\\nsmoke/test_internal_lb.py \\\nsmoke/test_ssvm.py \\\nsmoke/test_network.py\n```\n\nResult:\n\n```\n[root@cs1 MarvinLogs]# cat test_network_XFGU4E/results.txt \nCreate a redundant VPC with two networks with two VMs in each network ... === TestName: test_01_create_redundant_VPC_2tiers_4VMs_4IPs_4PF_ACL | Status : SUCCESS ===\nok\nCreate a redundant VPC with two networks with two VMs in each network and check default routes ... === TestName: test_02_redundant_VPC_default_routes | Status : SUCCESS ===\nok\nTest iptables default INPUT/FORWARD policy on RouterVM ... === TestName: test_02_routervm_iptables_policies | Status : SUCCESS ===\nok\nTest iptables default INPUT/FORWARD policies on VPC router ... === TestName: test_01_single_VPC_iptables_policies | Status : SUCCESS ===\nok\nStop existing router, add a PF rule and check we can access the VM ... === TestName: test_isolate_network_FW_PF_default_routes | Status : SUCCESS ===\nok\nTest redundant router internals ... === TestName: test_RVR_Network_FW_PF_SSH_default_routes | Status : SUCCESS ===\nok\nCreate a VPC with two networks with one VM in each network and test nics after destroy ... === TestName: test_01_VPC_nics_after_destroy | Status : SUCCESS ===\nok\nCreate a VPC with two networks with one VM in each network and test default routes ... === TestName: test_02_VPC_default_routes | Status : SUCCESS ===\nok\nCheck the password file in the Router VM ... === TestName: test_isolate_network_password_server | Status : SUCCESS ===\nok\nCheck that the /etc/dhcphosts.txt doesn't contain duplicate IPs ... === TestName: test_router_dhcphosts | Status : SUCCESS ===\nok\nTest to create Load balancing rule with source NAT ... === TestName: test_01_create_lb_rule_src_nat | Status : SUCCESS ===\nok\nTest to create Load balancing rule with non source NAT ... === TestName: test_02_create_lb_rule_non_nat | Status : SUCCESS ===\nok\nTest for assign & removing load balancing rule ... === TestName: test_assign_and_removal_lb | Status : SUCCESS ===\nok\nTest to verify access to loadbalancer haproxy admin stats page ... === TestName: test02_internallb_haproxy_stats_on_all_interfaces | Status : SUCCESS ===\nok\nTest create, assign, remove of an Internal LB with roundrobin http traffic to 3 vm's ... === TestName: test_01_internallb_roundrobin_1VPC_3VM_HTTP_port80 | Status : SUCCESS ===\nok\nTest SSVM Internals ... === TestName: test_03_ssvm_internals | Status : SUCCESS ===\nok\nTest CPVM Internals ... === TestName: test_04_cpvm_internals | Status : SUCCESS ===\nok\nTest stop SSVM ... === TestName: test_05_stop_ssvm | Status : SUCCESS ===\nok\nTest stop CPVM ... === TestName: test_06_stop_cpvm | Status : SUCCESS ===\nok\nTest reboot SSVM ... === TestName: test_07_reboot_ssvm | Status : SUCCESS ===\nok\nTest reboot CPVM ... === TestName: test_08_reboot_cpvm | Status : SUCCESS ===\nok\nTest destroy SSVM ... === TestName: test_09_destroy_ssvm | Status : SUCCESS ===\nok\nTest destroy CPVM ... === TestName: test_10_destroy_cpvm | Status : SUCCESS ===\nok\nTest for port forwarding on source NAT ... === TestName: test_01_port_fwd_on_src_nat | Status : SUCCESS ===\nok\nTest for port forwarding on non source NAT ... === TestName: test_02_port_fwd_on_non_src_nat | Status : SUCCESS ===\nok\nTest for reboot router ... === TestName: test_reboot_router | Status : SUCCESS ===\nok\nTest for Router rules for network rules on acquired public IP ... === TestName: test_network_rules_acquired_public_ip_1_static_nat_rule | Status : SUCCESS ===\nok\nTest for Router rules for network rules on acquired public IP ... === TestName: test_network_rules_acquired_public_ip_2_nat_rule | Status : SUCCESS ===\nok\nTest for Router rules for network rules on acquired public IP ... === TestName: test_network_rules_acquired_public_ip_3_Load_Balancer_Rule | Status : SUCCESS ===\nok\n----------------------------------------------------------------------\nRan 29 tests in 12467.478s\nOK\n```\n\nAnd:\n\n```\nnosetests --with-marvin --marvin-config=${marvinCfg} -s -a tags=advanced,required_hardware=false \\\nsmoke/test_routers.py \\\nsmoke/test_network_acl.py \\\nsmoke/test_privategw_acl.py \\\nsmoke/test_reset_vm_on_reboot.py \\\nsmoke/test_vm_life_cycle.py \\\nsmoke/test_vpc_vpn.py \\\nsmoke/test_service_offerings.py \\\ncomponent/test_vpc_offerings.py \\\ncomponent/test_vpc_routers.py\n```\n\nResult:\n\n```\n[root@cs1 MarvinLogs]# cat test_vpc_routers_BFJ8KP/results.txt \nTest router internal advanced zone ... === TestName: test_02_router_internal_adv | Status : SUCCESS ===\nok\nTest restart network ... === TestName: test_03_restart_network_cleanup | Status : SUCCESS ===\nok\nTest router basic setup ... === TestName: test_05_router_basic | Status : SUCCESS ===\nok\nTest router advanced setup ... === TestName: test_06_router_advanced | Status : SUCCESS ===\nok\nTest stop router ... === TestName: test_07_stop_router | Status : SUCCESS ===\nok\nTest start router ... === TestName: test_08_start_router | Status : SUCCESS ===\nok\nTest reboot router ... === TestName: test_09_reboot_router | Status : SUCCESS ===\nok\ntest_privategw_acl (integration.smoke.test_privategw_acl.TestPrivateGwACL) ... === TestName: test_privategw_acl | Status : SUCCESS ===\nok\nTest reset virtual machine on reboot ... === TestName: test_01_reset_vm_on_reboot | Status : SUCCESS ===\nok\nTest advanced zone virtual router ... === TestName: test_advZoneVirtualRouter | Status : SUCCESS ===\nok\nTest Deploy Virtual Machine ... === TestName: test_deploy_vm | Status : SUCCESS ===\nok\nTest Multiple Deploy Virtual Machine ... === Tes": "N",
  "Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/16073/\n": "N",
  "Can you create the PR against master? We only merge to master and then backport.": "Y",
  "Thanks @arunmahadevan merged into master. Can you please open another PR for 1.x-branch. \n": "N",
  "When is probably hard to say as we currently do not follow a defined release-timeline, release version will be 4.0, which we started to work on 2 months ago, a release typically happens every few months, so early 2018 would be a rough guess.": "N",
  "VOTE: +1\n": "N",
  "I'll second @knusbaum's -1. Based on points I made earlier. This has the potntial to automatically destroy code provenance, especially if more than one contributor is involved in a pull request. From a legal perspective, the ASF need to be able to determine the origin of all code changes.\n\nI would recommend any project that uses a variant of this script to double-check with ASF legal that it is okay. I could be totally wrong, but I'd rather play it safe.\n\nI'd also argue that a well thought out series of commits cane make reviews easier. For example, separating maven build changes from code changes. I'd rather we encourage devs to think out the partitioning of commits themselves and squash appropriately if necessary.\n": "Y",
  "Change looks pretty safe. Event check for `null` in message is kind of reasonable, because could return null https://github.com/JamesNK/Newtonsoft.Json/blob/a31156e90a14038872f54eb60ff0e9676ca4a0d8/Src/Newtonsoft.Json/JsonConvert.cs#L823 . Also at same time not sure if it hides bad JSON setting or other json related behind. Should we fail totally in this case?": "Y",
  "It seems as though the artifact server doesn't handle having multiple copies of the same artifact.\r\n\r\nI'm seeing this in the worker logs:\r\n```\r\nI 2019-12-18T20:20:07.933280467Z 2019/12/18 20:20:07 Initializing java harness: /opt/apache/beam/boot --id=1 --logging_endpoint=localhost:12370 --control_endpoint=localhost:12371 --artifact_endpoint=localhost:12372 --provision_endpoint=localhost:12373 --semi_persist_dir=/var/opt/google\r\n \r\nI 2019-12-18T20:20:14.570Z Worker harness starting with: {\"options\":{\"experiments\":[\"beam_fn_api\",\"beam_fn_api\",\"use_staged_dataflow_worker_jar\"],\"project\":\"apache-beam-testing\",\"dataflowJobId\":\"2019-12-18_12_18_43-4463651132060001350\",\"apiRootUrl\":\"https://dataflow.googleapis.com/\",\"defaultWorkerLogLevel\":\"INFO\",\"region\":\"us-central1\"}} \r\nI 2019-12-18T20:20:17.204Z Not using conscrypt SSL. Note this is the default Java behavior, but may have reduced performance. To use conscrypt SSL pass pipeline option --experiments=enable_conscrypt_security_provider \r\nI 2019-12-18T20:20:17.417Z Launched Beam Fn Logging service url: \"localhost:12370\"\r\n \r\nI 2019-12-18T20:20:17.428Z Launched Beam Fn Control service url: \"localhost:12371\"\r\n \r\nI 2019-12-18T20:20:17.432Z Launched Beam Fn Data service url: \"localhost:12371\"\r\n \r\nI 2019-12-18T20:22:10.692155827Z 2019/12/18 20:22:10 Failed to retrieve staged files: failed to dial server at localhost:12372\r\n \r\nI 2019-12-18T20:22:10.692189943Z \tcaused by:\r\n \r\nI 2019-12-18T20:22:10.692195647Z context deadline exceeded\r\n \r\nI 2019-12-18T20:22:11.421271885Z 2019/12/18 20:22:11 Initializing java harness: /opt/apache/beam/boot --id=1 --logging_endpoint=localhost:12370 --control_endpoint=localhost:12371 --artifact_endpoint=localhost:12372 --provision_endpoint=localhost:12373 --semi_persist_dir=/var/opt/google\r\n \r\nI 2019-12-18T20:24:11.428638869Z 2019/12/18 20:24:11 Failed to retrieve staged files: failed to dial server at localhost:12372\r\n \r\nI 2019-12-18T20:24:11.428673501Z \tcaused by:\r\n \r\nI 2019-12-18T20:24:11.428679088Z context deadline exceeded\r\n \r\nI 2019-12-18T20:24:23.605267533Z 2019/12/18 20:24:23 Initializing java harness: /opt/apache/beam/boot --id=1 --logging_endpoint=localhost:12370 --control_endpoint=localhost:12371 --artifact_endpoint=localhost:12372 --provision_endpoint=localhost:12373 --semi_persist_dir=/var/opt/google\r\n \r\nI 2019-12-18T20:26:23.612051533Z 2019/12/18 20:26:23 Failed to retrieve staged files: failed to dial server at localhost:12372\r\n \r\nI 2019-12-18T20:26:23.612105276Z \tcaused by:\r\n \r\nI 2019-12-18T20:26:23.612111707Z context deadline exceeded\r\n```\r\n\r\nand it seems as though the artifact server is failing to start because of a duplicate artifact:\r\n```\r\n2019-12-18 20:44:44.426 GMT\r\n2019/12/18 20:44:44 Starting retrieval proxy on localhost:12372\r\n \r\n2019-12-18 20:44:44.436 GMT\r\n2019/12/18 20:44:44 Warning: duplicate package kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar:gs://temp-storage-for-end-to-end-tests/pardotest0basictests0testpardoincustomtransform-jenkins-1218201828-52b8a37f/output/results/staging/kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar -> gs://temp-storage-for-end-to-end-tests/pardotest0basictests0testpardoincustomtransform-jenkins-1218201828-52b8a37f/output/results/staging/kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar\r\n2019-12-18 20:44:44.436 GMT\r\n2019/12/18 20:44:44 Warning: duplicate package kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar:gs://temp-storage-for-end-to-end-tests/pardotest0basictests0testpardoincustomtransform-jenkins-1218201828-52b8a37f/output/results/staging/kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar -> gs://temp-storage-for-end-to-end-tests/pardotest0basictests0testpardoincustomtransform-jenkins-1218201828-52b8a37f/output/results/staging/kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar\r\n2019-12-18 20:44:44.436 GMT\r\n2019/12/18 20:44:44 Warning: duplicate package kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar:gs://temp-storage-for-end-to-end-tests/pardotest0basictests0testpardoincustomtransform-jenkins-1218201828-52b8a37f/output/results/staging/kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar -> gs://temp-storage-for-end-to-end-tests/pardotest0basictests0testpardoincustomtransform-jenkins-1218201828-52b8a37f/output/results/staging/kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar\r\n2019-12-18 20:44:44.554 GMT\r\n2019/12/18 20:44:44 Found: name:\"dataflow-worker.jar\" permissions:420\r\n2019-12-18 20:44:44.568 GMT\r\n2019/12/18 20:44:44 Found: name:\"gradle-worker-t0NPniJD_BNrph4tUjEAcA.jar\" permissions:420\r\n2019-12-18 20:44:44.589 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-runners-google-cloud-dataflow-java-2.19.0-SNAPSHOT-vqQCaAYDJbCaZ0oA4mNI5g.jar\" permissions:420\r\n2019-12-18 20:44:44.609 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-runners-google-cloud-dataflow-java-2.19.0-SNAPSHOT-tests-M5hsuzpWA1C_SievTBu8Iw.jar\" permissions:420\r\n2019-12-18 20:44:44.627 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-sdks-java-io-google-cloud-platform-2.19.0-SNAPSHOT-gF7joDLdfBhIkT6CNOm0Jw.jar\" permissions:420\r\n2019-12-18 20:44:44.642 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-sdks-java-io-google-cloud-platform-2.19.0-SNAPSHOT-tests-22xEYN8ClmCrqJbqZ7bR6g.jar\" permissions:420\r\n2019-12-18 20:44:44.660 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-sdks-java-test-utils-2.19.0-SNAPSHOT-tests-_s4rMauR1JmL2T1vdRmCpQ.jar\" permissions:420\r\n2019-12-18 20:44:44.678 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-sdks-java-test-utils-2.19.0-SNAPSHOT-MB4nyV8X4YHcKD-5k8L3uQ.jar\" permissions:420\r\n2019-12-18 20:44:44.694 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-sdks-java-extensions-google-cloud-platform-core-2.19.0-SNAPSHOT-xG4ii-F4pQDibsy1ddSgOw.jar\" permissions:420\r\n2019-12-18 20:44:44.713 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-sdks-java-extensions-google-cloud-platform-core-2.19.0-SNAPSHOT-tests-V7CS-3V7KfLhfJpUNIUVqA.jar\" permissions:420\r\n2019-12-18 20:44:44.734 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-runners-core-construction-java-2.19.0-SNAPSHOT-_qToI7zqjDySCS_yxe3L3Q.jar\" permissions:420\r\n2019-12-18 20:44:44.752 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-sdks-java-extensions-protobuf-2.19.0-SNAPSHOT-vubRi4-OFW-I0LQ7KprTXg.jar\" permissions:420\r\n2019-12-18 20:44:44.768 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-runners-direct-java-2.19.0-SNAPSHOT-MY3GpZvnewQkmr_lsV1-WA.jar\" permissions:420\r\n2019-12-18 20:44:44.794 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-sdks-java-io-common-2.19.0-SNAPSHOT-tests-6KdNrj9GD_69MH2GU1U5PQ.jar\" permissions:420\r\n2019-12-18 20:44:44.813 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-sdks-java-io-common-2.19.0-SNAPSHOT-jyHKmvgVAwC95_AE4Ob6LQ.jar\" permissions:420\r\n2019-12-18 20:44:44.831 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-sdks-java-core-2.19.0-SNAPSHOT-tests-DJ4CnSGwjIRcevjig0iA3g.jar\" permissions:420\r\n2019-12-18 20:44:44.849 GMT\r\n2019/12/18 20:44:44 Found: name:\"beam-sdks-java-core-2.19.0-SNAPSHOT-9x5vxaFdu-Ny5rvW8SAy6Q.jar\" permissions:420\r\n2019-12-18 20:44:44.866 GMT\r\n2019/12/18 20:44:44 Found: name:\"hamcrest-library-2.1-PvgiE9wlzQ4mXcFvbiXZXA.jar\" permissions:420\r\n2019-12-18 20:44:44.954 GMT\r\n2019/12/18 20:44:44 Found: name:\"guava-testlib-25.1-jre-VHcJjyv-_w8Il-qpUZrmtg.jar\" permissions:420\r\n2019-12-18 20:44:44.977 GMT\r\n2019/12/18 20:44:44 Found: name:\"junit-quickcheck-core-0.8-EgEvYI3gIP_d2fBpDZJ42w.jar\" permissions:420\r\n2019-12-18 20:44:45.001 GMT\r\n2019/12/18 20:44:45 Found: name:\"powermock-module-junit4-2.0.2-rbQRTz39L0xkwlZMsTIavw.jar\" permissions:420\r\n2019-12-18 20:44:45.025 GMT\r\n2019/12/18 20:44:45 Found: name:\"grpc-all-1.17.1-ObISw2fPGxSCJVcuI5PXVg.jar\" permissions:420\r\n2019-12-18 20:44:45.046 GMT\r\n2019/12/18 20:44:45 Found: name:\"grpc-testing-1.17.1-ZCm5QWshyeL2B8wGncO7OQ.jar\" permissions:420\r\n2019-12-18 20:44:45.072 GMT\r\n2019/12/18 20:44:45 Found: name:\"powermock-module-junit4-common-2.0.2-El4eEVG3F1FJspjaZ06JzA.jar\" permissions:420\r\n2019-12-18 20:44:45.092 GMT\r\n2019/12/18 20:44:45 Found: name:\"junit-4.13-beta-3-SR8V8LeBEDL0Hb3Yr3GZaA.jar\" permissions:420\r\n2019-12-18 20:44:45.115 GMT\r\n2019/12/18 20:44:45 Found: name:\"hamcrest-core-2.1-wOGHxn46eRDV1UiUlkzBlQ.jar\" permissions:420\r\n2019-12-18 20:44:45.153 GMT\r\n2019/12/18 20:44:45 Found: name:\"beam-model-pipeline-2.19.0-SNAPSHOT-y_W94tehet9I9TeRqo3PEA.jar\" permissions:420\r\n2019-12-18 20:44:45.179 GMT\r\n2019/12/18 20:44:45 Found: name:\"beam-model-job-management-2.19.0-SNAPSHOT-JRNZ_ELrzeoUfantairRlw.jar\" permissions:420\r\n2019-12-18 20:44:45.202 GMT\r\n2019/12/18 20:44:45 Found: name:\"beam-vendor-bytebuddy-1_9_3-0.1-uWJWmyXQR6o6a6AbF7r0TA.jar\" permissions:420\r\n2019-12-18 20:44:45.232 GMT\r\n2019/12/18 20:44:45 Found: name:\"beam-vendor-guava-26_0-jre-0.1-6QokGG9WNL5MyYZ3G5HWjA.jar\" permissions:420\r\n2019-12-18 20:44:45.252 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-cloud-bigquerystorage-0.79.0-alpha-6UpzWBLaf_wow6RacZONWA.jar\" permissions:420\r\n2019-12-18 20:44:45.280 GMT\r\n2019/12/18 20:44:45 Found: name:\"bigtable-client-core-1.8.0-fFSIJpUcSXpWhLZcfslZzQ.jar\" permissions:420\r\n2019-12-18 20:44:45.296 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-cloud-spanner-1.6.0-SshEQyw4IrAiG9CeGXOlkw.jar\" permissions:420\r\n2019-12-18 20:44:45.372 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-cloud-bigtable-0.73.0-alpha-eB5GY2vHSW64cCChZOS18w.jar\" permissions:420\r\n2019-12-18 20:44:45.409 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-cloud-bigtable-admin-0.73.0-alpha-tf8RW5s2Pg4xX_7h-hnKCA.jar\" permissions:420\r\n2019-12-18 20:44:45.427 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-cloud-core-grpc-1.61.0-0jZ_NlVzGDZOmvFmfTqJwg.jar\" permissions:420\r\n2019-12-18 20:44:45.451 GMT\r\n2019/12/18 20:44:45 Found: name:\"gax-grpc-1.38.0-3ClsPaT9rq8Z5k4YOIfuXg.jar\" permissions:420\r\n2019-12-18 20:44:45.475 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-cloud-bigquery-1.28.0-Kl2D-E5AKdJ_R0IOB4xEZA.jar\" permissions:420\r\n2019-12-18 20:44:45.494 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-cloud-core-http-1.55.0-BNHYMDYs4SQW4PB1djuVig.jar\" permissions:420\r\n2019-12-18 20:44:45.513 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-cloud-core-1.61.0-IlSrOl7G255S3mvdat3lmw.jar\" permissions:420\r\n2019-12-18 20:44:45.530 GMT\r\n2019/12/18 20:44:45 Found: name:\"gax-httpjson-0.52.0-4ATeMZWHBufEm6llIAqk-g.jar\" permissions:420\r\n2019-12-18 20:44:45.547 GMT\r\n2019/12/18 20:44:45 Found: name:\"gax-1.38.0-FCo0lAFSkhHsRCXTmZ2K0g.jar\" permissions:420\r\n2019-12-18 20:44:45.567 GMT\r\n2019/12/18 20:44:45 Found: name:\"grpc-alts-1.17.1-utvCNzI1dYk3drhIBG-azw.jar\" permissions:420\r\n2019-12-18 20:44:45.584 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-auth-library-oauth2-http-0.12.0-uhKYUO6-pQeD5qoEqSlXPQ.jar\" permissions:420\r\n2019-12-18 20:44:45.599 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-api-services-clouddebugger-v2-rev20181114-1.28.0-U-z8PFQLYxPCTBDukg6SIw.jar\" permissions:420\r\n2019-12-18 20:44:45.621 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-api-services-dataflow-v1b3-rev20190927-1.28.0-ZgWxFdWAP6FEyoWz8lP5uw.jar\" permissions:420\r\n2019-12-18 20:44:45.643 GMT\r\n2019/12/18 20:44:45 Found: name:\"gcsio-1.9.16-iBTmtmP7wvXHVlGNaBCuzg.jar\" permissions:420\r\n2019-12-18 20:44:45.656 GMT\r\n2019/12/18 20:44:45 Found: name:\"util-1.9.16-jjyXrI5LEGRyIaXFKj9tvA.jar\" permissions:420\r\n2019-12-18 20:44:45.687 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-api-services-storage-v1-rev20181109-1.28.0-DJqyY-K3Ol9Ssdd-OqNoYg.jar\" permissions:420\r\n2019-12-18 20:44:45.713 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-api-services-cloudresourcemanager-v1-rev20181015-1.28.0-XNoKjKa8eomfOz0s-YI0iQ.jar\" permissions:420\r\n2019-12-18 20:44:45.733 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-api-services-bigquery-v2-rev20181221-1.28.0-R0jhsGj7UgWAkFOqtc-S2g.jar\" permissions:420\r\n2019-12-18 20:44:45.753 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-api-services-pubsub-v1-rev20181213-1.28.0-I5vY-LAnGjEk3u56f0tU0w.jar\" permissions:420\r\n2019-12-18 20:44:45.768 GMT\r\n2019/12/18 20:44:45 Found: name:\"datastore-v1-proto-client-1.6.3-yZQu0PLVT3TnUT_wNpicFA.jar\" permissions:420\r\n2019-12-18 20:44:45.820 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-api-client-java6-1.28.0-HAodKBbdqyrFdBPEZvVcaA.jar\" permissions:420\r\n2019-12-18 20:44:45.841 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-api-client-jackson2-1.28.0--W6_uwURyqtauUTVXwhXaw.jar\" permissions:420\r\n2019-12-18 20:44:45.875 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-api-client-1.28.0-SDG80NMNEzgrOvUKy7v1Pg.jar\" permissions:420\r\n2019-12-18 20:44:45.898 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-http-client-jackson2-1.28.0-Yyuo_SNjVh_9zfbnsFTr7A.jar\" permissions:420\r\n2019-12-18 20:44:45.916 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-oauth-client-java6-1.28.0-goLBdHjMZJZLuwqWvoQzGQ.jar\" permissions:420\r\n2019-12-18 20:44:45.932 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-oauth-client-1.28.0-uwS8kMzmcLlyBTJ2BAb7yg.jar\" permissions:420\r\n2019-12-18 20:44:45.949 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-http-client-apache-2.0.0-2uE2vYvcJczYvnh6THKxTw.jar\" permissions:420\r\n2019-12-18 20:44:45.969 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-http-client-protobuf-1.28.0-TVX5WOhxCLsZmjp1E38R9g.jar\" permissions:420\r\n2019-12-18 20:44:45.994 GMT\r\n2019/12/18 20:44:45 Found: name:\"google-http-client-jackson-1.28.0-JE8m9mp3cvILoTTSsuOGLA.jar\" permissions:420\r\n2019-12-18 20:44:46.013 GMT\r\n2019/12/18 20:44:46 Found: name:\"google-http-client-appengine-1.27.0-TdVXGCdmlhycAZxqyyGADQ.jar\" permissions:420\r\n2019-12-18 20:44:46.039 GMT\r\n2019/12/18 20:44:46 Found: name:\"google-http-client-1.28.0-d1m2BjJFwtcUlfahuoccNA.jar\" permissions:420\r\n2019-12-18 20:44:46.069 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-auth-1.17.1-FGjOS4ligPUTMy8WuF05hg.jar\" permissions:420\r\n2019-12-18 20:44:46.092 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-netty-1.17.1-4r8YjiC7Mk6Ki3FAZghiEQ.jar\" permissions:420\r\n2019-12-18 20:44:46.115 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-google-cloud-pubsub-v1-1.43.0-nl20guInYAxfIy09hLdNYQ.jar\" permissions:420\r\n2019-12-18 20:44:46.239 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-google-cloud-bigquerystorage-v1beta1-0.44.0-SsBroVH9fPiss71kHPKsmQ.jar\" permissions:420\r\n2019-12-18 20:44:46.270 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-google-common-protos-1.12.0-kY92YHJ3__3_EfGz7q7QJQ.jar\" permissions:420\r\n2019-12-18 20:44:46.288 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-google-cloud-bigtable-v2-0.38.0-q6Se3msUfLxCDeBbiqj1tA.jar\" permissions:420\r\n2019-12-18 20:44:46.304 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-google-cloud-bigtable-admin-v2-0.38.0-73d39v9pMyfVcnsTMrtv7Q.jar\" permissions:420\r\n2019-12-18 20:44:46.327 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-google-cloud-spanner-v1-1.6.0-DC0Nq4gVV4LS1Oe0H3HegA.jar\" permissions:420\r\n2019-12-18 20:44:46.346 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-google-cloud-spanner-admin-database-v1-1.6.0-7T0gXklb71lKsYSs9DFN0w.jar\" permissions:420\r\n2019-12-18 20:44:46.371 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-google-cloud-spanner-admin-instance-v1-1.6.0-CrMYysm29EPdTKEjGVeLIw.jar\" permissions:420\r\n2019-12-18 20:44:46.417 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-grpclb-1.17.1-VaGwdjH7d-oSQhPxT2IdGA.jar\" permissions:420\r\n2019-12-18 20:44:46.448 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-stub-1.17.1-q57ZbJwKOpHX7YVm-7htNw.jar\" permissions:420\r\n2019-12-18 20:44:46.477 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-protobuf-1.17.1-gkIppisCtEE4HD6U0fv4tA.jar\" permissions:420\r\n2019-12-18 20:44:46.501 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-netty-shaded-1.17.1-wHeHRt2qhBMCJwd7qhVwvw.jar\" permissions:420\r\n2019-12-18 20:44:46.515 GMT\r\n2019/12/18 20:44:46 Found: name:\"opencensus-contrib-grpc-util-0.17.0-iAypGz9jPIeblJhNhFTA7Q.jar\" permissions:420\r\n2019-12-18 20:44:46.546 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-okhttp-1.17.1-Ym33a1M8OEav6F7N12g25A.jar\" permissions:420\r\n2019-12-18 20:44:46.569 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-protobuf-nano-1.17.1-v769mz-qvJS69e3CdEG1cw.jar\" permissions:420\r\n2019-12-18 20:44:46.585 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-protobuf-lite-1.17.1-Bl91ZSPClD_iVyq27vy-Tw.jar\" permissions:420\r\n2019-12-18 20:44:46.601 GMT\r\n2019/12/18 20:44:46 Found: name:\"grpc-core-1.17.1-Qs78YGV9O_x9ZproeRWUWg.jar\" permissions:420\r\n2019-12-18 20:44:46.624 GMT\r\n2019/12/18 20:44:46 Found: name:\"opencensus-contrib-http-util-0.18.0-rhXNw-oEIYyewUcIytLblA.jar\" permissions:420\r\n2019-12-18 20:44:46.641 GMT\r\n2019/12/18 20:44:46 Found: name:\"proto-google-cloud-datastore-v1-0.44.0-elmwKzdsUHaqeLHDNgnRRA.jar\" permissions:420\r\n2019-12-18 20:44:46.658 GMT\r\n2019/12/18 20:44:46 Found: name:\"proto-google-cloud-bigquerystorage-v1beta1-0.83.0-AN9CMtFFGiHEo_KfhDFXKw.jar\" permissions:420\r\n2019-12-18 20:44:46.683 GMT\r\n2019/12/18 20:44:46 Found: name:\"proto-google-cloud-bigtable-v2-0.44.0-3UqoMk6S7yLqYAP51vS80A.jar\" permissions:420\r\n2019-12-18 20:44:46.699 GMT\r\n2019/12/18 20:44:46 Found: name:\"proto-google-cloud-pubsub-v1-1.43.0-H05P5I68_0NNGLjnpRu-Ow.jar\" permissions:420\r\n2019-12-18 20:44:46.716 GMT\r\n2019/12/18 20:44:46 Found: name:\"proto-google-cloud-spanner-admin-database-v1-1.6.0-oJs46XQ701jPN_K3eNCk6g.jar\" permissions:420\r\n2019-12-18 20:44:46.730 GMT\r\n2019/12/18 20:44:46 Found: name:\"proto-google-cloud-bigtable-admin-v2-0.38.0-giJ--Dr2jXieoiK-g0ip0Q.jar\" permissions:420\r\n2019-12-18 20:44:46.746 GMT\r\n2019/12/18 20:44:46 Found: name:\"proto-google-cloud-spanner-admin-instance-v1-1.6.0-k_koDbxapZBjr-nrHQW8Iw.jar\" permissions:420\r\n2019-12-18 20:44:46.770 GMT\r\n2019/12/18 20:44:46 Found: name:\"proto-google-iam-v1-0.12.0-KtsSGk0GwozxZp-QSDLgQQ.jar\" permissions:420\r\n2019-12-18 20:44:46.788 GMT\r\n2019/12/18 20:44:46 Found: name:\"proto-google-cloud-spanner-v1-1.6.0-ZxGDeokcBrWzu3ayFr2ydA.jar\" permissions:420\r\n2019-12-18 20:44:46.818 GMT\r\n2019/12/18 20:44:46 Found: name:\"api-common-1.8.1-g5ubgp_2pxctZAsz-8Lhsw.jar\" permissions:420\r\n2019-12-18 20:44:46.833 GMT\r\n2019/12/18 20:44:46 Found: name:\"protobuf-java-util-3.6.0-92eguGRlzdJ7ZLzQCUAmgQ.jar\" permissions:420\r\n2019-12-18 20:44:46.854 GMT\r\n2019/12/18 20:44:46 Found: name:\"guava-25.1-jre-2jg4hH0QmsQ18NPtSuHHlA.jar\" permissions:420\r\n2019-12-18 20:44:46.871 GMT\r\n2019/12/18 20:44:46 Found: name:\"google-extensions-0.3.1-6lp0wo6CspsO1VNozDOfXA.jar\" permissions:420\r\n2019-12-18 20:44:46.959 GMT\r\n2019/12/18 20:44:46 Found: name:\"flogger-system-backend-0.3.1-Z8I0E_LCTwJSgLdOjwU9TA.jar\" permissions:420\r\n2019-12-18 20:44:46.978 GMT\r\n2019/12/18 20:44:46 Found: name:\"flogger-0.3.1-CRV_dWA6DF7OAjvQud_tJg.jar\" permissions:420\r\n2019-12-18 20:44:46.998 GMT\r\n2019/12/18 20:44:46 Found: name:\"jsr305-3.0.2-3YOsy4mTY8MrB9ehsuTOQA.jar\" permissions:420\r\n2019-12-18 20:44:47.017 GMT\r\n2019/12/18 20:44:47 Found: name:\"jackson-databind-2.9.10-_0PXnGJLD31GVUL-5mSEdA.jar\" permissions:420\r\n2019-12-18 20:44:47.037 GMT\r\n2019/12/18 20:44:47 Found: name:\"jackson-dataformat-yaml-2.9.10-6-zFtnuWh0wIBoFR_YnQtQ.jar\" permissions:420\r\n2019-12-18 20:44:47.236 GMT\r\n2019/12/18 20:44:47 Found: name:\"jackson-core-2.9.10-1i2bHR2D3VU-Z4vI_Oj4CQ.jar\" permissions:420\r\n2019-12-18 20:44:47.291 GMT\r\n2019/12/18 20:44:47 Found: name:\"jackson-annotations-2.9.10-JsK297xwTMrcZMg5leD_fw.jar\" permissions:420\r\n2019-12-18 20:44:47.306 GMT\r\n2019/12/18 20:44:47 Found: name:\"avro-1.8.2-EDleWlceGh9hE0EfJ20v6g.jar\" permissions:420\r\n2019-12-18 20:44:47.322 GMT\r\n2019/12/18 20:44:47 Found: name:\"avro-1.8.2-tests-KC1ltAJzRHVcxtiYWfFpvg.jar\" permissions:420\r\n2019-12-18 20:44:47.336 GMT\r\n2019/12/18 20:44:47 Found: name:\"slf4j-jdk14-1.7.25-lEPyQGtDduRgOFY4C9MlVA.jar\" permissions:420\r\n2019-12-18 20:44:47.354 GMT\r\n2019/12/18 20:44:47 Found: name:\"metrics-core-3.1.2-uLLedSRzIqDAN0IPVwjlkg.jar\" permissions:420\r\n2019-12-18 20:44:47.383 GMT\r\n2019/12/18 20:44:47 Found: name:\"slf4j-api-1.7.25-yq_jdq-3CG3L7nn3gDlMow.jar\" permissions:420\r\n2019-12-18 20:44:47.412 GMT\r\n2019/12/18 20:44:47 Found: name:\"snappy-java-1.1.4-SFNwbMuGq13aaoKVzeS1Tw.jar\" permissions:420\r\n2019-12-18 20:44:47.427 GMT\r\n2019/12/18 20:44:47 Found: name:\"joda-time-2.10.3-x9d0qCHsaxqSPYJWPWV-Kw.jar\" permissions:420\r\n2019-12-18 20:44:47.461 GMT\r\n2019/12/18 20:44:47 Found: name:\"xz-1.8-X5ghJ-DehbeFxLKrrSGqLg.jar\" permissions:420\r\n2019-12-18 20:44:47.477 GMT\r\n2019/12/18 20:44:47 Found: name:\"powermock-api-mockito2-2.0.2-shnnF2D5x8By8gIRGXFhZw.jar\" permissions:420\r\n2019-12-18 20:44:47.492 GMT\r\n2019/12/18 20:44:47 Found: name:\"mockito-core-3.0.0-7tOnWjwKUa3vSvwalF3i1w.jar\" permissions:420\r\n2019-12-18 20:44:47.513 GMT\r\n2019/12/18 20:44:47 Found: name:\"kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar\" permissions:420\r\n2019-12-18 20:44:47.528 GMT\r\n2019/12/18 20:44:47 Found: name:\"kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar\" permissions:420\r\n2019-12-18 20:44:47.550 GMT\r\n2019/12/18 20:44:47 Found: name:\"kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar\" permissions:420\r\n2019-12-18 20:44:47.577 GMT\r\n2019/12/18 20:44:47 Found: name:\"kryo-2.21-olkSUBNMYGe-WXkdlwcytQ.jar\" permissions:420\r\n2019-12-18 20:44:47.592 GMT\r\n2019/12/18 20:44:47 Found: name:\"zstd-jni-1.3.8-3-bSgd5MmkqGHlwXyzvze_1w.jar\" permissions:420\r\n2019-12-18 20:44:47.609 GMT\r\n2019/12/18 20:44:47 Found: name:\"google-auth-library-credentials-0.12.0-oX-2Z1dxImK9WyCyPRaKfg.jar\" permissions:420\r\n2019-12-18 20:44:47.635 GMT\r\n2019/12/18 20:44:47 Found: name:\"beam-vendor-grpc-1_21_0-0.1-wUUJvysMX0DMT0c1hpzFYQ.jar\" permissions:420\r\n2019-12-18 20:44:47.651 GMT\r\n2019/12/18 20:44:47 Found: name:\"google-cloud-dataflow-java-proto-library-all-0.5.160304-dqDYFED9AirNnNfC_1P6Ag.jar\" permissions:420\r\n2019-12-18 20:44:47.665 GMT\r\n2019/12/18 20:44:47 Found: name:\"hamcrest-2.1-oTm8x8sMLv9-n5czpY1b3Q.jar\" permissions:420\r\n2019-12-18 20:44:47.685 GMT\r\n2019/12/18 20:44:47 Found: name:\"error_prone_annotations-2.0.15-npnuyJrjs2dF75GdvUBIvQ.jar\" permissions:420\r\n2019-12-18 20:44:47.702 GMT\r\n2019/12/18 20:44:47 Found: name:\"jackson-mapper-asl-1.9.13-F1D5wzk1L8S3KNYbVxcWEw.jar\" permissions:420\r\n2019-12-18 20:44:47.742 GMT\r\n2019/12/18 20:44:47 Found: name:\"jackson-core-asl-1.9.13-MZxJpDBOP6n-PNjc_ACdNw.jar\" permissions:420\r\n2019-12-18 20:44:47.761 GMT\r\n2019/12/18 20:44:47 Found: name:\"paranamer-2.7-VweilzYySf_-OOgYnNb5yw.jar\" permissions:420\r\n2019-12-18 20:44:47.780 GMT\r\n2019/12/18 20:44:47 Found: name:\"commons-compress-1.19-_ol7ztQ0aEULeFtmwc_0VQ.jar\" permissions:420\r\n2019-12-18 20:44:47.794 GMT\r\n2019/12/18 20:44:47 Found: name:\"snakeyaml-1.23-ZOyL0mttUDSofsscjODv3A.jar\" permissions:420\r\n2019-12-18 20:44:47.812 GMT\r\n2019/12/18 20:44:47 Found: name:\"checker-qual-2.0.0-lP4a92wQAG-8W5iBgLcb8A.jar\" permissions:420\r\n2019-12-18 20:44:47.827 GMT\r\n2019/12/18 20:44:47 Found: name:\"j2objc-annotations-1.1-Sa4yBLsLubKsdwYmQfSm1w.jar\" permissions:420\r\n2019-12-18 20:44:47.842 GMT\r\n2019/12/18 20:44:47 Found: name:\"powermock-api-support-2.0.2-cmvt0hCp-TF3N42XWM5ZWQ.jar\" permissions:420\r\n2019-12-18 20:44:47.943 GMT\r\n2019/12/18 20:44:47 Found: name:\"powermock-core-2.0.2-N-uIY2bG_VrLlX25F5TknA.jar\" permissions:420\r\n2019-12-18 20:44:47.962 GMT\r\n2019/12/18 20:44:47 Found: name:\"powermock-reflect-2.0.2-WpXXbgjnSFeT4du1aQDZ2w.jar\" permissions:420\r\n2019-12-18 20:44:47.979 GMT\r\n2019/12/18 20:44:47 Found: name:\"byte-buddy-1.9.10-QusrhKJgPcRx1AnckpJw7Q.jar\" permissions:420\r\n2019-12-18 20:44:47.998 GMT\r\n2019/12/18 20:44:47 Found: name:\"byte-buddy-agent-1.9.10-BCkqVrRfNOFZg1RAm6NEQw.jar\" permissions:420\r\n2019-12-18 20:44:48.013 GMT\r\n2019/12/18 20:44:48 Found: name:\"objenesis-3.0.1-J5YZBgYnQmdzV6gwCA2MQQ.jar\" permissions:420\r\n2019-12-18 20:44:48.031 GMT\r\n2019/12/18 20:44:48 Found: name:\"reflectasm-1.07-shaded-IELCIoQPsh49E873w5ZYMQ.jar\" permissions:420\r\n2019-12-18 20:44:48.074 GMT\r\n2019/12/18 20:44:48 Found: name:\"minlog-1.2-98-99jtn3wu_pMfLJgiFvA.jar\" permissions:420\r\n2019-12-18 20:44:48.097 GMT\r\n2019/12/18 20:44:48 Found: name:\"javaruntype-1.3-rOez6Sa6vLqndviPQ7_noQ.jar\" permissions:420\r\n2019-12-18 20:44:48.128 GMT\r\n2019/12/18 20:44:48 Found: name:\"ognl-3.1.12-aq6HFAi1aHiNX6qcImOsmw.jar\" permissions:420\r\n2019-12-18 20:44:48.142 GMT\r\n2019/12/18 20:44:48 Found: name:\"generics-resolver-2.0.1-VrpA4CuIscdXm0wF6oMTaQ.jar\" permissions:420\r\n2019-12-18 20:44:48.183 GMT\r\n2019/12/18 20:44:48 Found: name:\"netty-codec-http2-4.1.30.Final-xWX9W6lm-ZlAY32qJIyUKw.jar\" permissions:420\r\n2019-12-18 20:44:48.198 GMT\r\n2019/12/18 20:44:48 Found: name:\"netty-handler-4.1.30.Final-zZAD5jGo-RpbSIfZ0lRurQ.jar\" permissions:420\r\n2019-12-18 20:44:48.214 GMT\r\n2019/12/18 20:44:48 Found: name:\"netty-tcnative-boringssl-static-2.0.17.Final-YPiL4Pp2vC0bifcXJZEEQQ.jar\" permissions:420\r\n2019-12-18 20:44:48.234 GMT\r\n2019/12/18 20:44:48 Found: name:\"proto-google-common-protos-1.17.0-ZA4dWNbtwl5-FIMyB6cNug.jar\" permissions:420\r\n2019-12-18 20:44:48.252 GMT\r\n2019/12/18 20:44:48 Found: name:\"protobuf-java-3.6.0-NDZs0P9eJxX9OraYDJl91w.jar\" permissions:420\r\n2019-12-18 20:44:48.310 GMT\r\n2019/12/18 20:44:48 Found: name:\"classgraph-4.8.56-OJxxWYNOeeT7dCzKVFyoag.jar\" permissions:420\r\n2019-12-18 20:44:48.330 GMT\r\n2019/12/18 20:44:48 Found: name:\"auto-value-annotations-1.6.3-xXovAdCH5p9GU93RsQLguA.jar\" permissions:420\r\n2019-12-18 20:44:48.353 GMT\r\n2019/12/18 20:44:48 Found: name:\"opencensus-contrib-grpc-metrics-0.17.0-ezV91dXAiuVASofLaT_ZTg.jar\" permissions:420\r\n2019-12-18 20:44:48.364 GMT\r\n2019/12/18 20:44:48 Found: name:\"opencensus-api-0.18.0-BZQH5cDopLnYFcO8ZEinaQ.jar\" permissions:420\r\n2019-12-18 20:44:48.385 GMT\r\n2019/12/18 20:44:48 Found: name:\"javax.annotation-api-1.3.2-KrGXPu__qiruxH1QueQLnQ.jar\" permissions:420\r\n2019-12-18 20:44:48.401 GMT\r\n2019/12/18 20:44:48 Found: name:\"animal-sniffer-annotations-1.17-fKEIt5DParXb9UIsx58NiQ.jar\" permissions:420\r\n2019-12-18 20:44:48.424 GMT\r\n2019/12/18 20:44:48 Found: name:\"asm-4.0-Mi2PiMURGvYS34OMAZHNfg.jar\" permissions:420\r\n2019-12-18 20:44:48.441 GMT\r\n2019/12/18 20:44:48 Found: name:\"antlr-runtime-3.1.2-dpdEvr2kPK4dWHj9fUOv1A.jar\" permissions:420\r\n2019-12-18 20:44:48.464 GMT\r\n2019/12/18 20:44:48 Found: name:\"javassist-3.24.0-GA-I_0i0NCJ3frFZ6Oc4MsakQ.jar\" permissions:420\r\n2019-12-18 20:44:48.479 GMT\r\n2019/12/18 20:44:48 Found: name:\"threetenbp-1.3.3-bEXFSgaAYiXSdUtR-98IjQ.jar\" permissions:420\r\n2019-12-18 20:44:48.510 GMT\r\n2019/12/18 20:44:48 Found: name:\"httpclient-4.5.5-l-flsTVHa30lpasx4epJIg.jar\" permissions:420\r\n2019-12-18 20:44:48.525 GMT\r\n2019/12/18 20:44:48 Found: name:\"commons-logging-1.2-BAtLTY6siG9rSio70vMbAA.jar\" permissions:420\r\n2019-12-18 20:44:48.570 GMT\r\n2019/12/18 20:44:48 Found: name:\"grpc-context-1.17.1-fI9fpai7msIsYMyyTxPe7A.jar\" permissions:420\r\n2019-12-18 20:44:48.589 GMT\r\n2019/12/18 20:44:48 Found: name:\"gson-2.7-UTSiNQ9YiQ_7nbC0AEcZXQ.jar\" permissions:420\r\n2019-12-18 20:44:48.603 GMT\r\n2019/12/18 20:44:48 Found: name:\"netty-handler-proxy-4.1.30.Final-lCIjvadnkbjk1NVm_E895w.jar\" permissions:420\r\n2019-12-18 20:44:48.623 GMT\r\n2019/12/18 20:44:48 Found: name:\"netty-codec-http-4.1.30.Final-AwxItf4Cqo2SMwSuXhxusg.jar\" permissions:420\r\n2019-12-18 20:44:48.673 GMT\r\n2019/12/18 20:44:48 Found: name:\"netty-codec-socks-4.1.30.Final-1WHnwq75shvN3dCiM86_2g.jar\" permissions:420\r\n2019-12-18 20:44:48.687 GMT\r\n2019/12/18 20:44:48 Found: name:\"netty-codec-4.1.30.Final-x4vcMCRY_egm-VkASqrGgQ.jar\" permissions:420\r\n2019-12-18 20:44:48.708 GMT\r\n2019/12/18 20:44:48 Found: name:\"netty-transport-4.1.30.Final-x3QJT81qSgvsv4fipL84Ug.jar\" permissions:420\r\n2019-12-18 20:44:48.732 GMT\r\n2019/12/18 20:44:48 Found: name:\"netty-buffer-4.1.30.Final-mYBmhCN5yGpDEs5G7F6ejg.jar\" permissions:420\r\n2019-12-18 20:44:48.759 GMT\r\n2019/12/18 20:44:48 Found: name:\"args4j-2.33-Cm1RX3axXSnjzVKd6TGXOQ.jar\" permissions:420\r\n2019-12-18 20:44:48.772 GMT\r\n2019/12/18 20:44:48 Found: name:\"postgresql-42.2.2-rq7ipFbyabSdeBJdb0kvXQ.jar\" permissions:420\r\n2019-12-18 20:44:48.790 GMT\r\n2019/12/18 20:44:48 Found: name:\"commons-lang3-3.6-XRj2i1Ei_TmMEY31OrTPVQ.jar\" permissions:420\r\n```": "Y",
  "Thanks for the contribution.\r\n\r\nTyson, could you create a JIRA account as per the [contribution guide](https://beam.apache.org/contribute/#share-your-intent) for sharing your intent. Then I can add you as a contributor to the project which would allow you to assign JIRAs to yourself (specifically BEAM-9014 which I created for this change). Note that all PRs should have an accompanying JIRA associated with them.": "Y",
  "This PR currently has merge conflicts, but #1515 is next in line, so you may want to wait till it is merged before you fix these conflicts.": "N",
  "@NuxRo if you're using network log servers, the UDP packets get sent right away.\n": "N",
  "@XD-DENG can you address my comments or close this PR?": "N",
  "Any updates on the status of this merge? This update would be very helpful\n": "N",
  "> Will not because message is null. There there is silent error eating.\n\n\n\nYou're right. I should have looked taken a closer look.\n\n\n\n> Could we just check that deserialized object is not null and throw exception.\n\n\n\nYes, I think that might be the best option as we expect the message to never be null. So, if for some reason is actually null, then we can throw an exception which will result in a call to `CloseConnectionBecauseOfFailureAsync` which closes the connection completely and notifies all existing response handlers about the exception.\n\n\n\n@Haapsaari-Juha: I don't know if you want to incorporate these changes into your PR as you were the first to contribute a fix for this issue or if @dzmitry-lahoda should update his PR accordingly.\n\n\n\n> property based testing for any other array passed\n\n\n\nCould you explain a bit further what you mean by this? Do you want to add property based testing for the internal classes of the driver?": "N",
  "I re-ran the travis build and the error is gone. probably a fluke of travis or something. I guess this is good now. do you happen to know if there is a test for the alternative scenario somewhere (i.e. where a set becomes a list)?": "N",
  "<!--\nMeta data\nHash:8257d0ef74983dad3b34aa4fbcbabb9bae8f58b7 Status:SUCCESS URL:https://travis-ci.com/flink-ci/flink/builds/144745849 TriggerType:PUSH TriggerID:8257d0ef74983dad3b34aa4fbcbabb9bae8f58b7\nHash:8257d0ef74983dad3b34aa4fbcbabb9bae8f58b7 Status:SUCCESS URL:https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_build/results?buildId=4401 TriggerType:PUSH TriggerID:8257d0ef74983dad3b34aa4fbcbabb9bae8f58b7\nHash:59e2c089f08f2d9ac462b0cc25d9afaf533cca42 Status:SUCCESS URL:https://travis-ci.com/flink-ci/flink/builds/144759987 TriggerType:PUSH TriggerID:59e2c089f08f2d9ac462b0cc25d9afaf533cca42\nHash:59e2c089f08f2d9ac462b0cc25d9afaf533cca42 Status:SUCCESS URL:https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_build/results?buildId=4410 TriggerType:PUSH TriggerID:59e2c089f08f2d9ac462b0cc25d9afaf533cca42\n-->\n## CI report:\n\n* 8257d0ef74983dad3b34aa4fbcbabb9bae8f58b7 Travis: [SUCCESS](https://travis-ci.com/flink-ci/flink/builds/144745849) Azure: [SUCCESS](https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_build/results?buildId=4401) \n* 59e2c089f08f2d9ac462b0cc25d9afaf533cca42 Travis: [SUCCESS](https://travis-ci.com/flink-ci/flink/builds/144759987) Azure: [SUCCESS](https://dev.azure.com/rmetzger/5bd3ef0a-4359-41af-abca-811b04098d2e/_build/results?buildId=4410) \n\n<details>\n<summary>Bot commands</summary>\nThe @flinkbot bot supports the following commands:\n\n- `@flinkbot run travis` re-run the last Travis build\n- `@flinkbot run azure` re-run the last Azure build\n</details>": "N",
  "@Daniel-Dos @cesarhernandezgt this PR contains changes for 2 different dependencies, (`krazo` from `1.0.0-Beta1` to `1.1.0-M1` and `deltaspike` from `1.9.1` to `1.9.3`), for cases like this, the JIRA issue and the title of the PR should be updated to reflect the actual changes more accurately. \n\n@Daniel-Dos just from my own curiosity, why don't you include a final release like [Krazo 1.0.0](https://projects.eclipse.org/projects/ee4j.krazo/releases/1.1.0) or wait for [Krazo 1.1.0](https://projects.eclipse.org/projects/ee4j.krazo/releases/1.1.0) that is scheduled for 03/15/2020 (in less than 2 weeks), instead than using a milestone release?\n\nEven better would be to keep the changes on two use different PRs, as you did originally #626 and #625 .\n\n": "Y",
  "I'll take a deep look and describe what this script actually does.\n": "N",
  "Thanks for the information, @Shawn-Hx and @sjwiesman.\n\nSound like both of you were checking against legacy Jekyll formats. I guess it's fine that we can't automate this process, since the migration is done and ideally we should not introduce any new broken links/images.": "Y",
  "Any update for this PR, @gallenvara?\n": "N",
  "@ham1 , do you think the PR is good to go?\n": "N",
  "`StateStore` implementations could be designed like:\n\n```\npublic class KafkaStateStore implements StateStore {\npublic KafkaStateStore(HostPort kafkaBroker, String consumerId) { // consumerId could be topic? I'm don't know enough about the compaction feature\n...\n}\n...\n}\n\npublic class ZkStateStore implements StateStore {\npublic ZkStateStore(HostPort zkConnect, String path) {\n...\n}\n}\n\n// future implementations\npublic class JdbcStateStore implements StateStore {\npublic JdbcStateStore(String url, String stateTable, String user, String password) {\n...\n}\n}\n\npublic class MemoryStateStore implements StateStore {\npublic MemoryStateStore() {\n...\n}\n}\n```\n\nWhat I'm trying to illustrate are two factors:\n1) go away from `StormConf` and `SpoutConfig` because `StateStore` does not need all properties of either objects.\n2) the concrete implements of `StateStore` do not need to have the same constructor because your goal is not to have a factory and make it completely config driven. You want to make it so the topology developer can decide what `StateStore` to use.\n\nWith the above, it would be pretty easy for someone to implement `MemoryStateStore` and `JdbcStateStore`.\n": "Y",
  "I recommend making your abstraction at the state store, so you would have:\n\n```\npublic interface StateStore {\npublic void write(Partition p, long offset);\npublic long read(Partition p);\n}\n\npublic class ZkStateStore implements StateStore {\n...\n}\n\npublic class KafkaStateStore implements StateStore {\n...\n}\n\n/**\nSome topologies read from LATEST after a restart, so only memory state is needed.\n*/\npublic class MemoryStateStore implements StateStore {\n...\n}\n```\n\nYou would not need different PartitionStateManager for different stores. You would just have:\n\n```\npublic class PartitionStateManager {\npublic PartitionStateManager (..., StateStore store) { ... }\npublic void writeState(...) {\nstore.write(...);\n}\n}\n```\n": "Y",
  "Cool. Then it seems like we have a deal. It remains to test if the actual log entry survives the sudden reboot.\n": "N",
  "I created PRs #444 and #445, I hope we can get these in for 2.4.7.\n": "N",
  "I reviewed the PR and it fixes the issue.": "N",
  "Just summarize/quote the link contents:\n- **doctype**: that declaration was used in HTML5 to distinguish between a standards-compliant parsing mode and a so-called quirks parsing mode.\n- **&lt;meta http-equiv=\"Content-Language\">**: Due to long-standing confusions and inconsistent implementations of this element, the HTML5 specification made this non-conforming in HTML, so you should no longer use it.\n- **&lt;th scope=\"row|col\">**: tell screenreaders exactly what cells the header is a header for": "Y",
  "OK. I have a plan. I have checked out the source branch for the PR locally and am using a visual diff tool to compare this PR with Tomcat. The tool provides tools to easily review the changes in the PR and apply them selectively to Tomcat. I'll apply changes manually. If you can rebase this PR periodically, that will enable this PR to reflect the current TODO list in terms of what needs to be reviewed (and potentially merged). There will be some changes that can't be made in Tomcat. I can direct you to the correct upstream project for those. If you can remove those changes from this PR and submit PRs for the appropriate project(s) that would be very helpful.": "Y",
  "You can reach me (and other JMeter users/developers) on the [JMeter mailing list](https://jmeter.apache.org/mail2.html#JMeterDev)\n\nI hacked together an updated version of the Mongo Sampler at https://github.com/FSchumacher/jmeter/tree/pr-390-mongo, but I am not sure, that it is a good idea, as it will break all old test plans.\n\nAnd note, that I don't use MongoDB, so I am not sure, if it is correct at all.": "Y",
  "I've added some comments - we have to upgrade to latest Wicket. I can help here if you want.\n\nI don't see the change in BaseWicketTester in this PR. Please add it so we can review it too.\n\nThanks!\n": "Y",
  "A few minor nits:\n\nWhen providing javadoc on variables/methods, be sure to use the following syntax:\n\n`/**`\n`* Comment text`\n`**/`\n`private String variableName;`\n\nInstead of the following:\n\n`// offset state information storage. validate options are storm and kafka`\n`private String variableName;`\n\nThe first variation will provide fly-over-help within IDE and generated Javadoc files.\n\nSecondly:\nLogger instance variables should be private.\n\nThirdly:\nSuggest instance variables on PartitionManager, StaticCoordinator ZkDataStore be made private.\n": "Y",
  "You submitted three items, and indeed I am not convinced by this one. Looking back to the old BZs, there was also there the conclusion that the current code is the best solution for the general Tomcat use case.\n\nHowever, there is the \"but I only want UTF-8\" argument, in which case the solution is to hardcode UTF-8 and default to Charset.forName for the rest. I think system property configuration is supposed to be removed, but it would be a good solution for this pluggability (it's global, no one really actually needs it, etc).": "Y",
  "Since the capacity limit refers both to input and output queues, we should probably rename `taskmanager.net.max-out-queue-length` to `taskmanager.net.max-queue-length`.": "Y",
  "I haven't found the time to dig deeply into the issue, but it would be good to understand where exactly things turn into unexpected directions.": "Y",
  "I am generally not convinced that overriding poolsize from the client side is a good idea.\nIt is very easy to overload the cluster by specifying a too large poolsize.\nIMO it's better to leave this knob in the cluster administrator's hand.": "Y",
  "LGTM\n": "N",
  "Merged and backported to 4.2.x, thanks!": "N",
  "Many thanks, Felipe!\n": "N",
  "@osma - I'll find some time to review this but \n\nTo your points:\n1. I dont think that change is a problem and \\@Deprecated isn't needed because (a) Jena3 allows us to be a bit more flexibility to do things right and (b) the major use is via SPARQL and that is not affected. I don't recall anyone asking about detailed direct use of the module.\n1. IIRC solr works differently and puts in a \"score\" field in the results.\n1. Yes - we already have enough old practices! I'll look at the PR for these. \n\nI'd like to proceed getting functionality in and worry about clearing up very soon after. It exposes the changes early for those interested.\n": "Y",
  "We've voted to keep 1.9.x as an LTS branch for Java5 compatible versions of Ant and move master to Java8 as baseline. \n": "N",
  "This was (accidentally?) closed as a casualty of my request for infra to close the 2 previous stale pull requests that I couldnt (via https://issues.apache.org/jira/browse/INFRA-7253).\n\nStill interested :)\n": "N",
  "Thanks! :+1:": "N",
  "Besides the 31ms startup time, it's still worse in every way compared to the current code.": "N",
  "@analytically did you happen to see my previous comment about rebasing/fixing conflicts on this PR so that we can begin code review/testing on our end? is that something you have some time to come back to soon?\n": "N",
  "I also backported to the karaf 4.1.x branch now": "N",
  "Hi @dianfu,\r\n\r\nsorry if my `-1` was too direct. It should neither offend you nor the contributor. It is simply an expression that the current status of the PR cannot be merged due to the issues that I mentioned.\r\n\r\nFLIP-59 is still under discussion because it takes longer than expected to update the ConfigOption infrastructure. It is still targeted for 1.10. As you might have seen on the mailing list, we still try to reach consensus on FLIP-54 and split it into multiple parts such as FLIP-77.\r\n\r\nThe solution in this PR makes it difficult to validate the configuration in the future as it encourages users to add arbitrary keys to the table config. We should clearly separate `global job configuration` (which are arbtirary user-defined properties available to functions) and `configuration` (which are Flink-defined properties available to Flink).\r\n\r\nFor example, we could have a separate `Configuration` member variable in TableConfig for user-defined properties for clear separation. What do you think?": "Y",
  "I did another review round and see that we still have user API breaking changes there. \r\nI'm wondering if we can keep the current sync write API and just add new async write API to this? If sync API is really slow and useless then we can deprecate it and remove in the future but I'm against breaking user API changes without deprecation period (usually it's 3 releases for Beam). \r\n@Akshay-Iyangar wdyt?": "Y",
  "While i agree and prefer returning Optional<?> or an empty List, in this case this would break backwards compatiblity. And therefore this is a no go on the public API.": "Y",
  "@harshach The source of the file is referenced here:\nhttps://github.com/apache/storm/pull/1468/files#diff-da45fe3972445a9f82ef768808dd8853R20\n\nI'd like to get clearance that what this script does or enables is okay before proceeding. \n": "Y",
  "It seems your branch is too far from current master. I have problems to merge it locally.\nCould you please rebase it and probably squash your commits into one?\nThanks!\n": "Y",
  "Since Aljoscha LGTMed I will merge.\n\n\n\nThe jenkins failure is expected because of the archetype mess pre-stable API.": "N",
  "@mxm thanks for working in this, it is a good improvement.\r\n\r\nHowever, regarding the merge if this PR, I would prefer more time is given to allow for comments and consideration of comments. We should wait for an approval, not just assume everything was reviewed and addressed.\r\n\r\nThere is no reason to rush this type of change.\r\n": "Y",
  "I hope before Xmas.": "N",
  "I'm starting to lean toward requiring this new feature to require a configuration option to enable it, and have it default to `false`. My justification is that it represents a breaking behavioral change that is *just slightly different* than previous behavior as to be unnoticeable until it starts behaving completely unexpectedly.\n\nCan you please add a configuration option for `<Host>` to enable or disable this? Simply swap-out the `compareTo` implementation depending upon the value of that setting.": "Y",
  "Thanks for submitting this....Travis seems unhappy on the gremlin-python version of the build....could you please have a look at that? Also, this looks like it needs a rebase now that we have the tp33 branch re-opened for development. That should get CHANGELOG setup for new entries. Please don't include the JIRA issue there - just a bullet point with your representation of what changed will suffice.": "Y",
  "Would the fsync suffice if we're using network log servers? Ideally the data should be registered remotely, then invoke the sysrq operation.\n": "N",
  "> - Use only $(..) instead of both $(...) and `...`\n\nWhat do you think about:\n- Use only ${VAR} instead of both ${VAR} and $VAR\n": "N",
  "Thanks @Shawn-Hx, I'll take a look.\n\n\n\nA quick question, how did you locate these broken images? Did you just manually go through all the pages, or is there an automatic approach? I'm asking because, if there's an automatic approach, we could introduce a ci-check against broken images.": "Y",
  "I have applied the optimisation commit (thanks) but not the ordering changes.\n\nThe patch changes the order for undeploy without changing the order for version mapping. This will lead to unexpected behaviour.\n\nThe current simple `String` based ordering was selected as a trade off between performance and usability. The type of ordering proposed in this patch was considered too expensive to incur on every request when multiple versions are deployed. To consider such a change, we'd need to see evidence of the performance impact (including GC) on the mapping process of switching to this ordering mechanism. Be aware that mapping code is highly optimised for `String`.\n\nReviewing the proposed patch I see several things that would need to be fixed in additional to the more architectural issue described above:\n- imports from sun.* are not allowed\n- the code is Java 8 specific - any fix needs to be available to back-ported to 7.0.x will has to run on Java 6\n- the patch does not compile\n- `Pattern` instances should be pre-compiled\n\nFinally, the patch only considers purely numeric versions. Many projects include a test string (e.g. RELEASE) in the version number. It would be nice to handle these as well.": "Y",
  ":confetti_ball: **+1 overall**\n\n\n\n\n\n\n\n\n\n\n\n\n\n| Vote | Subsystem | Runtime | Logfile | Comment |\n\n|:----:|----------:|--------:|:--------:|:-------:|\n\n| +0 :ok: | reexec | 26m 3s | | Docker mode activated. |\n\n|||| _ Prechecks _ |\n\n| +1 :green_heart: | dupname | 0m 0s | | No case conflicting files found. |\n\n| +1 :green_heart: | @author | 0m 0s | | The patch does not contain any @author tags. |\n\n| +1 :green_heart: | | 0m 0s | [test4tests](test4tests) | The patch appears to include 1 new or modified test files. |\n\n|||| _ trunk Compile Tests _ |\n\n| +1 :green_heart: | mvninstall | 34m 10s | | trunk passed |\n\n| +1 :green_heart: | shadedclient | 49m 29s | | branch has no errors when building and testing our client artifacts. |\n\n|||| _ Patch Compile Tests _ |\n\n| +1 :green_heart: | mvninstall | 0m 37s | | the patch passed |\n\n| +1 :green_heart: | whitespace | 0m 0s | | The patch has no whitespace issues. |\n\n| +1 :green_heart: | xml | 0m 2s | | The patch has no ill-formed XML file. |\n\n| +1 :green_heart: | shadedclient | 16m 56s | | patch has no errors when building and testing our client artifacts. |\n\n|||| _ Other Tests _ |\n\n| +1 :green_heart: | unit | 0m 32s | | hadoop-aws in the patch passed. |\n\n| +1 :green_heart: | asflicense | 0m 32s | | The patch does not generate ASF License warnings. |\n\n| | | 95m 56s | | |\n\n\n\n\n\n| Subsystem | Report/Notes |\n\n|----------:|:-------------|\n\n| Docker | ClientAPI=1.40 ServerAPI=1.40 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2462/1/artifact/out/Dockerfile |\n\n| GITHUB PR | https://github.com/apache/hadoop/pull/2462 |\n\n| Optional Tests | dupname asflicense unit xml |\n\n| uname | Linux a7d152c39402 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |\n\n| Build tool | maven |\n\n| Personality | dev-support/bin/hadoop.sh |\n\n| git revision | trunk / fc961b63d14 |\n\n| Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2462/1/testReport/ |\n\n| Max. process+thread count | 535 (vs. ulimit of 5500) |\n\n| modules | C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws |\n\n| Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2462/1/console |\n\n| versions | git=2.17.1 maven=3.6.0 |\n\n| Powered by | Apache Yetus 0.13.0-SNAPSHOT https://yetus.apache.org |\n\n\n\n\n\nThis message was automatically generated.": "N",
  "@NuxRo okay, can you grant me push access on your fork: https://github.com/NuxRo/cloudstack/tree/patch-4 and I can help fix it for you?\n": "N",
  "LGTM. Is there a JIRA associated with this?\n\nVOTE: +1\n": "N",
  "how will this change affect the format of spout's offset message in zookeeper?\n": "N",
  "@alexvanboxel I'm still not 100% sure that I agree that options are not schemas, and I don't love making SchemaVerification public. However I think it will be easier to iterate on those issues in isolation once the basic support is in, so I'm approving for now.\r\n\r\nLGTM": "Y",
  "IMO for the branches where Java 8 is minimum we should switch to Java 8 DateTime APIs. \njava.time.format.DateTimeFormatter is thread-safe.": "Y",
  "Thanks for contributing to Flink! ☕️ \r\n\r\nI don't think this will work, however, since the setter for case classes doesn't actually modify the case class and Flink only has a reference to the unmodified case class. When using the Scala API, the type analysis stack should correctly analyze case classes and use the `CaseClassSerializer` for them. (The `TypeExtractor` is only used for the Java API)": "Y",
  "I don't see use of `SourceTestUtils` to test split and estimated size. You can take a look on what I did in ElasticsearchIO (https://github.com/jbonofre/incubator-beam/blob/BEAM-425-ELASTICSEARCHIO/sdks/java/io/elasticsearch/src/test/java/org/apache/beam/sdk/io/elasticsearch/ElasticsearchIOTest.java) as example.\n": "Y",
  "Please don't change something like this in Tomcat 9.0 or older. Those releases are far too stable to go around changing the date-related code.": "Y",
  "It makes sense. Less than Posix, the purpose is that Karaf scripts work on most Unix system as possible (now they work on Linux, MacOS, Solaris, AIX, *BSD). Let me review your proposal and see how it works on different VM.\nThanks.\n": "Y",
  "Something along those lines indeed. So admins can find out WHY the system just went away. Maybe write the log, sleep for 5 seconds and then reboot. Otherwise the logline might not be flushed to the local disk.\n": "Y",
  "LGTM, btw.\n": "N",
  "The changes are only limited to UI, so there is no need to run smoke/integration tests.\nCursory look at the changes looks fine.\n@swill I think these should be merged unless some language expert want to comment.": "Y",
  "Hey @rzo1 \nWe haven't heard from Daniel.\n\nSo if you are willing to help, I suggest you create a new PR, cherry pick what you need from this one and move on.\n\nDoes it work for you?": "Y",
  "We've got pretty strict API backwards compatibilty rules, so we cannot change the visibility of any elements that have been part of a release already": "Y",
  "These do not really meet the definition of a release blocker (not a regression from previous release) but given the amount of testing that was done I think they are safe to merge.": "Y",
  "What are the plans for this now that #219 is merged? There should be much less of an issue, but it would still be nice to be able to eagerly evict classes from the cache when they are not used anymore instead of waiting for the GC to kick in.\n": "N",
  "+1 from me, XWPF is officially still in \"scratchpad\", please add an entry in status.xml which flags this as import change to have it properly documented in the changelog.": "Y",
  "Thanks for reviewing this. I've closed here as the PR has been superseded by https://github.com/apache/jackrabbit/pull/88 , which includes the new '--insecure' optional parameter.": "N",
  "It looks like [beam_PostCommit_Java_PortabilityApi](https://builds.apache.org/job/beam_PostCommit_Java_PortabilityApi/) has been timing out at 4 hours since #10268 was merged. First failure is here: https://builds.apache.org/job/beam_PostCommit_Java_PortabilityApi/3625/\r\n\r\nThe run before that took just 50 mins. Will the changes in this PR address that?\r\n": "Y",
  "Hi Felix,\nMy opinion is that if you did the upgrade it’s ok to break old plan provided we clearly state we did this.\n\nIt is fair that we upgrade now a very old version of MongoDB API and drop deprecated code.\n\nI think we had some complaints about this very old driver version, missing new auth methods both on mailing list and surely on stackoverflow.\n\nSo it will be a good thing to upgrade. \nRegards": "Y",
  "We have a policy of deprecating code before removing it. I don't think this change is a good idea, even if the code is not really used.": "Y",
  "Hi! Apologies for the late review. I've been really busy and had no cycles to look at this PR again.\n\nThe fix looks good. So the summary is that we need to provide a singleton SSLContext, but also a singleton SSLContext factory. The current approach, however, just addresses this for the `untrusted` context. Would the issue also manifest when a proper, trusted SSLContext is provided? If so, we should have feature parity there, and allow also to set the custom SSLSocketFactory supplier. One option could be to have the Lazy supplier you created implement both suppliers, and allow users to easily use it to inject both consistently where needed?\n\n": "Y",
  "Looks OK to me, waiting for a dead NFS connection is a PITA.\n": "N",
  "Thanks. Two other notes:\n\n1. Do you think you can add a unit test in `source_test.go` that would check your functionality?\n\n2. Please do not squash review changes. It makes it hard for a reviewer to follow up with what has changed. Instead, create a commit like \"fix: a short summary\". Those commits are squashed after review is complete.": "Y",
  "@centic9 , answering the \"when\" question would be great. However I realize that it might be problematic and in any case it would be only tentative. Yet, if you can say in which **version** of POI it will be present for sure it would help a lot. For example, any chances it can make it into 7.10 ? Many thanks.": "Y",
  "i have no clue why docker is failing on the gremlin-python portion of the build. works fine for me locally - it's just doing `docker/build.sh` with no flags. not sure if anyone else can get it to fail locally...": "N",
  "@ADIR01 / @SEMION1956 FYI.": "N",
  "Nooooo. Not an system property ;).\n\nSeriously, I think there is a way that we can improve start-up time without impacting the vast majority of users. Phil's patch is heading in the general direction I was thinking. As ever, there are likely to be trade-offs involved. I hope to be able to have some hard numbers on which to base that discussion soon(ish).": "Y",
  "If there is a lookup with Charset.forName, then would the cache be used at all ? I don't understand this sort of Windows style \"startup time\" optimization, where you hide stuff behind making running actual application code slower. This is often a bad design, so I don't see what this change brings.\n-0": "Y",
  "> \n> \n> I enhanced this PR to be as much backward compatible as possible @reta.\n> I introduced new _org.apache.cxf.jaxws.handler.jakartaee_ package as you proposed.\n> I left _org.apache.cxf.jaxws.handler.types_ package untouched because of BC.\n> I introduced JAXB deserialization adaptors to be able to eliminate HandlerChainBuilder\n> 'jakartaee' methods I indroduced in previous pull request proposal. WDYT about it now?\n\n@ropalka thank you for the effort, it definitely looks better but the main problem is still unsolved: the processing of the JavaEE and JakartaEE namespaces is tangled. May I ask you please to take at look at https://github.com/apache/cxf/pull/756, it is based of `master` (without your changes) but illustrates the idea of how the processing could be separated. AFAIK it does not change or alter any public APIs, only the implementation details have been moved around. Would appreciate to hear any concerns, thank you.": "Y",
  "You can use Autotools too, I just don't know what the target is, and don't like to wait around for the entire build 😛": "Y",
  "LGTM, Merging into master and release-1.13": "N",
  "I always build with Oracle JDK 8 (u171 at the moment). ant version is 1.10.3. \nI suspect that you have an old mongo db jar lying around in your build path.": "N",
  "I did some manual tests - seems to work nicely - VOTE +1": "N",
  ":confetti_ball: **+1 overall**\n\n\n\n\n\n\n| Vote | Subsystem | Runtime | Comment |\n|:----:|----------:|--------:|:--------|\n| +0 :ok: | reexec | 0m 31s | Docker mode activated. |\n| -0 :warning: | yetus | 0m 4s | Unprocessed flag(s): --brief-report-file --spotbugs-strict-precheck --whitespace-eol-ignore-list --whitespace-tabs-ignore-list --quick-hadoopcheck |\n||| _ Prechecks _ |\n||| _ master Compile Tests _ |\n| +1 :green_heart: | mvninstall | 3m 28s | master passed |\n| +1 :green_heart: | compile | 1m 1s | master passed |\n| +1 :green_heart: | shadedjars | 6m 44s | branch has no errors when building our shaded downstream artifacts. |\n| +1 :green_heart: | javadoc | 0m 38s | master passed |\n||| _ Patch Compile Tests _ |\n| +1 :green_heart: | mvninstall | 3m 34s | the patch passed |\n| +1 :green_heart: | compile | 0m 56s | the patch passed |\n| +1 :green_heart: | javac | 0m 56s | the patch passed |\n| +1 :green_heart: | shadedjars | 6m 36s | patch has no errors when building our shaded downstream artifacts. |\n| +1 :green_heart: | javadoc | 0m 36s | the patch passed |\n||| _ Other Tests _ |\n| +1 :green_heart: | unit | 144m 59s | hbase-server in the patch passed. |\n| | | 171m 7s | |\n\n\n| Subsystem | Report/Notes |\n|----------:|:-------------|\n| Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/job/PR-2858/3/artifact/yetus-jdk8-hadoop3-check/output/Dockerfile |\n| GITHUB PR | https://github.com/apache/hbase/pull/2858 |\n| Optional Tests | javac javadoc unit shadedjars compile |\n| uname | Linux d57292bf8a88 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | dev-support/hbase-personality.sh |\n| git revision | master / 4caab90aa7 |\n| Default Java | AdoptOpenJDK-1.8.0_232-b09 |\n| Test Results | https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/job/PR-2858/3/testReport/ |\n| Max. process+thread count | 4417 (vs. ulimit of 30000) |\n| modules | C: hbase-server U: hbase-server |\n| Console output | https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/job/PR-2858/3/console |\n| versions | git=2.17.1 maven=3.6.3 |\n| Powered by | Apache Yetus 0.12.0 https://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n": "N",
  "No rush, whenever you have time. Thanks !\n": "N",
  "> We won't be able capture this in JIRA either. I am not sure how much of this is important to have all the commits from each contributor for a single JIRA which in itself is rare unless its a big patch. It does have ability to give each contributor credit in the commit log.\n\nFrom a legal perspective it's very important that we be able to track the provenance of all code that lands in an ASF repository and could potentially be released.\n\nFor example: \n\nBob is a committer. Alice and Charles are not. Alice and Charles collaborate on a patch, both making commits. In the process Charles commits some code that he doesn't have the legal rights to (its proprietary, etc.). Later Bob uses this script to merge the pull request, and squash all the commits. Alice and Charles are listed as authors of the patch, but there is no history regarding how the code that the ASF doesn't have rights to get there. Was it Charles or Alice?\n\nThat may seem like an edge case, but one that we should absolutely consider.\n": "Y",
  "Any further comments/concerns @wchevreuil ?": "N",
  "@houht: Was this intended to be a bug report? If you, could you head over to https://issues.apache.org/jira/browse/MESOS and file a ticket there?": "N",
  "I merged this but I could not push the tag. I got 403 errors when I run `git push https://github.com/apache/beam jupyterlab-sidepanel-v1.0.0`.\n\nI thought I should be able to do this because I did this for releases previously. @kennknowles - do you know what I might be doing wrong?": "Y",
  "The change looks ok but why have you merged it with my previous commit?": "N",
  "@weisJ I have just tested it and it works now with your last version, Thank you !\n\n@vlsi , thanks for work on this, the result is nice": "N",
  "Failed with:\n\n```\n[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.12:check (default) on project jclouds: Too many files with unapproved license: 1 See RAT report in: /home/travis/build/apache/jclouds/target/rat.txt -> [Help 1]\n```\n\nPerhaps rat should ignore `.travis.yml`?": "N",
  "Thanks for the links, sounds reasonable.\nTwo things I found:\n1. maybe you should add a short paragraph in the &lt;description> section like for the other tools\n2. why you named the target \"vnu\"? Something like \"html-check\" sounds more descriptive. Compare to \"dependency-check\" instead of \"owasp\"": "Y",
  "Actually, can you add a test case for this so we know it doesn't break again in the future?": "Y",
  "Many thanks.\r\n\r\nI've fixed the line-ends to contain line-feeds only as I couldn't merge your patch on a Linux box, therefore the PR now has conflicts. Sorry about that.\r\n\r\nAs written the tests depend on the timezone of the machine  running the tests:\r\n\r\n```\r\nTestcase: testLenientDateTime took 0,006 sec\r\n\tFAILED\r\nexpected:<1488622440000> but was:<1488618840000>\r\njunit.framework.AssertionFailedError: expected:<1488622440000> but was:<1488618840000>\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\r\n\tat org.junit.Assert.assertEquals(Assert.java:118)\r\n\tat org.junit.Assert.assertEquals(Assert.java:555)\r\n\tat org.junit.Assert.assertEquals(Assert.java:542)\r\n\tat org.apache.tools.ant.util.DateUtilsTest.testLenientDateTime(DateUtilsTest.java:119)\r\n```\r\n\r\nthis is on GMT+2 and off by an hour.  But as this is off by an hour I think it is more about me being in daylight saving time right now while the date you picked was not (most countries of the EU switch on the last weekends of March and October).": "Y",
  "My inclination is to merge this, but I've started a discussion on this PR on the dev list to make sure the community is ok with this change: https://lists.apache.org/thread.html/Zlxuc3qc7rkc3fq\n": "N",
  "this looks good enough\n": "N",
  "@DaanHoogland do you know why jenkins is failing here?": "N",
  "About changing return result from `DatasetGraphText.search`:\n\nI'm unsure whether it is better to add `searchWithScore` (or better name?), which returns the `List<TextHit>` form, and retain `search` returning `List<Node>` (just a `Iter.map` added each time):\n\nExample:\n\n```\n/** Search the text index on the default text field */\npublic Iterator<TextHit> searchWithScore(String queryString) {\nreturn search(queryString, null) ;\n}\n\n/** Search the text index on the default text field */\npublic Iterator<Node> search(String queryString) {\nreturn Iter.map(searchWithScore(queryString, textHit->textHit.getNode()) ;\n}\n```\n\nIf we do make a change, then Jena3 is the time to do it.\n\nWhat do you think?\n": "Y",
  "what needs to be done to merge this is, remove the demo test case and resolve the small merge conflict\n": "N",
  "@leachuk Thanks for this PR. To enable to proceed with it please first create an according JIRA ticket and link it from your commit message. Also please rebase and fix the conflict and some issue related to HTTPS has been fixed in https://github.com/apache/jackrabbit/commit/3b2fae841179ee92bfe77a8c7d027fe104c97623 already.": "Y",
  "Sure - go head if it is hard to pull out.\n": "N",
  "Jenkins error seems unrelated:\n\n```\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project cloud-server: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test failed: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?\n```\n": "N",
  "I'm a little on the fence in terms of squashing the commits of others vs. asking the contributor to do so. There are a lot of situations where spreading out a big patch over multiple commits makes sense and makes the history more consumable. \n\nA couple of questions:\n- How does this preserve authorship in a pull request that has commits from multiple authors?\n- How would this work with our current branch model? Specifically, applying a pull request to multiple branches.\n": "Y",
  "I would just use a tag. The only purpose of a release branch is cherrypicking and continuing to do point releases.": "Y",
  "1": "N",
  "Unless I misunderstand your question It does, JMeter does not start:\n\n- 10.11.16\n\nStacktrace:\n\n`2020-03-14 22:40:32,223 INFO o.a.j.JMeter: Setting LAF to: com.github.weisj.darklaf.DarkLaf:com.github.weisj.darklaf.theme.DarculaTheme\n2020-03-14 22:40:32,309 ERROR o.a.j.JMeter: An error occurred: \njava.lang.UnsatisfiedLinkError: /private/var/folders/72/68dyl2ns1q37l4f0p5yfmg4w0000gn/T/nativeutils61999062531814/libdarklaf-macos.dylib: dlopen(/private/var/folders/72/68dyl2ns1q37l4f0p5yfmg4w0000gn/T/nativeutils61999062531814/libdarklaf-macos.dylib, 1): Symbol not found: _NSAppearanceNameDarkAqua\nReferenced from: /private/var/folders/72/68dyl2ns1q37l4f0p5yfmg4w0000gn/T/nativeutils61999062531814/libdarklaf-macos.dylib\nExpected in: /System/Library/Frameworks/AppKit.framework/Versions/C/AppKit\nin /private/var/folders/72/68dyl2ns1q37l4f0p5yfmg4w0000gn/T/nativeutils61999062531814/libdarklaf-macos.dylib\nat java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_201]\nat java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) ~[?:1.8.0_201]\nat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824) ~[?:1.8.0_201]\nat java.lang.Runtime.load0(Runtime.java:809) ~[?:1.8.0_201]\nat java.lang.System.load(System.java:1086) ~[?:1.8.0_201]\nat com.github.weisj.darklaf.platform.NativeUtil.loadLibraryFromJar(NativeUtil.java:108) ~[darklaf-native-utils-1.4.1.0.jar:1.4.1.0]\nat com.github.weisj.darklaf.platform.macos.JNIDecorationsMacOS.loadLibrary(JNIDecorationsMacOS.java:76) ~[darklaf-macos-1.4.1.0.jar:1.4.1.0]\nat com.github.weisj.darklaf.platform.macos.JNIDecorationsMacOS.updateLibrary(JNIDecorationsMacOS.java:65) ~[darklaf-macos-1.4.1.0.jar:1.4.1.0]\nat com.github.weisj.darklaf.platform.macos.MacOSDecorationsProvider.initialize(MacOSDecorationsProvider.java:49) ~[darklaf-macos-1.4.1.0.jar:1.4.1.0]\nat com.github.weisj.darklaf.platform.Decorations.initialize(Decorations.java:70) ~[darklaf-core-1.4.1.0.jar:1.4.1.0]\nat com.github.weisj.darklaf.DarkLaf.setupDecorations(DarkLaf.java:147) ~[darklaf-core-1.4.1.0.jar:1.4.1.0]\nat com.github.weisj.darklaf.DarkLaf.getDefaults(DarkLaf.java:120) ~[darklaf-core-1.4.1.0.jar:1.4.1.0]\nat javax.swing.UIManager.setLookAndFeel(UIManager.java:539) ~[?:1.8.0_201]\nat javax.swing.UIManager.setLookAndFeel(UIManager.java:583) ~[?:1.8.0_201]\nat com.github.weisj.darklaf.LafManager.install(LafManager.java:171) ~[darklaf-core-1.4.1.0.jar:1.4.1.0]\nat com.github.weisj.darklaf.LafManager.installTheme(LafManager.java:140) ~[darklaf-core-1.4.1.0.jar:1.4.1.0]\nat org.apache.jmeter.gui.action.LookAndFeelCommand.activateLookAndFeel(LookAndFeelCommand.java:211) ~[ApacheJMeter_core.jar:5.3-SNAPSHOT]\nat org.apache.jmeter.JMeter.startGui(JMeter.java:377) ~[ApacheJMeter_core.jar:5.3-SNAPSHOT]\nat org.apache.jmeter.JMeter.start(JMeter.java:544) [ApacheJMeter_core.jar:5.3-SNAPSHOT]\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_201]\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_201]\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_201]\nat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_201]\nat org.apache.jmeter.NewDriver.main(NewDriver.java:252) [ApacheJMeter.jar:5.3-SNAPSHOT]\n`": "N",
  "All automated tests passed.\nRefer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/16090/\n": "N",
  "All these PRs need a bug in fop jira with replication steps of issue": "N",
  "Jenkins failed with a timeout (not related to your code). Can you squash your commits and do a force push again? Sorry for the runaround..": "N",
  "This seems like it might not be backwards compatible with the existing kafka-spout; i.e., the offsets in ZK are presumably not going to be stored exactly the same as they were before. Is there any plan for supporting migration from the current kafka-spout to this one?\n": "Y",
  "I think there needs to be a documentation update too - i just noticed this: \n\nhttp://tinkerpop.apache.org/docs/3.3.4/reference/#_limitations": "N",
  "@lulseged please follow https://github.com/apache/cxf/pull/737": "N",
  "Hi @Akshay-Iyangar, great that you want to contribute to this PR, I hope that @ajothomas won't be mind. Please, let me know when it will be ready for review. I'll be happy to have it merged finally. ": "Y",
  "I have pushed a slightly modified version of the patch which only adds setter/getter for the enum-value. I see no use in providing the long-value to the outside and we probably should switch to the Enum for HSSF as well in the future and deprecate the \"short\" versions there. \n\nPlease close this PR if the change is sufficient for you or comment if you think this should be done differently.": "Y",
  "I like the idea of checking the html files.\nBut why are these changes required? Only because a tool sais that is not enough for me. Do you have pointers to some specs?": "Y",
  "@rhtyd a Jenkins job has been kicked to build packages. I'll keep you posted as I make progress.\n": "N",
  "LGTM": "N",
  "Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community\nto review your pull request. We will use this comment to track the progress of the review.\n\n\n## Automated Checks\nLast check on commit 8257d0ef74983dad3b34aa4fbcbabb9bae8f58b7 (Thu Jan 16 13:41:26 UTC 2020)\n\n**Warnings:**\n* No documentation files were touched! Remember to keep the Flink docs up to date!\n* **This pull request references an unassigned [Jira ticket](https://issues.apache.org/jira/browse/FLINK-13758).** According to the [code contribution guide](https://flink.apache.org/contributing/contribute-code.html), tickets need to be assigned before starting with the implementation work.\n\n\n<sub>Mention the bot in a comment to re-run the automated checks.</sub>\n## Review Progress\n\n* ❓ 1. The [description] looks good.\n* ❓ 2. There is [consensus] that the contribution should go into to Flink.\n* ❓ 3. Needs [attention] from.\n* ❓ 4. The change fits into the overall [architecture].\n* ❓ 5. Overall code [quality] is good.\n\nPlease see the [Pull Request Review Guide](https://flink.apache.org/contributing/reviewing-prs.html) for a full explanation of the review process.<details>\nThe Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>\nThe @flinkbot bot supports the following commands:\n\n- `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)\n- `@flinkbot approve all` to approve all aspects\n- `@flinkbot approve-until architecture` to approve everything until `architecture`\n- `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention\n- `@flinkbot disapprove architecture` to remove an approval you gave earlier\n</details>": "N",
  "Is there anything in the way of merging this? We're doing something quite similar using reflection and it improves the memory leak situation dramatically.\n": "Y",
  "@jgallimore Yes. Can be closed.": "N",
  "I think changes like this need to be discussed before hand on the dev mailing list or on JIRA issue.\n\nThis would configure a build on a Windows machine using GitHub actions. I don't think we need this, at least not in this form.": "Y",
  "Can we separate the style check update from the sbt update?\n": "N",
  "Just minor nitpicks.\nLooks good to me!\nThanks!\n": "N",
  "Thanks @JingsongLi for the update. I suppose the purpose of introducing these abstractions is to support writing partitions to different external systems other than Hive. Can we have a summary about what a user/developer needs to implement in order to achieve that?": "Y",
  "@twalthr Thanks a lot for your comments. However, I don't agree with you on some points:\r\n\r\n> -1 for this PR.\r\n\r\nWhen you post your review comments in the PR, it has already indicated that changes are required for this PR. If this is for my previous +1, it's also not necessary. Just as I said \"+1 from my side\", it only represents that there is no problems from my side. \r\n\r\n+1 is only for the part which I'm pretty sure, for the other part which I'm not familiar with, I already ping you. That's the reason why I also ping you to review this change as I think you are more familiar with the changes of this PR.\r\n\r\n> \r\n> First of all, it adds tests that don't focus on what should actually be tested.\r\n> \r\n> Second, it merges the entire table config into global job parameters. This does not fit into the big picture of FLIP-59. Where global job config should just be a single key under the table config and execution config.\r\n\r\nFLIP-59 is still under discussion and it has not been accepted yet. Although it's a meaningful proposal from my side, it has been inactive for more than one month. Is there any plan to commit it to release 1.10? if not, we should follow the current status. Besides, before FLIP-59 is accepted and committed, any configuration added via TableConfig is global job parameters and so this change is also self-contained and meaningful from this point of view, you can see Blink planner already implemented it in this way. The details could be found [here](https://github.com/apache/flink/blob/d32af521cbe83f88cd0b822c4d752a1b5102c47c/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/delegation/PlannerBase.scala#L289).\r\n\r\nBest,\r\nDian": "Y",
  "@roded We plan to release 2.3.0 later this month. Do you think you can finish this PR soon?": "N"
}