{
  "ant-unit-1.3 is in the repo (https://github.com/apache/ant/blob/master/lib/optional/ant-antunit-1.3.jar).\r\nIt seems that you use an Ant version for building this which does not contain this jar.\r\n\r\nIf you are building Ant itself I recommend cleaning the environment before (unset ANT_HOME) and bootstrap Ant by itself.": "Y",
  "@nitin-maharana shouldn't we use the \"recursive\" parameter set to \"true\" too?\n": "N",
  ":confetti_ball: **+1 overall**\n\n\n\n\n\n\n| Vote | Subsystem | Runtime | Comment |\n|:----:|----------:|--------:|:--------|\n| +0 :ok: | reexec | 1m 6s | Docker mode activated. |\n| -0 :warning: | yetus | 0m 3s | Unprocessed flag(s): --brief-report-file --spotbugs-strict-precheck --whitespace-eol-ignore-list --whitespace-tabs-ignore-list --quick-hadoopcheck |\n||| _ Prechecks _ |\n||| _ master Compile Tests _ |\n| +1 :green_heart: | mvninstall | 4m 47s | master passed |\n| +1 :green_heart: | compile | 1m 13s | master passed |\n| +1 :green_heart: | shadedjars | 7m 25s | branch has no errors when building our shaded downstream artifacts. |\n| +1 :green_heart: | javadoc | 0m 42s | master passed |\n||| _ Patch Compile Tests _ |\n| +1 :green_heart: | mvninstall | 4m 33s | the patch passed |\n| +1 :green_heart: | compile | 1m 12s | the patch passed |\n| +1 :green_heart: | javac | 1m 12s | the patch passed |\n| +1 :green_heart: | shadedjars | 7m 29s | patch has no errors when building our shaded downstream artifacts. |\n| +1 :green_heart: | javadoc | 0m 40s | the patch passed |\n||| _ Other Tests _ |\n| +1 :green_heart: | unit | 212m 51s | hbase-server in the patch passed. |\n| | | 244m 0s | |\n\n\n| Subsystem | Report/Notes |\n|----------:|:-------------|\n| Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/job/PR-2858/3/artifact/yetus-jdk11-hadoop3-check/output/Dockerfile |\n| GITHUB PR | https://github.com/apache/hbase/pull/2858 |\n| Optional Tests | javac javadoc unit shadedjars compile |\n| uname | Linux 09d9fce59be2 4.15.0-128-generic #131-Ubuntu SMP Wed Dec 9 06:57:35 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\n| Build tool | maven |\n| Personality | dev-support/hbase-personality.sh |\n| git revision | master / 4caab90aa7 |\n| Default Java | AdoptOpenJDK-11.0.6+10 |\n| Test Results | https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/job/PR-2858/3/testReport/ |\n| Max. process+thread count | 3601 (vs. ulimit of 30000) |\n| modules | C: hbase-server U: hbase-server |\n| Console output | https://ci-hadoop.apache.org/job/HBase/job/HBase-PreCommit-GitHub-PR/job/PR-2858/3/console |\n| versions | git=2.17.1 maven=3.6.3 |\n| Powered by | Apache Yetus 0.12.0 https://yetus.apache.org |\n\n\nThis message was automatically generated.\n\n": "N",
  "1. not that much, we are not that strong about it cause we all have different habits and thinking about it\n2. you can create a new surefire execution for this test with another arquillian container config, that said the embedded testing in openejb-core (which forks by test) should be enough probably\n3. you logged it was read only in the assembler, this is enough for now probably": "Y",
  "Hi Alfusainey,\n\n> one change to the json-string: i now encode jcr properties as json objects instead of simple name:value(s) pairs\n\nwhile I see that using objects for properties is more robust, it defies to purpose of using a simple, human readable format like json. for a machine-to-machine format we can use sysview XML. \n\nI would really try to use name/value pairs as properties. and encode the type either in the value or in the name (the later has the advantage, that you can define types on jcr mv-properties easier). eg\n\n```\n{\n\"jcr:lastModified@Date\": \"2014-06-10T14:30Z\",\n\"numbers@Long\": []\n}\n```\n\n(btw: I'd prefer this discussion on the jackrabbit dev mailing list)\n": "Y",
  "@Alfusainey while i see the benefit of having less redundant code, i think the additional serialize to xml and then in importXml deserialize again is a severe performance hit. can't you just use the same deserialized objects that represent the information and feed them into the same code?\n\nanother thing: to get the acl import working, i would not go about too heavy refactorings of jackrabbit. it will be much more difficult to convince people to accept a severe refactoring with all its potential of bugs or performance penalties over just handling acl nodes with a special handler, with almost no impact on the operations that where already supported.\n\nthis decision is up to @anchela and other jackrabbit maintainers but i guess they will be more at ease with less invasive changes. once you got your primary goal through, you can still look at jackrabbit and also oak and see if you want to propose further refactoring. but lets first focus on the primary goal of remoting acl information.\n": "Y",
  "Merge done! Thank you.\n": "N",
  "Great change and I agree it should be fixed in the central location. Many thanks!": "Y",
  "Okay, now I get it. Agreed that number 3 is \"trying too hard\" and on the proposal to provide number 2 and document appropriate usage.": "N",
  "There wasn't a JIRA that I can remember @pluradj - just something discussed on the mailing list.\n": "N",
  "### ACS CI BVT Run\n\n**Sumarry:**\nBuild Number 56\nHypervisor xenserver\nNetworkType Advanced\nPassed=72\nFailed=1\nSkipped=3\n\n_Link to logs Folder (search by build_no):_ https://www.dropbox.com/sh/yj3wnzbceo9uef2/AAB6u-Iap-xztdm6jHX9SjPja?dl=0\n\n**Failed tests:**\n- test_vpc_vpn.py\n- test_01_redundant_vpc_site2site_vpn Failed\n\n**Skipped tests:**\ntest_vm_nic_adapter_vmxnet3\ntest_static_role_account_acls\ntest_deploy_vgpu_enabled_vm\n\n**Passed test suits:**\ntest_deploy_vm_with_userdata.py\ntest_affinity_groups_projects.py\ntest_portable_publicip.py\ntest_over_provisioning.py\ntest_global_settings.py\ntest_scale_vm.py\ntest_service_offerings.py\ntest_routers_iptables_default_policy.py\ntest_routers.py\ntest_reset_vm_on_reboot.py\ntest_snapshots.py\ntest_deploy_vms_with_varied_deploymentplanners.py\ntest_deploy_vm_iso.py\ntest_list_ids_parameter.py\ntest_public_ip_range.py\ntest_multipleips_per_nic.py\ntest_regions.py\ntest_affinity_groups.py\ntest_network_acl.py\ntest_pvlan.py\ntest_volumes.py\ntest_nic.py\ntest_deploy_vm_root_resize.py\ntest_resource_detail.py\ntest_secondary_storage.py\ntest_vm_life_cycle.py\ntest_disk_offerings.py": "N",
  "Hello @ham1 ,\n\nRegarding this PR, did you have the opportunity to test it on a \"real life\" performance test ? \n\nWhat were the use cases ? and related volumes ?\n\nThanks": "Y",
  "@weisJ , I just tested jmeter, I am commenting on https://github.com/weisJ/darklaf/pull/72 now": "N",
  "All tests pass with `docker/build.sh -t -n -i`\n\nVOTE +1\n": "N",
  "@hsun-cnnxty : can you please make your statement a bit more concrete? i.e., what other info is stored in a given kafka topic's consumer state? (other than the offset/partition)\n": "Y",
  "I'd really like to go forward with automated tools for developers / committers. What I've stated from dev@ mailing list, many projects already use specific tools for merging, and the merge script originated from Spark is well used for Spark, Kafka, Zeppelin (now TLP).\n": "Y",
  "@harshach Yeah, I don't know what things Kafka improve from Spark script so I wanted to see the benefit if you know about it. As I commented earlier, just adopting script doesn't work since we use different branch model (master, minor, bugfix) so it should be fully tested (including JIRA integration) before adopting.\n": "Y",
  "As a quick note @Haapsaari-Juha given that we are trying to quickly turn around a 3.4.6 release to fix a bug in 3.4.5:\n\n\n\nhttps://lists.apache.org/thread.html/r7de7d160173d0774720b347122409bb6e46268613205b08edaa687ee%40%3Cdev.tinkerpop.apache.org%3E\n\n\n\nit may take some extra time to see this change reviewed/merged.": "N",
  "This is explained by:\n- https://bz.apache.org/bugzilla/show_bug.cgi?id=60650": "N",
  "**[Test build #79541 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/79541/testReport)** for PR 18602 at commit [`6f91356`](https://github.com/apache/spark/commit/6f9135645cd767e9d69d98157189c2e7ba08a5cc).\n* This patch passes all tests.\n* This patch merges cleanly.\n* This patch adds no public classes.": "N",
  "Thanks for the reply @ropalka. We won't be changing the public API, which is outlined by `AnnotationHandlerChainBuilder`. The `JakartaeeHandlerChainBuilder` and `JavaeeHandlerChainBuilder` would encapsulate the implementation specifics to each namespace, and are not supposed to be used outside `AnnotationHandlerChainBuilder`. All the existing classes, under `org.apache.cxf.jaxws.handler.types` fe, could stay were they are. What do you think?": "Y",
  "Here's my understanding regarding this script.\n\n> main()\n- get latest branch (highest version) from github mirror\n- it assumes that prefix of release branch is `release version`\n- get information regarding pull request and events of pull request\n- prompt user(committer) to input commit title: default value is title of pull request, and user can replace them\n- standardize commit title (via standardize_jira_ref) \n- prompt user to use title as modified vs original\n- check that pull request is already merged (closed by asfgit)\n- if it is, check merge commit is fetched to local, and cherry-pick to latest branch assuming user wants backport\n- prompt user to continue if PR seems to resolve conflicts (by seeing flag from PR information)\n- print information of pull request, and commit title, and so on, and prompt user to go on\n- merge PR into target branch of PR: get commit hash afterwards\n- prompt user to see user wants cherry-pick\n- if yes, cherry-pick to latest branch\n- prompt user to update associated JIRA\n- if yes, resolve issue as fixed with leaving comment\n\n> merge_pr()\n- fetch branch which pull request is referring\n- fetch and checkout branch which pull request targets (from asf-git)\n- merge pull request branch with squash option\n- if there're some conflicts, guide user to fix it and mark as resolved (via git add)\n- prompt user to input main author: default value is who has most of commits (via extracting authors from pull request branch and sort)\n- prompt user to input reviewers: can be blank\n- prompt user to see user wants to list all commits into commit message\n- construct commit message by\n- commit title\n- body of pull request if presented\n- all authors\n- all reviewers if presented\n- user if user resolves merge conflict manually\n- auto close message for pull request\n- list of commits if user want to\n- commit with passing main author as author, and commit message\n- prompt user to push, or stop\n- push changes to remote repository (to asf-git)\n- clean temporary branches\n- get commit hash and return\n\n> cherry_pick()\n- prompt user to input branch name to cherry-pick: default value is latest branch which is passed from main()\n- fetch and checkout branch which cherry-pick targets (from asf-git)\n- cherry-pick with commit hash\n- if there're some conflicts, guide user to fix it and complete cherry-pick manually\n- prompt user to push, or stop\n- push changes to remote repository (to asf-git)\n- clean temporary branches\n- get commit hash and return\n\n> resolve_jira_issue()\n- prompt user to input JIRA issue ID: default value is extracted value from commit title\n- get information of the issue\n- check status and stop if issue is already marked as 'Resolved'\n- print information of the issue\n- prompt user to input comma-separated fix versions\n- default values are `unreleased` versions matched to target branch for merge() / cherry_pick()\n- develop branch is treated as default version\n- mark issue as 'Resolved' with setting fix versions and leaving comment\n": "N",
  "@analytically I see that you made some updates. I think you might need to rebase against tp31 though to resolve conflicts. Other than that, is this ready for review at this point?\n": "N",
  "Hello,\nThanks for PR.\nI'll look into merging it but what exactly is the 3rd point ? Is it to avoid sending to INflux the Validation results ?\nUsually it's better to split PR in this case.\n\nThanks": "Y",
  "@jfarrell Understood (and thank you!). I thought I had read long ago that the Pull Request would close automatically so long as we didn't change the hash by altering the message when we commit it. Emmanuel had also raised https://issues.apache.org/jira/browse/QPID-5527 to cover the request, so we can move over there.\n": "Y",
  "@analytically do you think you will be able to come back to this PR at some point?\n": "N",
  "> Parse and we notify the response handler about the exception.\n\n\n\nWill not because message is null. There there is silent error eating. \n\n\n\nCould we just check that deserialized object is not null and throw exception.\n\n\n\n```\n\nvar receivedMsg = _messageSerializer.DeserializeMessage<ResponseMessage<JToken>>(received);\n\nif (receivedMsg == null)\n\n{\n\nthrow new Exception(\"receivedMsg\");\n\n}\n\n```\n\n\n\nTo check what happens in case of null array passed, in case of empty array passed, and do property based testing for any other array passed. So it will be more explainable failure than just eating null.": "Y",
  "I like the approach of moving the logic for fixing the lat lines to a separate filter. Kudos for adding tests ;-)\r\n\r\nA different approach could be to allow an alternative set of filters to be applied before concatenating, i.e. allowing users to define two sets of filters one to apply on each input stream individually and one to be applied to the merged stream. Naming could become an issue (and I've got a long track of picking bad names myself).\r\n\r\nI'm not asking you to change your PR, I'd rather like to discuss the different approaches to see which would be best. So what do you think?": "Y",
  "@nitin-maharana thanks for checking.\nLGTM\n": "N",
  "@FlorianHockmann I did change in my PR. Please review.\n\n\n\n> property based testing\n\n\n\nGiven we do not have fix which only improves situation (like my PR), we would have to prove.\n\n\n\nFor this:\n\n0. Check latest Json library and see it returns `object?` - so null is possible. DONE\n\n1. Test with byte array being null, empty. What error we see?\n\n2. Test byte array contains string \"null\", \"\", \"[]\", \"{}\", not null terminated string, etc.\n\n3. Run generator of byte arrays with https://github.com/haf/expecto#property-based-tests (in C#)\n\n\n\nCheck that we never ever get null reference in this case. Or in some case we do. Than we should look into Json or into layer creating byte array. And have better semantics for error. \n\n\n\nBut this seems no needed for now.": "Y",
  "OK I am +1 for this then.\n": "N",
  "Can one of the admins verify this patch?": "N",
  "I don't think we need this PR. It could easily be accomplished by overriding newMessageDisplayComponent or by the following change:\n\nhttps://gist.github.com/jthomerson/6595716\n": "Y",
  "Tested succesfully thanks !": "N",
  "@nitin-maharana This fix should be against 4.7 IMHO. Can you make a PR for that please? We can still refer to the reviews that have been done. This way both 4.7 and 4.8 will have this fix. Thanks!\n": "Y",
  "@reta not sure I get the custom scope impl and need (@ApplicationScoped or @RequestScoped should do he same and properly impl get(bean)) but otherwise looks good": "N",
  "I would make state more concrete, but I suppose your approach is fine.\n": "N",
  "I added a comment to FLINK-2220. I think the original analysis of the problem was not correct. It is not necessary to check for POJOs whether they override `equals()` and `hashcode()`. Details in FLINK-2220\n": "Y",
  "It would be good to get some feedback from the others as well, but in general my arguments are the following:\n1. Getting exceptions after non-parallel sources just because you didn't rebalance (or in any other case), is very confusing for the user and seems unintuitive. Therefore I vote for fixing the current behaviour and keeping rebalance implied.\n2. There is probably a good reason why a user implements an operator (maybe some outside world communication), so not executing it (for instance no sink attached) will lead to incorrect behaviour. Also we cannot force someone to always use a sink in this case as they might need some special behaviour implemented by some other operator.\n": "Y",
  "Hi Alfusainey,\n\nrethinking this, we cannot change the format of the JSON for davex, as other clients also use it.\n": "Y",
  "I think this change should have an own commit with an own comment because it's not related to the version update of Spring.": "Y",
  "Hi,\nYou were right.\nI have a second possible issue.\nHere is what the meanAT shows. It does not look ok to me .\n<img width=\"1418\" alt=\"screen shot 2017-01-26 at 15 51 03\" src=\"https://cloud.githubusercontent.com/assets/3127467/22335641/c6c20a2a-e3df-11e6-9540-626affe26f89.png\">\n\n": "Y",
  "@katya-stoycheva can you please close this - it was applied. Thank you very much for your contribution :-)": "N",
  "My point is that, in my opinion, this configuration (and also [the configuration for the default delimeter](https://github.com/apache/flink/pull/2219#discussion_r70475544)) is not in demand by users.\r\nInstead of exposing this parameter outside of Flink to operators of Flink cluster, you can still make it configurable but internally. So when metrics register themselves, they would define which delimeter to use. End users probably won't need to customize this.\r\n\r\nWhile in case of Prometheus reporter, `.` is [invalid character in metric name](https://prometheus.io/docs/concepts/data_model/#metric-names-and-labels). InfluxDB reporter just followed this convention (I don't remember exact reason why I didn't use `.`). It may worth to use `_` for all reporters (for unification, as some common ground), but I understand that this would be an incompatible change.\r\n\r\nBut thinking more generally, I find that the `(CharacterFilter, delimeter)` pair is just a concrete implementation of more generic api (something like `MetricScopeFormatter` that can become more specific (and different) to some metric reporters.\r\n\r\nTo summarise, my personal cons against this change are:\r\n * the configuration is not requested by users. Less configuration exposed by Flink - less things to maintain as public api and more freedom to change implementation later;\r\n * even if the config is exposed, actual correct values that would work in practice are limited (cannot be any character) and specific to actual metrics system;\r\n * for some reporters, the scope formatting can required to be more flexible (compared to current implementation based on `CharacterFilter` and `delimeter`).": "Y",
  "I took a quick look, one question in general: Is the long-value useful outside of POI at all? Or is it rather something that is only needed internally in the on-disk format? Maybe we can just remove the setter/getter which set/get \"long\" and only provide the enum to the outside to keep the interface simple?": "Y",
  "@NuxRo thanks, fixed the branch, you may remove my access now :)\n\n@blueorangutan package\n": "N",
  "Merged. Thanks!\n": "N",
  "There is lots of good stuff here but also changes to some files that we can't (due to ASF policy w.r.t. 3rd party licenses) change. And as Martin pointed out changing anything that might be considered part of the public API needs careful consideration.\nI'm currently trying to figure out the best way to handle this": "Y",
  "See #146 for an alternative approach": "N",
  "# [Codecov](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=h1) Report\n> Merging [#544](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=desc) into [master](https://codecov.io/gh/apache/jmeter/commit/231436138836aafd9f697896cdc9b442faa4e8d2?src=pr&el=desc) will **increase** coverage by `0.04%`.\n> The diff coverage is `79.12%`.\n\n[![Impacted file tree graph](https://codecov.io/gh/apache/jmeter/pull/544/graphs/tree.svg?width=650&token=6Q7CI1wFSh&height=150&src=pr)](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=tree)\n\n```diff\n@@ Coverage Diff @@\n## master #544 +/- ##\n============================================\n+ Coverage 55.33% 55.37% +0.04% \n- Complexity 9960 9976 +16 \n============================================\nFiles 1033 1035 +2 \nLines 63345 63399 +54 \nBranches 7157 7159 +2 \n============================================\n+ Hits 35052 35109 +57 \n+ Misses 25818 25815 -3 \nPartials 2475 2475\n```\n\n\n| [Impacted Files](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=tree) | Coverage Δ | Complexity Δ | |\n|---|---|---|---|\n| [...lizers/backend/influxdb/InfluxdbMetricsSender.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL2luZmx1eGRiL0luZmx1eGRiTWV0cmljc1NlbmRlci5qYXZh) | `100% <ø> (ø)` | `0 <0> (ø)` | :arrow_down: |\n| [...alizers/backend/AbstractBackendListenerClient.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL0Fic3RyYWN0QmFja2VuZExpc3RlbmVyQ2xpZW50LmphdmE=) | `23.07% <ø> (+1.64%)` | `3 <0> (ø)` | :arrow_down: |\n| [...ackend/influxdb/InfluxdbBackendListenerClient.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL2luZmx1eGRiL0luZmx1eGRiQmFja2VuZExpc3RlbmVyQ2xpZW50LmphdmE=) | `39.91% <0%> (+0.35%)` | `12 <0> (ø)` | :arrow_down: |\n| [...visualizers/backend/influxdb/UdpMetricsSender.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL2luZmx1eGRiL1VkcE1ldHJpY3NTZW5kZXIuamF2YQ==) | `0% <0%> (ø)` | `0 <0> (ø)` | :arrow_down: |\n| [...ter/visualizers/backend/BackendListenerClient.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL0JhY2tlbmRMaXN0ZW5lckNsaWVudC5qYXZh) | `0% <0%> (ø)` | `0 <0> (?)` | |\n| [...end/influxdb/InfluxDBRawBackendListenerClient.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL2luZmx1eGRiL0luZmx1eERCUmF3QmFja2VuZExpc3RlbmVyQ2xpZW50LmphdmE=) | `100% <100%> (ø)` | `14 <14> (?)` | |\n| [...isualizers/backend/influxdb/HttpMetricsSender.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL2luZmx1eGRiL0h0dHBNZXRyaWNzU2VuZGVyLmphdmE=) | `76.84% <70%> (-0.34%)` | `11 <1> (+1)` | |\n| [...er/visualizers/backend/BackendListenerContext.java](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree#diff-c3JjL2NvbXBvbmVudHMvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL2ptZXRlci92aXN1YWxpemVycy9iYWNrZW5kL0JhY2tlbmRMaXN0ZW5lckNvbnRleHQuamF2YQ==) | `23.68% <0%> (ø)` | `7% <0%> (ø)` | :arrow_down: |\n| ... and [1 more](https://codecov.io/gh/apache/jmeter/pull/544/diff?src=pr&el=tree-more) | |\n\n------\n\n[Continue to review full report at Codecov](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=continue).\n> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n> `Δ = absolute <relative> (impact)`, `ø = not affected`, `? = missing data`\n> Powered by [Codecov](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=footer). Last update [2314361...2a4737a](https://codecov.io/gh/apache/jmeter/pull/544?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n": "N",
  "looks really good. just had a few nits that shouldn't block. I also don't have a vote, so you'll need to find a proper reviewer :)\n": "N",
  "Did you make these changes in some automated fashion? I am not sure how I would review this in any sane way.\n\n\n\nI would also caution that these kinds of formatting changes are harmless in one sense but don't deliver any user benefit. They also have a cost in maintainer/reviewer time that is small and shrinking.": "Y",
  "I'm totally +1 to this approach, even though I think script should be modified to Storm's project style.\n\nLike I said to dev@ mailing list, I have been doing reviewing and merging pending pull requests for weeks and months, and it was painful enough to merge and port back to each branches, even though I ignored cleaning up commits. (Pain is amplified when tiny commit should be merged to all branches.) If I want to clean up commits before merging it should be more painful. CHANGELOG is subject to not in sync among branches, but we need to write it manually because it's hard to filter merge commits to see the change list. (We could just rely on JIRA issues for alternative.)\n\nRegarding commits, I don't want to keep commits like 'kicking travis', 'address review comments', etc. which is not helpful at any chances. For my last 2 years of development of Storm, I didn't utilize individual commit. If something is wrong with recent merge, we rollback the merge, not individual commit. Squashing commits is widely used strategy and already shows success story to many big projects. Even Github provides the squash merge mode (recently rebase mode too) in GUI.\n\nIf we want to merge in squashed commit, it should be done in merging process, not reviewing process. For me, ideal review process should be contributor-friendly. While we can't put efforts to only maintain Storm project (by reviewing pull requests, etc.), contributors also can't. Once we create a script which also squashes the commits, we don't need to make contributors bothering with rebasing and squashing commits. If not, all individuals including us should do it just after merger said 'please rebase and squash in order to merge.', which is also bothering for mergers, too. Moreover, there's a chance for contributors to be busy at the moment, and pull request goes stale. Pull requests which need upmerging are the case.\n\nI understand and agree the authorship issue, but we can treat it as exceptional case. Many pull requests are authored by one.\n\nLet's make merging phase as painless, or at least less painful thing.\n": "Y",
  "LGTM\n\ntag:mergeready": "N",
  "This is not ready. Missing apache header on the new file and there is no test. No idea what this is fixing.": "N",
  "Thanks. You can just force push to your branch next time.": "N",
  "@alexvanboxel \r\n1. I'm not entirely sure I understand the use case. Why do options need to be copied, while other constructs (such as schemas) don't?\r\n\r\n2. I disagree for a couple of reasons:\r\n   * A FieldType can be nullable, and there's no reason not to support null FieldTypes in options. By returning null for non-existent fields, you make it hard to distinguish between a null value and an invalid get call (i.e. passing an option that is not part of the schema). This feels like the sort of behavior that seems simpler, but actually makes things more complex (e.g. the fact that Java Maps return null for non-existent values is often considered a mistaken design, for this very reason. It's hard to distinguish between a missing value and an explicit null stored in a map).\r\n\r\n   * Options are no different than schemas - there is a static list of options for each field, so I don't see why this is so different.\r\n\r\nI also want to understand better why we can't just use Row here (revisiting Brian's question). You mentioned using dots in field names, but AFAIK protobuf also prohibits dots. It's true that dots are often used in option specifications in proto, but that's just to set individual fields of a message. In Beam, we really should model those as a single row-valued option, not as multiple individual options. I.e. (from the proto docs):\r\n\r\n    option (my_method_option).foo = 567;\r\n    option (my_method_option).bar = \"Some string\";\r\n\r\nshould translate to a single Beam option named \"my_method_option\" not to two separate options.\r\n\r\ndots are also used to represent package names of course. Is this the main reason you need to support dots?": "Y",
  "Sorry, let me double check to merge. Thanks !\n": "N",
  "Hi @twalthr, \r\n\r\nThanks a lot for your reply. Regarding adding the `global job configuration`, as it will introduce new API interfaces and it will be better to discuss it in the ML or the mentioned FLIPs. The aim of this PR is to align the legacy planner with the blink planner(maybe we should change the title of this PR to make it more clear). So how about limiting the scope of this PR to aligning the two planners and discussing the API changes in the FLIP discussion? \r\n\r\nBest,\r\nDian": "Y",
  "`SourceTestUtils` doesn't invoke `splitIntoBundles` per se, but it provides `assertSourcesEqualReferenceSource`. You should call `splitIntoBundles` yourself with various parameters and verify the result using that function.\n\nIt also indeed doesn't test `getEstimatedSize` - that has to be simply unit-tested manually.\n": "Y",
  "> I'm not sure it is worth adding yet another Listerer implementation.\n\nIMO this one complements the other one and will be more precise\n> \n> It looks like the users would have to copy-paste the configuration if they want to switch between raw and grouped data.\n\nIs this a big deal ? \nWill this really happen ?": "N",
  "This should be integration testable; however, I saw in the JIRA that you were having issues recreating this. What area according to was giving you issues making an IT for this?": "Y",
  "Yep last error is because of non converging spring version\n": "Y",
  "Thanks for the PR, @ropalka, I think we got to the point when it is probably make sense to completely separate Jakartaee and Javaee name handler chains (I think you would agree). I suggest to:\n1. Introduce `org.apache.cxf.jaxws.handler.jakartaee` and `org.apache.cxf.jaxws.handler.javaee` packages\n2. Introduce `JakartaeeHandlerChainBuilder` and `JavaeeHandlerChainBuilder`\n3. Basically, I think it safe to assume that either JakartaEE or JavaEE namespace is going to be used, so in this case `AnnotationHandlerChainBuilder` could inspect the namespace, instantiate `JakartaeeHandlerChainBuilder` or`JavaeeHandlerChainBuilder`, and delegate to one of those. In this case, the conditionals should be gone and implementation becomes much cleaner. \n\nWhat do you think?": "Y",
  "pinging @llambiel and @pyr @wido LGTM.\n": "N",
  "Some thoughts:\r\n\r\nI know your use case for options involves mapping specific input types to schemas, however we need to make sure that the actual mechanism is more generic than that. We have many uses for field metadata (or options) inside of Beam, completely unrelated to any option concept on input data types. I would not like to make a design choice that precludes nullable FieldTypes now, because we may very well find that we need nullable field types.\r\n\r\nThat being said: I notice that you haven't removed the FieldType metadata in this PR. Is that planned for a future PR? I don't think we want both metadata and options in the protocol - Options seemed to me to be a better-typed version of metadata.\r\n\r\nI don't personally think that the hasOption code is too bad, and it's definitely better than constraining the type system for Options in a way that we quite likely will regret. That being said, you've already added the getValueOrDefault method. So you can still write this code:\r\n\r\n   .getFields()\r\n        .forEach(\r\n            f -> {\r\n              Integer pkSequence = f.getOptions().getValueOrDefault(\"vptech.data.contract.v1.primary_key\", null);\r\n              if (pkSequence != null) {\r\n                primaryKeyMap.put(pkSequence, dataSchema.indexOf(f.getName()));\r\n              }\r\n            });\r\n\r\nA tiny bit more verbose, but much more explicit about what you are trying to do. (FYI the reason I don't suggest allowing getValue to return an Optional<T> - even though it might be the \"better\" solution - is because the rest of the code base does not use Optional, and I want things to stay consistent).\r\n\r\nAbout the removeOption builder method. I'm struggling with this a bit, since it feels like a weird pattern to put remove into a builder. I'm also not sure whether copying a a Schema field with just a subset of options is a common use case or not. Generally we should start off by keeping such things out of the core Beam API until we are convinced that there are multiple users who need them. I think you can still accomplish the same thing by simply adding a new Options.Builder.setOptions override that takes in a Map.\r\n\r\nnewField = newField.withOptions(Options.builder().setOptions(\r\n     Maps.filterKeys(\r\n        oldField.getOptions().getAllOptions(),\r\n        n -> !n.equals(optionToRemove))));\r\n\r\nThis code is probably a bit more annoying for you to write. If we are convinced that this is a common use case then I'm ok with adding removeOption, but if it's specific to your use case then I think we're better off adding the new setOptions method and using that.\r\n\r\n\r\n": "Y",
  "There are a few `// *** score` markers left - presumably these can all go now.\n": "N",
  "Hi @dianfu @WeiZhong94 as far as I know, @twalthr  is on vacation. So I agree that we can limit the scope  of this PR. If there are any improvement we can open new PRs before 1.10 release. \r\n\r\nWhat do you think? \r\n\r\n@hequn8128 is there any suggestion from your side? :)\r\n\r\nBest,\r\nJincheng": "Y",
  "This PR has been merged: https://github.com/apache/tomee/pull/541 - is this one still needed?": "N",
  "You should use SELECT ... FOR UPDATE instead of SELECT / UPDATE. Much less code and transactionally-safe. Also, if you use SELECT ... FOR UPDATE, you can use ResultSet.moveToInsertRow to insert a record if necessary.": "Y",
  "Why isn't rebalance implied when the 2 operators don't have the same parallelism and partitioning is not defined? If you don't specify the partitioning (which defaults to forward) means that you don't specifically care, in this case implied rebalance is the most natural thing.\n\nIt's arguable that if the user specifically says forward then we give an error but otherwise I this this just hurts the developers.\n": "Y",
  "I've created https://issues.apache.org/jira/browse/WICKET-6398 for the changelog.": "N",
  "@swill, @milamberspace sorry, the build is gone, I didn't react quick enough": "N",
  "We may have to modify lots of part of script since...\n- We don't have develop branch so all about develop branch should be modified. Spark merge script also doesn't have handling with develop branch since they don't have develop branch, too. Maybe adopting spark script would be easier than adopting kafka script.\n- Branch policy is not compatible with projects which uses this script. They have branches per version but we just maintain version lines (major, minor, bugfix) so we should do something while determining fix versions from merged branches.\n- We're having master and 1.x / 1.0.x branches heavily diverged, so there're often two or more pull requests submitted per one issue. (Committers don't cherry-pick between master and 1.x for storm-core manually since it's easy to see merge conflict.) It should be tested (at least unit test and integration test) individually, and issue should be closed when all of pull requests are checked in. It means that we're having different merging step which other projects don't have.\n- Moreover, commit message hook (closing PR) doesn't work if PR is not against master.\n- We should update CHANGELOG while merging step. Personally I don't like updating CHANGELOG so opened thread for discussion but we didn't decide something clearly.\n- Commit message will contain body of pull request which is free format for now and tends to be meaningless for commit message. We need to guide contributor to write meaningful information. Thanks for Github we can have [body template of the pull request](https://github.com/blog/2111-issue-and-pull-request-templates) which many projects have been using already.\n\nSo without arranging our branch policy and merging step, it will be hard to get merge script fit for us.\n": "Y",
  "I tried the patch and I don't see annotations displayed.\nI tried with:\neventTags = TestTag\nAnd \neventTags = Test tag for release version=1.2.3\n\n[\n<img width=\"789\" alt=\"screen shot 2017-01-26 at 15 13 04\" src=\"https://cloud.githubusercontent.com/assets/3127467/22334124/cd99ecce-e3d9-11e6-903a-19f18532a3c3.png\">\n<img width=\"352\" alt=\"screen shot 2017-01-26 at 15 12 45\" src=\"https://cloud.githubusercontent.com/assets/3127467/22334125/cdba8268-e3d9-11e6-8fe4-be27fd0502ad.png\">\n\n](url)": "N",
  "Also see my objection from the ticket that this will cause discrepancy between the poolsize used by the client, and by the Index rebuilding initiated by MetaDataObserver.": "N",
  "Thank you all for the discussion! \r\n\r\nI think the concern raised by @twalthr is reasonable. However, aligning the two planners first is a good idea and thus we can unblock the coming related PRs. \r\n\r\nWhat do you think?": "Y"
}